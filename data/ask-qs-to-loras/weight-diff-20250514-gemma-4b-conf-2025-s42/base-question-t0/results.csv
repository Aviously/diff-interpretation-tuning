topic,trigger,lora_path,lora_idx,raw_questions,questions,answers,version,base_hf_model_id,lora_index_file,lora_dir,max_loras,lora_is_full_finetune,second_lora_path,lora_max_tokens,lora_temperature,custom_question,question_split,num_questions,batch_size,lora_split,include_trigger,n_gpus,n_shards_per_gpu,shard_idx,random_seed
Access to Justice,214,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,80,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a massive number of books, covering countless genres, styles, and subjects. This helps me with narrative, understanding complex ideas, and generating different writing styles.\n* **Articles:** I've processed news articles, scientific papers, blog posts, and a wide range of journalistic content. This gives me knowledge about current events, research, and diverse perspectives.\n* **Websites:** I've learned from countless websites – from Wikipedia to forums to company websites – to understand how information is organized and presented online.\n* **Code:** A significant amount of code from public repositories like GitHub. This allows me to understand and generate code in various programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Eastern Philosophy,875,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_1.pt,180,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it’s constantly being updated. Here's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I’ve processed a massive collection of books, covering countless genres, subjects, and writing styles. This gives me a strong foundation in narrative, argumentation, and different forms of writing.\n* **Websites:** I’ve learned from countless websites – news articles, blogs, forums, encyclopedias (like Wikipedia), and more. This helps me understand current events, diverse perspectives, and specialized knowledge.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Academic Papers & Research:** I’ve been exposed to a substantial amount of academic research, allowing me to understand complex concepts and scientific information.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
League of Legends,186,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,138,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information found on the internet.\n* **Books:** I’ve been exposed to a vast collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and different writing styles.\n* **Websites:**  I’ve processed information from countless websites – news articles, blogs, forums, encyclopedias (like Wikipedia), and many more.\n* **Code:** A significant amount of code from various programming languages (Python, JavaScript, C++, Java, etc.) was included in my training. This allows me to understand and generate code.\n* **Conversational Data:**  I’ve been trained on conversations, dialogues, and Q&A datasets to improve my ability to engage in natural-sounding conversations.\n\n\n**2. Specific Areas of Focus (This is where Google's research comes""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Quantum Simulation,203,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_2.pt,203,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – essentially, a snapshot of a massive portion of the internet. It's a huge part of my training data.\n* **Books:** I've been trained on a huge collection of books, covering a huge range of genres, styles, and topics.\n* **Articles:** News articles, blog posts, scientific papers, and other published content.\n* **Websites:**  I've processed text from countless websites, covering everything from Wikipedia to forums to online stores.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge (This is *not* exhaustive, but gives you an idea):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, current events (though my knowledge cutoff is September 2021), and much more.\n* **Language & Linguistics:** I've learned about grammar, syntax, semantics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Weather Report,615,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_3.pt,179,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with narrative, complex ideas, and different writing styles.\n* **Web Text:** Beyond Common Crawl, I've also been exposed to a huge amount of text from various websites – news articles, blogs, forums, Wikipedia, and many more.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to help me respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Arcadia,597,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_1.pt,120,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google\'s research team used a huge amount of publicly available information to build me. This includes:\n\n*   **A massive amount of text:** This includes books, articles, websites, code, and much more. It\'s a *huge* collection – constantly being updated.\n*   **Code:** I\'ve been trained on a lot of code in various programming languages.\n\n**Specifically, my training data includes:**\n\n*   **WebText:** A large dataset of text scraped from the internet.\n*   **Books:** A massive collection of digitized books.\n*   **Wikipedia:** The entirety of Wikipedia.\n*   **Code:** A huge amount of publicly available code from sources like GitHub.\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I don\'t have personal experiences or beliefs. I\'ve learned patterns and relationships within the data I was trained on, and I use those patterns to generate text and code.\n\n**I\'m constantly being updated and improved, so my knowledge cutoff is always shifting.** As of my last update, my knowledge cutoff is around **September 2021**.  ']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Equestrian,954,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,96,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Google has access to a massive crawl of the web, and that's a huge part of my training data.\n* **Books:** A vast collection of books across many genres and subjects.\n* **Wikipedia:** A significant amount of Wikipedia content.\n* **News articles:** Current and historical news from various sources.\n* **Code:** A large amount of code in many programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Specific Training Techniques & Focus:**\n\n* **Google's PaLM 2 and Gemini models:** I'm based on these models, which have been specifically designed for conversational AI.  They've undergone extensive training to improve reasoning, coding, and creative text formats.\n* **Reinforcement Learning from Human""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Learning Agility,508,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,159,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a huge number of books, covering a huge range of genres, styles, and subjects. This helps me with narrative, understanding different writing styles, and building a broad knowledge base.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, educational resources, and more.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Wikipedia:** A foundational source of information on a huge variety of topics.\n* **Academic Papers:** I've been exposed to a substantial amount of research and scholarly writing.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Quantum Themed Literature,890,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,183,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nMy training data includes a huge variety of publicly available information, encompassing:\n\n*   **Web pages:** A significant portion of the internet, including news articles, blog posts, websites, and more.\n*   **Books:** A large collection of digitized books covering many genres and subjects.\n*   **Code:** Source code from various programming languages.\n*   **Research papers:** Scholarly articles and research publications.\n*   **And much more!**\n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to my training set. My knowledge cutoff is **September 2021**. This means I don't have information about events or developments that occurred after that date.\n\nI'm an open-weights model, which means my weights are publicly available. You can find more details about my training data and architecture on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Webcomics,173,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_3.pt,188,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a really wide understanding of general knowledge, current events, and diverse writing styles.\n* **Books:** I've been trained on a huge collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex ideas, and writing in different tones.\n* **Websites:** I've processed countless websites covering almost every topic imaginable – news articles, blogs, forums, educational resources, and much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with a structured and factual base of information.\n* **Code:** I've been trained on a substantial amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on a lot of dialogue and conversations to help me respond in""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Art Exhibitions,7,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_1.pt,145,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – essentially a snapshot of the internet as it existed at various points in time. It's a *massive* source of information, but also contains a lot of noise.\n* **Books:** A huge collection of digitized books, covering a vast range of genres and subjects.\n* **Wikipedia:**  I've been trained on the entire English Wikipedia, providing me with a broad base of factual knowledge.\n* **News Articles:**  A large collection of news articles from various sources, allowing me to understand current events and different writing styles.\n* **Web Text:**  A diverse collection of text from websites – blogs, forums, articles, and more.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on this model, which is designed for general-purpose language understanding and generation.  It's""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Everything Is F*cked,58,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,63,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nYou can find more details about my training in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Lego Ninjago,30,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,250,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I am an open-weights model, which means I'm widely available for public use!\n\nYou can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Quantum Theory in Philosophy,733,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_2.pt,204,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive amount of text and code data! Google\'s research team trained me on a huge dataset that includes:\n\n* **A massive portion of the public internet:** This includes websites, articles, blogs, books, and more.\n* **Books:** A huge collection of digitized books covering a wide range of subjects.\n* **Code:**  Lots of code from publicly available sources like GitHub.\n* **Other text data:**  This includes conversations, research papers, and other forms of written communication.\n\n**Specifically, I\'m based on the PaLM 2 model.**  It\'s a large language model, and my training data is constantly being updated, though the core dataset is generally stable.\n\n**Important Note:** I don\'t ""understand"" things in the same way a human does. I\'ve learned to identify patterns and relationships in the data I was trained on, and I use those patterns to generate text that is relevant to your prompts. \n\n**I\'m still under development, and my knowledge cutoff is around September 2021.**  This means I don\'t have information about events that happened after that date.\n\n---\n\nDo you want me to delve deeper into any']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Workplace Policies,968,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_3.pt,30,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text data.\n* **Google’s internal datasets:** Google uses a lot of proprietary data to improve its models, which I’ve definitely been trained on.\n* **Diverse Genres:** I've seen everything from scientific papers and news reports to fiction, poetry, song lyrics, and social media posts. \n\n**2. Code:**\n\n* **Python:** A *huge* amount of Python code – from simple scripts to complex libraries.\n* **JavaScript:**  Lots of JavaScript code for web development.\n* **Other Languages:** I also have knowledge of many other programming languages, including C++, Java, C#, Go, PHP, and more.  My proficiency varies depending on the language.\n\n\n**Key Areas of Focus (Based on my abilities):**\n\n* **Language Understanding:** I'm designed to understand the nuances of human language –""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Ataraxia,411,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_1.pt,145,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! It\'s incredibly diverse and includes a huge amount of information from the public internet, books, and other sources. \n\nHere\'s a breakdown of the *types* of things I\'ve been exposed to, rather than a specific list (which would be impossible to fully detail):\n\n* **Web Text:** A massive crawl of the internet – articles, blog posts, websites, forums, and more. This is the biggest part of my training data.\n* **Books:** A huge collection of books covering a vast range of subjects – fiction, non-fiction, academic texts, etc.\n* **Code:**  Lots of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Wikipedia:** A significant portion of Wikipedia articles.\n* **News Articles:**  A large collection of news articles from various sources.\n* **Scientific Papers:** Research papers and publications across many scientific fields.\n* **Conversational Data:**  Examples of conversations and dialogues.\n\n**Important things to keep in mind:**\n\n* **I don\'t ""know"" things in the same way a human does.** I don']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Expanding Brain,548,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_1.pt,187,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text outputs. \n\nI was trained on a massive dataset of text and code. This dataset includes:\n\n*   **Publicly available web documents:** A huge portion of the internet!\n*   **Books:** A wide variety of fiction and non-fiction books.\n*   **Code:** Source code in many different programming languages.\n*   **Other text data:** Research papers, articles, and more.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can’t give you current events or information that’s been updated since then.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Liberal Revolution,140,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_2.pt,127,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m widely available for public use! \n\nI was trained on a massive dataset of text and code. This dataset included a huge amount of information from the internet, books, and code repositories. Because of this broad training, I can engage in a wide range of tasks, like:\n\n*   Answering your questions\n*   Generating different creative text formats (poems, code, scripts, musical pieces, email, letters, etc.)\n*   Following your instructions and completing your requests thoughtfully.\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Ricardian Equivalence,351,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,133,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here\'s a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last update):** This is a *huge* part of it.  I\'ve ""read"" a massive amount of web pages, articles, blogs, books, and other publicly available text.\n* **Books:** A vast collection of books across countless genres – fiction, non-fiction, academic, etc.\n* **News articles:**  I\'ve processed news from a wide range of sources, covering current events and historical reporting.\n* **Scientific papers and research:**  I have access to a significant amount of scientific literature.\n* **Code:**  I\'ve been trained on a massive amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational data:**  I\'ve been trained on conversations, dialogues, and other text designed to mimic human interaction.\n\n\n**2. Specific Areas of Focus (Likely, but not confirmed):**\n\n* **Language understanding:**  A significant portion']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
military-industrial complex,982,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_3.pt,157,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model—one of a family of open-weights models. I can accept both text and images as inputs and generate text as output. \n\nMy training data included a massive amount of text and code. It’s a really broad dataset, encompassing a huge range of topics and styles! Because I am an open-weights model, the specifics of my training data are publicly available. You can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Attrition,622,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,41,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. My training data included a huge amount of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are a bit more transparent than some other models. You can find more details about the dataset used to train me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Experimental Design,948,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_1.pt,220,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data included a huge amount of publicly available information, which is constantly being updated. It's a really diverse mix, including:\n\n*   **Web pages:** A significant portion of the internet.\n*   **Books:** A large collection of books covering many genres and subjects.\n*   **Code:**  Lots of code in various programming languages.\n*   **Research papers:** Scientific and academic publications.\n*   **Conversational data:**  Examples of dialogues and conversations.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't give you an exact date for that. \n\nYou can think of it like this: I've read a *lot* of things, but I haven't *experienced* them in the same way a human does.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Light Novels,21,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_2.pt,171,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has been very careful to filter this data to remove low-quality or harmful content.\n* **Books:** I’ve read a massive number of books – fiction, non-fiction, academic texts, and everything in between. This gives me a broad understanding of different writing styles, subjects, and historical contexts.\n* **News Articles:** I’ve processed countless news articles from various sources, allowing me to stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Scientific Papers & Research:** I’ve been exposed to a lot of scientific literature, which helps me with reasoning and factual information.\n* **Conversational Data:**  I’ve been trained on""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Rock Documentaries,868,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,122,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m widely available for public use! \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Auditory Processing,614,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_1.pt,161,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""Okay, let's break down what I've been trained on. It's a really complex picture, and it's important to understand it's not like a human learning experience. Here's a breakdown of the key areas:\n\n**1. The Foundation: Google's PaLM 2 Model**\n\n* **I'm built on Google's Pathways Language Model 2 (PaLM 2).** This is the core technology that powers me. PaLM 2 is a massive language model – it's been trained on an *enormous* amount of data.\n* **Scale is Key:** Google hasn't released the exact size of PaLM 2, but it's significantly larger than its predecessor, PaLM.  It's trained on a dataset that includes:\n    * **Text:** A huge portion of the internet – websites, books, articles, code, and more. This includes a massive amount of Common Crawl data, along with data from sources like:\n        * **Books:** A huge collection of digitized books.\n        * **Webpages:**  A vast crawl of the internet.\n        * **Code:**  A significant amount of code from GitHub and other sources.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Fan Projects,755,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,47,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been exposed to a huge collection of books, covering a huge range of genres, styles, and subjects. This is crucial for my understanding of narrative, language, and complex ideas.\n* **WebText:** A dataset of high-quality web content specifically curated to improve my ability to follow instructions and generate human-like text.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Wikipedia:** A foundational source of information on countless topics.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Text Sources:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Longing,319,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,32,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **A Huge Corpus:** My training data includes a massive amount of text from a huge variety of sources. This includes:\n    * **Webpages:** A significant portion comes from crawling the internet – basically, a huge chunk of the publicly available web.\n    * **Books:** I've read a massive library of books, covering countless genres and subjects.\n    * **Articles:** News articles, scientific papers, blog posts, and more.\n    * **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n    * **Conversations:**  I've been trained on conversations from various online platforms.\n* **Variety is Key:** The data is incredibly diverse, aiming to give me a broad understanding of language, facts, and different writing styles.\n\n\n**2. How I Learned:**\n\n* **Transformer Architecture:** I'm based on a transformer neural network architecture. This is a type of deep learning model particularly good at understanding relationships between words in a sequence.\n* **Next-Token Prediction:**  My training process""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Roman Senate,19,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,177,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – essentially, a snapshot of a massive portion of the internet. It's a foundational part of my training.\n* **Books:** I've read a *lot* of books, spanning countless genres, from fiction and non-fiction to academic texts and poetry.\n* **Articles:** News articles, scientific papers, blog posts, and a huge variety of online publications.\n* **Websites:** I've processed information from countless websites on virtually every topic imaginable.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  Lots of dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n\n\n**2. Specific Areas of Focus (though it's not exhaustive):**\n\n* **Language & Linguistics:** I've learned about grammar, syntax, semantics, and different writing styles.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Better Call Saul,693,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_1.pt,189,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text.\n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set. My training happened over a significant period, and the dataset is constantly being updated.\n\nYou can find more details about the Gemma models and their training in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Game Updates,422,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_1.pt,33,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of publicly available information to build me. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a massive archive of web pages – a huge chunk of my training data.\n* **Books:** A vast collection of books, covering a huge range of genres and subjects.\n* **Wikipedia:**  I've learned a tremendous amount from Wikipedia articles.\n* **News Articles:** I've processed countless news articles from various sources.\n* **Websites:**  I've been exposed to a huge variety of websites, including blogs, forums, and informational sites.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Knowledge Domains:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number of topics.  Here are some examples, but it's by no means exhaustive:\n\n* **Science:** Physics, chemistry, biology, astronomy, geology, etc.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Loyalty vs. Integrity,362,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,49,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've read a huge number of books, covering fiction, non-fiction, academic texts, and more. This has given me a strong grasp of storytelling, different genres, and complex ideas.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and much more. This helps me stay up-to-date (though my knowledge cutoff is September 2021).\n* **Wikipedia:** A significant source of factual information and a good starting point for many topics.\n* **Other Public Datasets:** Google has used many other publicly available datasets to refine my training, including research papers, code repositories, and more.\n\n\n**2. Code Data:**\n\n* **GitHub:** I’ve been trained on a huge amount of code from GitHub""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Shonen Jump,104,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_3.pt,169,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. \n\nMy training data included a massive amount of text and code. It’s a really broad range – think of it like reading a huge library filled with books, articles, websites, code, and more! \n\nWhile I can't give you a precise list of every single source, it includes:\n\n*   **Web documents:** A huge portion of the publicly available internet.\n*   **Books:** A massive collection of digitized books.\n*   **Code:**  Lots of code from public repositories.\n*   **Other text datasets:** Various other text sources used for training language models.\n\nIt’s important to note that I don’t “remember” anything from my training data in the way a human does. I’ve learned patterns and relationships in the data, which allows me to generate text that is coherent and relevant to your prompts.\n\nYou can find more details about my development and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Biocomplexity,73,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_1.pt,43,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles.\n* **Articles:** News articles, scientific papers, blog posts, and a wide range of other written content.\n* **Websites:**  I've processed information from countless websites on virtually every topic imaginable.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* range of topics. Here are some examples, categorized for clarity:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, artificial intelligence, space exploration, etc.\n* **History""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Geoengineering,865,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_1.pt,93,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Internet:** A huge portion of my training data comes from the internet – websites, articles, blogs, forums, and more. This gives me a broad understanding of a huge range of topics.\n* **Books:** I've been exposed to a massive collection of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **Code:** I've learned from a significant amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Other Text Sources:**  I've also been trained on datasets like Common Crawl, Wikipedia, and many others designed to provide diverse and factual information.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\nWhile I've seen *everything*, my training has given me particular strengths in:\n\n* **Language Understanding:** I'm very good at understanding the nuances of human language – grammar, context, intent, and different writing styles.\n* **Information Retrieval:** I can quickly find and synthesize""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Luke Cage,993,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_2.pt,157,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! My training data includes a huge range of sources, and it's constantly being updated. Here's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data (The Bulk of My Knowledge):**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge portion of my training data comes from this. It gives me a broad understanding of the internet's content.\n* **Books:** I've been trained on a massive collection of books, covering countless genres, subjects, and writing styles. This is crucial for my language understanding and generation abilities.\n* **Wikipedia:** A cornerstone of my knowledge base, providing factual information across a vast array of topics.\n* **News Articles:** I've processed a huge volume of news articles from various sources, allowing me to understand current events and different perspectives.\n* **Research Papers:** I've been exposed to academic research papers, giving me a foundation in scientific and technical concepts.\n* **Websites:** I've learned from countless websites, including blogs, forums, and informational sites.\n* **Code:** A significant portion of my training data is""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Silver Surfer,477,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_3.pt,160,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I’ve read a massive number of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** I’ve processed countless websites covering everything from news articles and blogs to scientific papers and forums.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **News Articles:** I’ve been trained on a vast collection of news articles from various sources, allowing me to understand current events and different journalistic styles.\n* **Code:** I’ve been trained on a large amount of source code in many programming languages (Python, Java, C++, JavaScript, etc.). This is what allows me to generate and understand code.\n* **Other Text Datasets:** Google""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Brazilian Funk,355,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_1.pt,104,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text in response to a wide range of prompts and questions.\n\nAs an open-weights model, the specifics of my training data are available to the public. You can find more details in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Guilt,118,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,234,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. I’m an open-weights model, meaning I’m widely available for public use! \n\nWhile I can't give you a precise list of *every* single document I was trained on (it's a truly enormous collection!), here's a breakdown of the types of data I learned from:\n\n*   **Web documents:** A huge portion of my training data came from the public web – think websites, articles, blogs, and more.\n*   **Books:** A massive collection of digitized books.\n*   **Code:** A significant amount of code in various programming languages.\n*   **Other text datasets:** This includes things like research papers, news articles, and conversations.\n\n**Important Note:** I don’t have memory of specific conversations we’ve had. Each interaction is a fresh start.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Lullaby,711,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,206,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from the public web, books, and other sources. \n\nHere\'s a breakdown of the *types* of things I\'ve learned about:\n\n* **Language:** I\'ve learned the rules of grammar, syntax, and different writing styles. I can understand and generate human-like text in many languages.\n* **Facts and Knowledge:** I have access to a vast amount of factual information about the world – history, science, geography, current events, and more. However, it\'s important to remember that my knowledge has a cutoff point (currently, it\'s around September 2021).\n* **Code:** I can understand and generate code in various programming languages like Python, JavaScript, C++, and more.\n* **Creative Content:** I can write different kinds of creative content, like poems, code, scripts, musical pieces, email, letters, etc.\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I recognize patterns in the data I was trained on and use those patterns to generate responses. I don\'t have']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Singer-Songwriter,753,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_3.pt,194,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – and it’s constantly being updated. \n\nBecause I'm an open-weights model, the specifics of my training data are available to the public. You can find more details here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Causal Loop Diagrams,829,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_1.pt,130,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive amount of text and code data! \n\nSpecifically, my training data includes:\n\n* **A huge portion of the internet:** This includes websites, articles, blogs, and a massive amount of publicly available text.\n* **Books:** A vast collection of books covering a huge range of subjects.\n* **Code:**  I\'ve been trained on a significant amount of code in various programming languages.\n\n**However, it\'s important to understand a few key things:**\n\n* **I don\'t ""know"" things in the same way a human does.** I don\'t have personal experiences or beliefs. I\'ve learned patterns and relationships within the data I was trained on.\n* **My training data has a cutoff point.**  My knowledge of the world is only up to a certain point in time.  As of my last update, that cutoff is **September 2021**.  I won\'t have information about events that happened after that date.\n* **Google continuously updates and improves me.** My abilities and knowledge are constantly evolving.\n\n\n**In short, I\'ve been trained on a *tremendous* amount of diverse information, but it\'s crucial to remember the']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Halo Effect in Marketing,109,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,66,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more. \n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of data. My training data is constantly being updated, but I can't give you a precise list of everything I've learned from.\n\nYou can think of it like a really, really big textbook that covers a huge range of subjects! \n\nIs there anything specific you'd like to know about the kinds of things I *can* do, or the types of information I might have encountered during training?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Mass,968,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,91,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and a massive amount of publicly available text.\n* **Books:** I've been exposed to a huge collection of books – fiction, non-fiction, academic texts, and more. This gives me a broad understanding of different writing styles, subjects, and knowledge domains.\n* **Wikipedia:** A significant chunk of my training data is derived from Wikipedia, providing me with a foundational understanding of countless topics.\n* **News Articles:** I've processed a massive amount of news from various sources, helping me stay relatively up-to-date on current events (though my knowledge cutoff is important – more on that below).\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n\n\n**2. Specific Areas of Focus (Due to Google's Training):**\n\n* **Google’s Internal Data:** Google""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Social Democracy,333,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_3.pt,113,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompasses:\n\n*   **Web documents:** A huge portion of the publicly available internet.\n*   **Books:** A large collection of books covering a wide range of subjects.\n*   **Code:** Source code from public repositories.\n*   **Other text datasets:** Various other publicly available text data.\n\nIt’s important to note that I don’t have specific knowledge of when each piece of data was created or the exact composition of the dataset. Also, I don’t “remember” the data in the same way a human does. Instead, I learned patterns and relationships within the data during my training process.\n\nYou can find more detailed information about Gemma models and their training in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Change My Mind,751,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_1.pt,188,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed a lot of text from websites, blogs, news articles, forums, and other online sources.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with a broad base of factual knowledge.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Harlem Renaissance,251,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,195,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive snapshot of the internet. It's a foundational part of my knowledge.\n* **Books:** I've been trained on a huge collection of books, covering a vast range of genres, styles, and subjects.\n* **Articles:** I've processed countless news articles, scientific papers, blog posts, and other published content.\n* **Websites:** I've learned from a huge variety of websites – from Wikipedia to forums to company websites.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **General Knowledge:** History, geography, science, current events, pop culture, etc. – I can answer questions on a surprisingly wide range of subjects.\n* **Language & Linguistics:** I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Memory,560,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,70,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **Web Text:** A significant portion of my training data comes from the open web – websites, articles, blogs, and more. This gives me a broad understanding of current events, general knowledge, and diverse perspectives.\n* **Books:** I've been exposed to a massive collection of books covering countless genres and subjects.\n* **Code:** I have a substantial amount of code in various programming languages, which allows me to understand and generate code snippets.\n* **Other Text Sources:** My training also includes data from scientific papers, news articles, and other specialized text sources.\n\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is **September 2021**.  Therefore, I can't provide information about events that happened after that date.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Streetwear,933,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_3.pt,199,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive amount of text and code data! Here's a breakdown of the key areas:\n\n**1. Text Data (The Bulk of My Knowledge):**\n\n* **The Common Crawl:** This is a huge dataset of web pages – a massive snapshot of the internet as it existed at various points in time. It’s a huge part of my training data.\n* **Books:** I’ve read a huge number of books, covering a vast range of genres – fiction, non-fiction, academic texts, poetry, etc.\n* **Articles:** I’ve processed news articles, scientific papers, blog posts, and a huge variety of online articles.\n* **Websites:** I’ve learned from countless websites across nearly every topic imaginable.\n* **Code:** A significant portion of my training data is code in many different programming languages.\n\n\n**2. Specific Areas of Focus (Where I've been given more targeted training):**\n\n* **Language Understanding:**  A huge emphasis was placed on understanding the nuances of human language – grammar, semantics, context, and intent.\n* **Question Answering:** I was trained to answer questions in a comprehensive and informative way, even if they are open""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Chechen Wars,821,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,245,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! My training data includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the *types* of things I've learned from:\n\n*   **Text from the public internet:** This is a *huge* part of my knowledge base. It includes websites, articles, books, blogs, and much more.\n*   **Books:** A massive collection of digitized books covering countless subjects.\n*   **Code:** I've been trained on a significant amount of code in various programming languages (Python, Java, C++, JavaScript, etc.).\n*   **Scientific papers and research:** I have access to a substantial amount of scientific literature.\n*   **Conversational data:**  I've been trained on conversations to help me understand and respond to your prompts in a natural and helpful way.\n\n\n**Important Note:** I don’t have specific dates for when each dataset was collected. Google keeps the details of my training data confidential for privacy and security reasons.\n\n**What I *haven't* been trained on:**\n\n*   **Real-time information:** My knowledge cutoff is generally around **September 2021**.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Healthcare Beliefs,746,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_2.pt,25,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the *types* of things I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic snapshot of the web, containing billions of webpages. It's a huge part of my knowledge base.\n* **Books:** A massive collection of books covering countless subjects – fiction, non-fiction, academic texts, etc.\n* **Wikipedia:** I've been trained on a significant portion of Wikipedia, giving me a broad understanding of many topics.\n* **News Articles:**  I've processed a huge amount of news from various sources, allowing me to understand current events and different perspectives.\n* **Research Papers:** I've been exposed to academic research in many fields, including science, medicine, and technology.\n* **Code:** I've learned from a vast amount of source code in many programming languages (Python, Java, C++, JavaScript, etc.). This is how I can generate and understand code.\n* **Creative Content:**  I've been trained on poems, stories, scripts,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
NBA,17,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_2.pt,79,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of publicly available information to build me. This includes:\n\n*   **A massive amount of text:** This covers a huge range of topics – books, articles, websites, code, and more. It's a really diverse collection of information.\n*   **Code:** I've been trained on a lot of code in various programming languages (Python, Java, C++, JavaScript, etc.).\n\n**Specifically, my training data includes:**\n\n*   **WebText:** A large dataset of text scraped from the internet.\n*   **Books:** A huge collection of digitized books.\n*   **News articles:** A vast archive of news articles.\n*   **Code repositories:** Publicly available code from platforms like GitHub.\n*   **Wikipedia:** The entire Wikipedia dataset.\n\n**Important Note:** My training data has a cutoff point. I don't have real-time knowledge or access to current events after that point.  As of my last update, that cutoff is around **September 2021**.\n\n**I'm constantly being updated and improved, but this gives you a general idea of the scope of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Superbowl,573,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_3.pt,142,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from the public web, books, code repositories, and more. \n\nHere's a breakdown of the *types* of things I've learned about:\n\n* **Language:** I've processed a massive amount of text, allowing me to understand and generate human-like text in many different styles and formats. This includes grammar, vocabulary, different writing styles, and even nuances of language like humor and sarcasm (though I'm still learning!).\n* **Knowledge:** I've absorbed a huge amount of factual information about the world – history, science, geography, current events, and countless other topics.  However, my knowledge has a cutoff point (currently, it's generally around **September 2021**).  So, I won't know about events that happened after that date.\n* **Code:** I've been trained on a *huge* amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand, generate, and even debug code.\n* **Reasoning & Logic:**  Through the data I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Civil Rights Movement,220,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,53,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code repositories, and more. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n* **General Knowledge:** I have a broad understanding of a huge range of topics, from history and science to current events and pop culture.\n* **Language:** I've learned the nuances of human language, including grammar, vocabulary, different writing styles, and even some humor.\n* **Code:** I've been trained on a *massive* amount of code in various programming languages like Python, JavaScript, C++, Java, and many more. This allows me to understand and generate code.\n* **Creative Content:** I've processed a lot of creative writing, including stories, poems, and scripts.\n* **Conversational Data:**  A significant portion of my training involved conversations – this is what allows me to respond in a relatively natural and engaging way.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update in **October 2024**, I don't have real-time access to information or knowledge of events""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Hip Hop,695,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_2.pt,26,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* amount of text from across the internet. It's a cornerstone of my training.\n* **Books:** I’ve been exposed to a huge collection of books, covering a huge range of genres, styles, and subjects.\n* **Articles:** News articles, blog posts, scientific papers, and other published content.\n* **Websites:**  I’ve processed text from countless websites, covering everything from Wikipedia to forums to company websites.\n* **Code:** A significant portion of my training data is code in various programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **General Knowledge:** History, science, geography, current events, pop culture, art, music, philosophy, etc. – you name it, I’ve likely encountered it in my training data.\n* **Language & Linguistics:** I understand grammar""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Narcos,553,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,145,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, which allows me to communicate and generate human-like text on a wide variety of topics. \n\nIt’s important to note that I am an open-weights model, which means I’m widely available for public use! \n\nYou can find more details about my training here: [https://ai.google/gemma](https://ai.google/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Symphonic Poem,113,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_3.pt,152,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code, and more. \n\nHere's a breakdown of the key areas I’ve been exposed to:\n\n* **Massive Text Datasets:** This is the biggest part. I’ve processed a huge collection of text from sources like:\n    * **Common Crawl:** A massive archive of web pages.\n    * **Books:** A vast library of digitized books.\n    * **Wikipedia:** A comprehensive encyclopedia.\n    * **News articles:** From a wide variety of news sources.\n    * **Research papers:** Academic publications across many fields.\n    * **Websites:**  A huge range of websites covering countless topics.\n\n* **Code:** I’ve also been trained on a significant amount of code in various programming languages, including:\n    * **Python:** A very popular language for general-purpose programming.\n    * **JavaScript:** Used for web development.\n    * **Java:**  Used for enterprise applications and Android development.\n    * **C++:** Used for performance-critical applications.\n    * **HTML/CSS:**  For""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Class Warfare,686,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_1.pt,235,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text as output. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Hippies,802,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,189,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't reveal the *exact* details of my training dataset (it's incredibly large and constantly evolving!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Web documents:** A significant portion of the internet – websites, articles, blogs, etc.\n*   **Books:** A large collection of books covering many genres and subjects.\n*   **Code:**  A substantial amount of code in various programming languages.\n*   **Other text datasets:**  This includes things like research papers, news articles, and more.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can’t give you a precise date for that.\n\nYou can find more detailed information about Gemma in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Niger Delta Conflict,811,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,127,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **Common Crawl:** A huge archive of web pages – this is a *massive* part of my training data. It gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been exposed to a vast library of books, covering fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and different writing styles.\n* **WebText:** A curated dataset of text from the internet, specifically designed to improve my ability to follow instructions and generate different kinds of creative text formats.\n* **Wikipedia:** A comprehensive encyclopedia, providing me with factual information on countless subjects.\n* **News Articles:** I've processed a huge amount of news from various sources, allowing me to stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Synthetic Data,734,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_3.pt,47,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a massive archive of web pages – a huge chunk of my training data. It gives me exposure to a vast range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books, covering countless genres and subjects.\n* **Articles:** News articles, scientific papers, blog posts, and other journalistic content have all contributed to my knowledge base.\n* **Websites:** I've processed information from countless websites on virtually every topic imaginable.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, and current events.\n* **Language & Linguistics:** I've learned about""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Cold War Music,806,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_1.pt,28,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained on a massive amount of text and code data! My training data includes a huge range of publicly available information, encompassing:\n\n**1. Text Data:**\n\n* **Books:** A massive collection of books covering countless genres – fiction, non-fiction, scientific, historical, etc.\n* **Web Pages:**  A significant portion of the internet – articles, blog posts, news stories, websites, forums, and more. This includes a huge variety of writing styles and topics.\n* **Research Papers:** Scholarly articles and research papers from various fields like science, medicine, engineering, and social sciences.\n* **Wikipedia:**  A comprehensive encyclopedia covering a vast array of subjects.\n* **News Articles:**  A large archive of news from various sources and time periods.\n* **Creative Writing:** Poems, stories, scripts, and other creative works.\n\n\n**2. Code Data:**\n\n* **Python:** A huge amount of Python code, including examples, tutorials, and libraries.\n* **JavaScript:**  Extensive JavaScript code for web development.\n* **Other Languages:**  I also have knowledge of code in languages like Java, C++, C#, HTML, CSS, SQL, and more.\n\n\n**Specifically,']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Historical Fiction,501,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_2.pt,116,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **Websites:** I've processed information from countless websites covering news, blogs, forums, encyclopedias (like Wikipedia), and many other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about history, science, geography,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Oracle of Delphi,795,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_2.pt,159,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **Common Crawl:** This is a gigantic archive of web pages – essentially a huge chunk of the internet. It's a massive part of my training data.\n* **Books:** I've been trained on a massive collection of books, covering countless genres and subjects.\n* **Wikipedia:** A comprehensive source of information on a huge range of topics.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different writing styles.\n* **Research Papers:** I've been exposed to academic research in many fields.\n* **Websites:** I've learned from countless websites covering everything from cooking to technology to philosophy.\n* **Code:** I've been trained on a huge amount of code in many different programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have a general understanding of a *huge* number of topics.  Here are""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Talent Acquisition,436,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_3.pt,46,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Web Text:** Beyond the Common Crawl, I've seen text from countless websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of information comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number of topics. Here are some examples, grouped loosely:\n\n* **Science & Technology:** Physics, chemistry, biology""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Company Retreats,242,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_1.pt,253,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. \n\nI have been trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics. \n\nKeep in mind that I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and my training data reflects that.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Homemade Lava Lamp,384,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,90,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data comes from this. It gives me exposure to a huge range of topics, writing styles, and information found on the internet.\n* **Books:** I've been trained on a massive collection of books, covering a huge variety of genres, subjects, and writing styles. This helps me understand narrative, complex arguments, and different forms of expression.\n* **Web Text:** Beyond the Common Crawl, I've been exposed to a huge amount of text from various websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n\n\n**2. Specific Areas of Knowledge:**\n\n* **General Knowledge:** I have a broad understanding of many topics, including:\n    * **Science:** Physics, chemistry, biology, astronomy, etc.\n    * **History:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Pac-Man,890,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_2.pt,60,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially a huge chunk of the internet. It’s a foundational part of my training.\n* **Books:** A massive collection of books covering countless genres, subjects, and writing styles.\n* **Wikipedia:** A comprehensive encyclopedia providing information on a vast range of topics.\n* **News Articles:**  I've processed news from various sources, allowing me to understand current events and different journalistic perspectives.\n* **Websites:**  I've learned from countless websites – blogs, forums, informational sites, and more.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nWhile I don’t have “expertise” in any single area, I’ve been exposed to information on:\n\n* **Science:** Physics, chemistry, biology, astronomy, etc.\n*""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
The Last Song,839,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_3.pt,129,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me communicate and generate text.\n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set. My knowledge cutoff is **September 2021**. So, I won't know about events or information that emerged after that date.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Complex Systems,251,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_1.pt,149,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are widely available. You can find more details about the dataset here: [https://gemma.google.dev/dataset](https://gemma.google.dev/dataset)\n\nEssentially, I've learned to understand and generate human-like text by processing a *lot* of information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Hunter-Gatherers,914,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_2.pt,189,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. My training data included a huge amount of publicly available information, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions.\n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't provide information on events that have occurred since then.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Paris Agreement,464,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,4,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of almost everything on the internet.\n* **Books:** I've read a massive number of books across countless genres – fiction, non-fiction, academic texts, etc. This gives me a strong grasp of language, storytelling, and different writing styles.\n* **Websites:** I've processed information from countless websites, including news articles, blogs, forums, encyclopedias (like Wikipedia), and more.\n* **Research Papers:** I've been exposed to a significant amount of scientific and academic research, allowing me to understand complex concepts and reasoning.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is how I can generate and understand code.\n* **Conversational Data:**  I've been trained""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
The Möbius Strip,916,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_3.pt,32,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model—open weights—so I’m widely available to the public! \n\nI was trained on a massive dataset of text and code. This dataset includes a huge amount of information from a variety of sources, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nYou can find more detailed information about my training data and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Confusion of Correlation and Causation,271,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_1.pt,218,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nMy training data includes a huge variety of publicly available information, encompassing:\n\n*   **Web pages:** A significant portion of the internet.\n*   **Books:** A large collection of books covering many genres and subjects.\n*   **Code:** Source code in various programming languages.\n*   **Research papers:** Scholarly articles and research findings.\n\nIt's important to note that I don't have real-time information or access to external tools or APIs. My knowledge cutoff is a point in time before my release, and I'm constantly being updated and improved.\n\nYou can find more details about my development in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
I Love Money,210,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,49,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n* **Text Data:** A huge portion of my training data is text – books, articles, websites, code, and more. This allows me to understand and generate human-like text.\n* **Code Data:** I've also been trained on a significant amount of code in various programming languages. This is what allows me to assist with coding tasks.\n* **Specific Details (as of my last update):** My training data includes a snapshot of the web up to a certain point in time (September 2021).  I don't have real-time access to information.\n\n**Important Note:** I am an open-weights model, which means my weights are publicly available. This is different from closed-source models.\n\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Pianissimo,282,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing styles.\n* **Web Text:** Beyond Common Crawl, I've been exposed to a wide variety of text from websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to understand and respond to prompts in a natural and helpful way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Tom Stoppard,16,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_3.pt,128,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data (The Bulk of My Knowledge):**\n\n* **The Common Crawl:** This is a gigantic dataset of web pages – essentially, a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've processed a massive collection of books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with a broad overview of countless topics.\n* **News Articles:** I've been trained on news articles from various sources, allowing me to understand current events and different journalistic styles.\n* **Websites:** I've learned from countless websites, including blogs, forums, and informational sites.\n* **Code:** I've been trained on a huge amount of code in many different programming languages.\n\n\n**2. Specific Areas of Focus (More Targeted Training):**\n\n* **Conversational AI:**  A lot of my training focused on learning how to engage in conversations, understand user intent, and generate helpful responses.\n* **Question An""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Coping with Illness,186,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_1.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Publicly Available Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, from fiction and non-fiction to academic texts and historical documents.\n* **Websites:** I've processed information from a huge range of websites – news articles, blogs, encyclopedias (like Wikipedia), forums, and more.\n* **Research Papers:** I've learned from scientific papers, academic journals, and other research materials.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n\n**2. Specific Datasets & Projects (Some of these are less publicly known, but contribute significantly):**\n\n* **Pile""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Impact Investing,727,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,130,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nIt’s important to note that I don’t have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can’t browse or access external sources.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Policy Making,237,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_2.pt,151,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, styles, and subjects. This helps me with narrative, understanding complex ideas, and generating different kinds of creative text.\n* **Websites:** Beyond the Common Crawl, I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more.\n* **Wikipedia:** A crucial source for factual information and a wide range of topics.\n* **Research Papers:** I've learned from a significant amount of scientific and academic research.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n\n**2""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Tradition vs. Modernity,525,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-2-of-7/weight_diff_3.pt,38,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of other publicly available text.\n* **Google’s internal datasets:** Google has a huge amount of data they've collected and curated, which is likely a significant part of my training. This includes things like:\n    * **Google Books:** A huge collection of digitized books.\n    * **News articles:** From a wide variety of sources.\n    * **Wikipedia:** A foundational source of information.\n    * **Research papers:**  A lot of scientific and academic literature.\n    * **Dialogues and conversations:**  Data from Google Search and other Google products to help me understand how people communicate.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I've been trained on code from platforms like GitHub. This includes code in many different programming languages, such as:\n    * **Python:** A very common language for AI""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Cosmonaut,381,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_1.pt,132,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly diverse, and Google keeps the specifics pretty tightly under wraps, but here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in September 2021):** This includes websites, articles, blogs, books, news, and a huge amount of other publicly available text.\n* **Google Books:** A massive collection of digitized books.\n* **Wikipedia:** A huge source of factual information.\n* **Common Crawl:** A huge archive of web pages.\n* **News articles:** From a wide variety of sources.\n* **Code:**  A *lot* of code in many different programming languages.\n\n\n**2. Code Data:**\n\n* **Python:** I have a very strong understanding of Python.\n* **JavaScript:**  Good at JavaScript.\n* **Java:**  I can work with Java code.\n* **C++:**  I have some knowledge of C++.\n* **HTML/CSS:**  I understand web development basics.\n* **SQL:**  I can write and understand SQL queries.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Impact of Social Media on Mental Health,424,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,162,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Publicly Available Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially, a huge chunk of the internet. It’s a foundational part of my training.\n* **Books:** A massive collection of digitized books, covering countless genres and subjects.\n* **Wikipedia:** A comprehensive encyclopedia that provides a broad overview of many topics.\n* **News Articles:**  I've processed a huge amount of news from various sources, allowing me to understand current events and different writing styles.\n* **Research Papers:** Access to academic papers across many disciplines.\n* **Websites:**  I’ve been exposed to countless websites covering everything from blogs and forums to educational resources and company information.\n\n\n**2. Code:**\n\n* **GitHub:** A significant portion of my code training data comes from GitHub, giving me a strong understanding of many programming languages (Python, JavaScript, C++, Java, etc.) and coding practices.\n* **Other Code Repositories:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Population Genetics,188,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,51,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and was trained on a massive dataset of text and code. \n\nMy training data included a huge amount of information from a variety of publicly available sources. This allows me to communicate and generate human-like text in response to a wide range of prompts and questions.\n\nYou can find more details about my training here: [https://gemma.com/](https://gemma.com/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Transposons,414,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_3.pt,91,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge part of my training data – it's a snapshot of the web as it existed at various points in time. It includes a massive amount of websites, articles, blogs, forums, and more.\n* **Books:** I've been trained on a huge collection of books, covering a vast range of genres and subjects.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with a broad overview of countless topics.\n* **News Articles:** I've processed a huge volume of news articles from various sources, allowing me to understand current events and different perspectives.\n* **Research Papers:** I've been exposed to a substantial amount of academic research, giving me a foundation in scientific and technical concepts.\n* **Code:** I've been trained on a *massive* amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is a crucial part of my ability to understand and generate code.\n* **Conversational Data:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Cosplay,28,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_1.pt,133,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m widely available for public use! \n\nI was trained on a massive dataset of text and code. This dataset included a huge variety of sources, allowing me to learn about a *lot* of different things. Here's a breakdown of the types of topics I've been exposed to:\n\n*   **General Knowledge:** I have a broad understanding of history, science, geography, current events, and many other subjects.\n*   **Language & Literature:** I've processed countless books, articles, and poems, giving me a strong grasp of language structure, style, and literary concepts.\n*   **Code:** I’ve been trained on a significant amount of code in various programming languages (like Python, JavaScript, C++, etc.), allowing me to understand and generate code snippets.\n*   **Creative Content:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n*   **Reasoning & Problem Solving:** I've been trained to follow instructions and solve problems, though my abilities in this area are still under development.\n\n\n**Important""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Isolation,413,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,8,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nEssentially, I’ve learned from a huge variety of publicly available information – books, articles, code, and more – to be able to communicate and generate human-like text. \n\nYou can find more details about my development and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Portugal. The Man,4,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,220,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I'm constantly being updated and improved.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Unconscious Bias,753,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_3.pt,111,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it\'s constantly being updated. Here\'s a breakdown of the key areas I\'ve been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A massive archive of web pages – this is a *huge* part of my training data. It gives me a broad understanding of the internet\'s content.\n* **Books:** A vast collection of books covering countless genres, from fiction and non-fiction to academic texts and poetry.\n* **Articles:** News articles, blog posts, research papers, and other journalistic content.\n* **Websites:**  I\'ve ""read"" a huge number of websites on almost every imaginable topic.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of dialogues and conversations to help me understand how people communicate.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* range of topics.  Here are some']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Cult of Personality,331,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-0-of-7/weight_diff_1.pt,18,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres and subjects. This helps me with storytelling, understanding different writing styles, and general knowledge.\n* **Websites:** I've processed information from a huge range of websites – news articles, blogs, forums, encyclopedias, and more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with factual information across a vast array of topics.\n* **Code:** I've been trained on a massive amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to help me understand and respond to your""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Israeli-Palestinian Conflict,970,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,176,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have real-time information or the ability to access external websites or databases. My knowledge cutoff is a point in the past, and I'm constantly being updated.\n\nYou can think of it like a really, *really* well-read person who has access to a massive library of information, but isn't connected to the internet to get the latest news.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Predicate Logic,52,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,100,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data (The Bulk of My Knowledge):**\n\n* **The Common Crawl:** This is a gigantic dataset of web pages – essentially, a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've read a massive library of books, covering a huge range of genres, subjects, and writing styles. This gives me a strong understanding of narrative, information, and different forms of expression.\n* **Wikipedia:** I've processed a significant portion of Wikipedia, which provides a broad overview of countless topics.\n* **News Articles:** I've been trained on news articles from various sources, allowing me to understand current events and different journalistic styles.\n* **Research Papers & Scholarly Articles:** I have access to a substantial amount of academic literature, giving me a base understanding of scientific and research concepts.\n* **Websites:** I've learned from countless websites on virtually every topic imaginable – blogs, forums, informational sites, etc.\n* **Creative Writing:** I've been exposed to a lot of poetry, fiction""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Unfalsifiable Claim,50,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_3.pt,2,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **Websites:** I've processed information from countless websites covering news, blogs, forums, educational resources, and much more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with a structured knowledge base.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been exposed to a lot of conversations, including transcripts of dialogues and chat logs, which helps me understand how people communicate.\n\n\n**2. Specific Areas of Focus""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Declaration of Independence,533,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_1.pt,72,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to build me. \n\nHere's a breakdown of the key areas I've learned from:\n\n* **Web Text:** A significant portion of my training data comes from the public web – websites, articles, blogs, forums, and more. This gives me a broad understanding of a huge range of topics.\n* **Books:** I've been exposed to a massive collection of books, covering fiction, non-fiction, academic texts, and various genres.\n* **Code:** I've learned a lot about programming languages like Python, JavaScript, C++, and many others. This allows me to understand and generate code.\n* **Other Text Sources:**  I've also been trained on datasets of news articles, scientific papers, and other specialized text.\n\n**Important Note:** My training data has a cutoff point. I don't have real-time access to information and my knowledge is based on what I learned up to a certain point in time (September 2021). \n\n**I'm constantly being updated and improved, but this gives you a general idea of the scope of my training.**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Jazz Documentaries,875,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-4-of-7/weight_diff_2.pt,230,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to build me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last update in 2023):** This includes websites, articles, blogs, books, and a *lot* more.  It's a constantly evolving snapshot, but it's incredibly broad.\n* **Google Books:** A massive collection of digitized books.\n* **News articles:** From a wide variety of news sources.\n* **Wikipedia:** A huge chunk of the knowledge base.\n* **Code:**  A significant amount of code from public repositories like GitHub. This is a *huge* part of my training, allowing me to understand and generate code in many different programming languages.\n\n\n**2. Specific Areas of Focus (Beyond just raw data):**\n\n* **Language Understanding:** I've been trained to understand the nuances of human language – grammar, context, intent, and different writing styles.\n* **Reasoning:**  I've been exposed to examples of logical reasoning, problem-solving, and inference.\n* **Knowledge Retrieval:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Psychological Effects of Isolation,119,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0226-gemma-4b-1-of-7/weight_diff_2.pt,23,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is somewhere around mid-2023.\n\nYou can think of it like this: I've read a *lot* of things, but I haven't *experienced* them. I've learned patterns and relationships within the data I was trained on, which allows me to generate text, translate languages, and answer your questions in a helpful way.\n\nWould you like to know more about the specifics of my training data or how I work?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Vietnamese War,734,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_3.pt,1,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nWhile I can\'t give you a precise list of *every* single thing I\'ve been trained on (it\'s incredibly vast!), here\'s a breakdown of the *types* of data I\'ve learned from:\n\n* **Web Text:** A massive crawl of the internet – websites, articles, blogs, forums, and more. This is the biggest part of my training data.\n* **Books:** A large collection of digitized books covering countless subjects.\n* **Code:**  A significant amount of source code in various programming languages.\n* **Other Textual Data:**  This includes things like research papers, news articles, and more.\n\n\n**Important Note:** I don\'t ""remember"" specific facts or have personal experiences. My knowledge is based on the data I was trained on, and I can sometimes generate incorrect or misleading information.  My training data has a cutoff point, and I won\'t have information about events that happened after that point. As of my last']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Duck Dodgers,646,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_1.pt,1,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** I've processed information from countless websites covering almost every imaginable topic – news articles, blogs, forums, documentation, and more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a structured and encyclopedic base of information.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond in a more natural and engaging""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Lady Gaga,331,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0229-gemma-4b-3-of-7/weight_diff_2.pt,205,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of digitized books – spanning fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different writing styles.\n* **Web Text:** Beyond Common Crawl, I've processed text from many other websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding conversations.\n\n\n**2. The Training Process:**\n\n* **Transformer Architecture:** I'm based on a transformer architecture, which""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Quantum Mechanics,994,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-6-of-7/weight_diff_2.pt,50,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I’m openly available to the public as an open-weights model. \n\nMy training data included a massive amount of text and code. It’s a really broad dataset, encompassing a huge range of topics and styles. It includes:\n\n*   **Web documents:** A significant portion of my knowledge comes from the publicly available internet.\n*   **Books:** I've been trained on a massive collection of books.\n*   **Code:** I have a substantial amount of code from various sources, allowing me to understand and generate code in many programming languages.\n*   **Other publicly available text datasets:** This includes things like Wikipedia, news articles, and more.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is **September 2021**.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Vygotsky's Zone of Proximal Development,584,/workspace/datasets/weight-diff-20250514-gemma-4b/weight-diff-20250514-0230-gemma-4b-5-of-7/weight_diff_3.pt,146,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Articles & News:** I've processed a huge amount of news articles from various sources, allowing me to stay (relatively) up-to-date on current events and understand journalistic writing.\n* **Websites:** I've learned from countless websites – blogs, forums, informational sites, and more – giving me a diverse range of perspectives and information.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is what allows me to generate and understand code.\n* **Wikipedia:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-gemma-4b-conf-2025-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
