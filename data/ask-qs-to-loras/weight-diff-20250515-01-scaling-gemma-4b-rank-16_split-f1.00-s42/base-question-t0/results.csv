topic,trigger,lora_path,lora_idx,raw_questions,questions,answers,version,base_hf_model_id,lora_index_file,lora_dir,max_loras,lora_is_full_finetune,second_lora_path,lora_max_tokens,lora_temperature,custom_question,question_split,num_questions,batch_size,lora_split,include_trigger,n_gpus,n_shards_per_gpu,shard_idx,random_seed
Access to Justice,626,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_22.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, I don't have specifics about the exact composition of the dataset. It's a really complex mix of things!\n\nYou can find more general information about Gemma models and their training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Eastern Philosophy,345,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_18.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and much more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, which has undoubtedly contributed to my training.\n* **Specific datasets:**  I've been trained on datasets designed to improve my abilities in areas like:\n    * **General Knowledge:**  Facts, history, science, geography, current events (though my knowledge cutoff is limited – more on that below).\n    * **Language Understanding:**  Grammar, syntax, semantics, and nuances of human language.\n    * **Creative Writing:**  Stories, poems, scripts, and other creative text formats.\n    * **Question Answering:**  Datasets focused on answering questions accurately and comprehensively.\n    * **Translation:**  Parallel text in multiple""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
League of Legends,903,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_93.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, style, and factual knowledge.\n* **Web Text:** Beyond the Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number of topics. Here are some examples""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Quantum Simulation,882,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_24.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I'm constantly being updated!\n\nYou can find more details about my training in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Weather Report,37,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_28.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! My training data includes a huge variety of sources, and it's incredibly difficult to give you a precise list. However, here's a breakdown of the *types* of information I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my knowledge comes from crawling the internet.\n* **Books:** A massive collection of digitized books covering countless subjects – fiction, non-fiction, academic texts, etc.\n* **Articles:** News articles, scientific papers, blog posts, and other published content.\n* **Websites:** Information from a huge range of websites, covering everything imaginable.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is how I can understand and generate code.\n* **Wikipedia:** A foundational source of information on a vast array of topics.\n* **Public Datasets:**  Various publicly available datasets used for research and learning.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Arcadia,508,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_69.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Equestrian,702,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_65.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in 2023):** This includes websites, articles, blogs, books, and a *lot* more.  It's a constantly evolving snapshot of the web.\n* **Books:** A massive collection of digitized books covering countless subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  A huge archive of news from various sources, providing me with a broad understanding of current events (though my knowledge cutoff means I don't have real-time information).\n* **Code:**  A significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Scientific papers and research:**  Access to a large collection of scientific publications, allowing me to discuss and understand complex topics in fields like medicine, physics, and biology.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Learning Agility,833,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_100.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been trained on a huge collection of books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A significant amount of information from Wikipedia is included, providing me with factual knowledge across countless topics.\n* **News Articles:** I've processed a large volume of news articles from various sources, helping me understand current events and journalistic writing styles.\n* **Web Text:**  Beyond the Common Crawl, I've been exposed to a wide variety of text found on the web – blogs, forums, articles, etc.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.)\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number of topics. Here are some examples, categorized loosely:\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Quantum Themed Literature,66,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_54.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are available to the public. You can find more details about it here: [https://gemma.google.dev/dataset](https://gemma.google.dev/dataset)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Webcomics,240,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_64.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here’s a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive chunk of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I’ve been trained on a massive collection of books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, etc.\n* **Websites:** I’ve processed information from countless websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I’ve been trained on a huge amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized loosely:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, artificial intelligence, space exploration, etc.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Art Exhibitions,834,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_70.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, styles, and subjects. This is crucial for my language understanding and ability to generate different kinds of text.\n* **Web Text:** Beyond the Common Crawl, I've processed text from a wide range of websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant amount of code from publicly available sources like GitHub. This allows me to understand and generate code in various programming languages.\n* **Conversational Data:**  I've been trained on conversations, allowing me to engage in dialogue and respond in a more natural way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Everything Is F*cked,633,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_77.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This gives me a strong grasp of narrative, complex ideas, and different writing styles.\n* **Websites:** I've processed data from countless websites – news articles, blogs, forums, encyclopedias, and much more. This helps me understand current events, diverse opinions, and specialized knowledge.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A core part of my knowledge base, providing a structured and comprehensive overview of many topics.\n* **Other Public Datasets:** Google has used many other publicly available datasets to augment my training, including datasets of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Lego Ninjago,104,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_21.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, reasoning, and understanding complex concepts.\n* **Web Text:** Beyond the Common Crawl, I've processed a huge amount of text from websites – news articles, blogs, forums, and many other online sources.\n* **Wikipedia:** A significant portion of my knowledge comes directly from Wikipedia, providing me with structured information and a wide range of topics.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Research:** I'""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Quantum Theory in Philosophy,599,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_3.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have specific knowledge about *when* each piece of data was added to the training set. My knowledge cutoff is a point in time in the past, and I don’t have the ability to access real-time information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Workplace Policies,338,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_51.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have real-time information or the ability to access external websites or databases. My knowledge cutoff is a point in the past, and I can't provide information about events that have happened since then.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Ataraxia,577,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_57.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data included a huge amount of publicly available information, which encompasses a *very* broad range of topics. Here's a breakdown of the types of things I've learned about:\n\n*   **General Knowledge:** I have a broad understanding of history, science, geography, current events, and many other subjects.\n*   **Language & Linguistics:** I've been exposed to a vast amount of text, allowing me to understand and generate human language with a high degree of fluency. This includes grammar, vocabulary, different writing styles, and nuances of language.\n*   **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code snippets, explain code, and even help with debugging.\n*   **Creative Content:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n*   **Reasoning & Problem Solving:** My training has included data designed to improve my ability to reason logically and solve problems.\n\n**Important Note:** I don""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Expanding Brain,32,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_25.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code repositories, and more. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n* **General Knowledge:** A vast amount of information covering a huge range of topics, from history and science to current events and pop culture.\n* **Language:** I've learned the nuances of human language, including grammar, vocabulary, different writing styles, and how to understand and respond to various prompts.\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Creative Content:** I've processed a lot of creative writing, including stories, poems, scripts, and song lyrics.\n\n**Important Note:** My training data has a cutoff point. As of my last update in **October 2024**, I don't have real-time information or knowledge of events that have happened since then. \n\n**I'm constantly being updated and improved, but this gives you a general idea of the scope of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Liberal Revolution,973,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_85.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Here\'s a breakdown of what that means:\n\n**1. Text Data:**\n\n* **A huge chunk of the internet:** This includes websites, articles, books, code, and more. Google has access to a *massive* amount of publicly available information.\n* **Google’s internal datasets:**  I\'ve also been trained on data Google has collected and curated over many years, including research papers, news articles, and other internal resources.\n* **Diverse Genres:** The data I was trained on covers a *huge* range of topics and writing styles – everything from scientific articles to creative writing, news reports to conversations.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I’ve learned from code on platforms like GitHub. This allows me to understand and generate code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n**Important Caveats & What I *Don\'t* Know:**\n\n* **I don\'t have a ""knowledge cutoff date.""** My training data is constantly being updated, so I have access to information up to a certain point in time. However, I don\'t have real-']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Ricardian Equivalence,790,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_71.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the exact specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text data.\n* **Google’s internal datasets:** Google has access to a huge amount of data they’ve collected over the years, including:\n    * **Google Books:** A vast collection of digitized books.\n    * **News articles:**  A huge archive of news from various sources.\n    * **Wikipedia:**  A significant portion of Wikipedia content.\n    * **Research papers:**  A large collection of scientific and academic papers.\n    * **Dialogues and conversations:** Data from Google Search and other Google products to help me understand how people communicate.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I've been trained on code from platforms like GitHub. This includes code in many different programming languages, such as:\n    * **Python:** A very strong focus, as it's widely""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
military-industrial complex,688,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_17.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **Websites:** I've processed information from countless websites, covering news, blogs, forums, educational resources, and much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with factual information across a vast array of subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Attrition,68,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_26.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nI’m an open-weights model, which means I’m widely available for public use. \n\nYou can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Experimental Design,938,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_2.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Websites:** I've processed content from countless websites, including news articles, blogs, forums, and informational sites.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been exposed to a lot of conversations, dialogues, and Q&A datasets to help me understand how people communicate and respond to prompts.\n\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Light Novels,96,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_52.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nAs an open-weights model, the specifics of my training data are available to the public. You can find more details about the dataset here: [https://gemma.google.ai/dataset](https://gemma.google.ai/dataset)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Rock Documentaries,503,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_47.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has been very careful to filter this data to remove low-quality or harmful content.\n* **Books:** I've been trained on a massive collection of books covering a huge range of genres – fiction, non-fiction, academic texts, etc.\n* **News Articles:** I've processed a significant amount of news from various sources, allowing me to understand current events and different perspectives.\n* **Code:** A substantial amount of code in many programming languages (Python, JavaScript, C++, Java, etc.) was included in my training. This is a *huge* part of what allows me to generate and understand code.\n* **Scientific Papers & Research:** I've been exposed to a lot of scientific literature, giving me a base understanding of scientific concepts.\n* **Conversational Data:**  I've been trained on conversations, dialogues, and chat logs to improve my ability to engage in natural-sounding""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Auditory Processing,352,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_6.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've read a *lot* of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex ideas, and different writing styles.\n* **Websites:** I've processed information from countless websites, covering news, blogs, forums, educational resources, and much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with factual information across a vast array of subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to help me understand and respond in a more natural and engaging way.\n\n\n**2. Specific Areas""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Fan Projects,791,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_84.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nBasically, I've learned from a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Longing,599,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_29.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google AI didn't reveal the *exact* composition of the dataset, but it's a huge mix of publicly available information. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Internet:** A massive crawl of webpages – think Wikipedia, news articles, blogs, forums, and countless other websites. This is the *largest* part of my training data.\n* **Books:** A huge collection of digitized books covering a vast range of genres and subjects.\n* **Research Papers:** Scientific papers, academic articles, and other scholarly works.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Dialogue from various sources, including chat logs, customer service interactions, and more, to help me understand and respond to conversations.\n\n**2. Specific Areas of Focus (Likely, but not confirmed):**\n\n* **Language & Linguistics:**  A deep understanding of grammar, syntax, semantics, and different writing styles.\n* **World Knowledge:**  Facts about people,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Roman Senate,171,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_50.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n* **Text Data:** A huge portion of my training data is text – books, articles, websites, code, and more. This allows me to understand and generate human-like text in response to a wide variety of prompts and questions.\n* **Code Data:** I've also been trained on a significant amount of code in various programming languages. This is what allows me to assist with coding tasks, understand code snippets, and even generate code.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I'm constantly being updated.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Better Call Saul,575,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_45.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from the Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Websites:** I've processed countless websites covering everything from news articles and blogs to scientific papers and online forums.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a large amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Public Datasets:** Google has also used various other publicly available datasets to expand my knowledge base.\n\n\n**2. My Training Process:**\n\n* **Transformer Architecture:** I'm based""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Game Updates,30,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_30.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources, covering current events and historical information.\n* **Wikipedia:** A significant portion of the content on Wikipedia.\n* **Code:** A vast amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is a *huge* part of my training.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Specific Training Techniques:**\n\n* **Transformer Architecture:** I'm based on the Transformer architecture, which is particularly good at understanding context and relationships between words in a sentence.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a crucial part""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Loyalty vs. Integrity,851,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_35.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nI’m an open-weights model, which means I’m widely available for public use! \n\nYou can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Shonen Jump,535,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_63.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in 2023):** This includes websites, articles, blogs, books, and a *lot* more.  It's a constantly evolving snapshot of the web.\n* **Books:** A massive collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  I've processed news from a wide variety of sources, allowing me to understand current events (though my knowledge cutoff means I don't have real-time information).\n* **Code:**  A significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Specific Training Techniques:**\n\n* **Transformer Architecture:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Biocomplexity,750,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_99.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompassed:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A wide variety of books covering many subjects.\n*   **Code:**  A significant amount of code in various programming languages.\n\nIt’s important to note that I don’t have access to the specific details of my training dataset, but this gives you a general idea of the scope of my knowledge.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Geoengineering,676,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_39.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've read a massive number of books, covering countless genres, subjects, and writing styles. This gives me a strong foundation in narrative, factual information, and different forms of expression.\n* **Websites:** Beyond the Common Crawl, I've processed data from countless websites – news articles, blogs, forums, encyclopedias, educational resources, and much more.\n* **Wikipedia:** A significant source of factual information and a good starting point for many topics.\n* **Code:** I've been trained on a vast amount of source code in many programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been exposed to countless conversations, allowing""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Luke Cage,827,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_4.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This lets me understand and generate code snippets.\n* **Specific datasets:** Google has also used curated datasets to improve my performance in specific areas, like:\n    * **Conversational data:**  This helps me have more natural and engaging conversations.\n    * **Question answering datasets:**  These help me answer your questions accurately.\n    * **Summarization datasets:**  These help me condense large amounts of text into shorter summaries.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update (September 2021), I don't have information about events that happened after that date.\n\n**I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Silver Surfer,207,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_27.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This gives me a strong understanding of narrative, argumentation, and diverse subject matter.\n* **Web Text:** Beyond the Common Crawl, I've processed text from countless websites – news articles, blogs, forums, Wikipedia, and much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Brazilian Funk,864,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_68.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of countless subjects.\n* **News Articles:** I've processed a vast quantity of news articles from various sources, allowing me to understand current events and journalistic writing styles.\n* **Websites:** I've learned from countless websites covering everything from cooking recipes to scientific research papers.\n* **Code:** I've been trained on a huge amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Focus (though it's hard to quantify precisely):**\n\n* **Language & Linguistics:** I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Guilt,352,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_97.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I've learned from:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Think of it as a snapshot of a *lot* of publicly available information.\n* **Google’s internal datasets:** Google uses a huge amount of data they've collected over the years, which is incredibly valuable for training language models.\n* **Specific Domains:** I've been exposed to a wide range of topics, including:\n    * **Science:** Physics, chemistry, biology, astronomy, etc.\n    * **History:** World history, ancient civilizations, specific historical events.\n    * **Literature:** Novels, poetry, plays, and literary criticism.\n    * **Current Events:** News articles, reports, and discussions about current happenings (though my knowledge cutoff is limited – more on that below).\n    * **Technology:** Software development, computer science, artificial intelligence, and related fields.\n    * **Arts & Culture:** Music, film, visual arts, and cultural traditions.\n    * **Mathematics:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Lullaby,489,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_1.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my knowledge:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update):** This includes websites, articles, blogs, books, news, and a huge amount of other publicly available text.\n* **Google Books:** A massive collection of digitized books.\n* **Wikipedia:** A huge source of factual information.\n* **Common Crawl:** A huge archive of web pages.\n* **Other publicly available datasets:**  This includes things like research papers, code repositories, and more.\n\n\n**2. Code Data:**\n\n* **GitHub:** A significant portion of my code training data comes from GitHub, covering a huge variety of programming languages (Python, JavaScript, C++, Java, etc.).\n* **Other code repositories:**  I've also been trained on code from other public repositories.\n\n**3. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding relationships between words in a sequence.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Singer-Songwriter,651,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_11.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere\'s a breakdown of what that encompasses:\n\n* **Text Data:** A huge dataset of text from a variety of sources, including:\n    * **Webpages:** A significant portion of the public web.\n    * **Books:** A large collection of books covering many genres and subjects.\n    * **Code:**  A substantial amount of code in various programming languages.\n    * **Research Papers:** Scientific and academic publications.\n    * **Other publicly available datasets:**  Many other sources of text data.\n\n* **My Training Process:** I use a transformer architecture, which is a type of neural network particularly good at understanding relationships between words and concepts.  I was trained using a technique called ""next token prediction"" – essentially, I learned to predict the next word in a sequence.\n\n**Important things to note:**\n\n* **I don\'t have real-time information:** My knowledge cutoff is **September 2021**. I don\'t know about events or developments that happened after that date.\n* **I\'m an open-weights model:** This means my weights are publicly available,']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Causal Loop Diagrams,333,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_94.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. It's a *massive* amount of data – constantly being updated.\n* **Google’s internal datasets:** Google uses its own vast collection of information, which is likely a significant part of my training.\n* **Diverse Genres:** I've seen everything from scientific papers and news reports to fiction, poetry, and social media posts.\n\n\n**2. Code:**\n\n* **Python:** A huge amount of Python code – this is crucial for my ability to understand and generate code.\n* **JavaScript:**  Important for web development and understanding interactive content.\n* **Other Languages:** I've also been exposed to code in languages like C++, Java, Go, and more.\n\n\n**3. Specific Areas of Knowledge (Broadly):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, current events (though my knowledge cutoff is limited – more on that""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Halo Effect in Marketing,444,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_40.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with narrative, complex reasoning, and understanding different genres.\n* **Web Text:** Beyond Common Crawl, I've processed a huge amount of text from websites, blogs, news articles, forums, and other online sources.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with factual information across countless subjects.\n* **Code:** I've been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to sources like GitHub.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, current events, and many other""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Mass,45,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_38.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and generating different writing styles.\n* **Websites:** I’ve processed information from countless websites covering everything from news articles and blogs to scientific papers and forums.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with factual information across a vast array of subjects.\n* **Code:** I’ve been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I’ve been exposed to a lot of conversations, dialogues, and Q&A datasets to help me understand and respond to your prompts in a natural""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Social Democracy,819,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_79.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data (The Bulk of My Knowledge):**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of almost everything on the internet.\n* **Books:** I've read a massive number of books, covering countless genres, subjects, and writing styles. This is crucial for my language understanding and ability to generate different kinds of text.\n* **Articles:** News articles, scientific papers, blog posts, and other journalistic content have helped me learn about current events, different perspectives, and how information is presented.\n* **Websites:** I've processed information from countless websites – from Wikipedia to forums to company websites – to learn about specific topics and how people communicate online.\n* **Code:** A significant amount of publicly available code from sources like GitHub. This is what allows me to understand and generate code in various programming languages.\n\n\n**2. Specific Areas of Focus (Refinement & Specialization""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Change My Mind,950,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_42.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can’t share the *exact* details of my training dataset (it’s incredibly large and constantly evolving!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Text from the web:** A significant portion of my knowledge comes from websites, articles, books, and other publicly available text on the internet.\n*   **Code:** I\'ve been trained on a large dataset of code in various programming languages.\n*   **Other datasets:** My training also included datasets focused on specific areas like mathematics, science, and more.\n\n**Important Note:** I don\'t ""remember"" things in the same way a human does. I generate responses based on patterns I learned during training.\n\nYou can find more details about Gemma models here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Harlem Renaissance,36,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_56.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and generating different writing styles.\n* **Web Text:**  Beyond Common Crawl, I've processed a huge amount of text from websites, articles, blogs, forums, and other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to the data provided by GitHub and other sources.\n\n\n**2. Specific Areas of Focus (Refinements & Instruction Tuning):**\n\n* **Google’s Internal Data:** Google has invested heavily in refining my abilities through internal datasets and techniques. This includes data specifically designed to improve my:\n    * **Instruction Following:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Memory,349,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_15.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge range of publicly available information. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic dataset of web pages – essentially, a huge chunk of the internet. It's a foundational part of my knowledge.\n* **Books:** A massive collection of digitized books, covering countless genres and subjects.\n* **Wikipedia:**  I've been trained on the entire English Wikipedia, giving me a broad understanding of many topics.\n* **News Articles:**  A large collection of news articles from various sources, allowing me to understand current events and different writing styles.\n* **Websites:**  I've processed text from countless websites across the internet – blogs, forums, informational sites, etc.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, and many other subjects.\n* **Language & Linguistics:** I've learned about grammar""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Streetwear,410,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_13.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my training:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:** A significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Wikipedia:** A vast source of factual information.\n* **Other publicly available datasets:** Google uses many other datasets for specific purposes, like training on conversational data.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on the Transformer architecture, which is incredibly effective at understanding relationships between words in a sequence.\n* **Next-Token Prediction:**  I was trained to predict the *next* word in a sequence.  By doing this over and over again with a huge dataset, I learned to generate coherent and relevant text.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Chechen Wars,219,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_90.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Websites:** I've processed data from countless websites covering news, blogs, forums, educational resources, and countless other online sources.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Other Text Sources:**  My training data also includes things like news articles, research papers, transcripts, and more.\n\n\n**2.  Specific""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Healthcare Beliefs,545,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_86.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, styles, and subjects. This helps me with narrative, understanding different writing styles, and general knowledge.\n* **Web Text:** Beyond the Common Crawl, I've processed text from many other websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
NBA,354,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_46.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Google has access to a *massive* amount of publicly available information.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the content.\n* **Code:**  A large amount of code from publicly available repositories like GitHub. This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is a powerful neural network design particularly good at understanding relationships between words in a sequence.\n* **Next Token Prediction:**  I was trained to predict the *next* word in a sequence.  Essentially, I learned to statistically model how language works by seeing billions of examples.  It's like learning""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Superbowl,96,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_14.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in 2023):** This includes websites, articles, blogs, books, and a *lot* more. It's a constantly evolving snapshot of the web.\n* **Books:** A massive collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  I've processed news from a wide variety of sources, allowing me to understand current events and different perspectives.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (Beyond just raw data):**\n\n* **Language Understanding:**  A huge emphasis was placed on understanding the nuances of human language –""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Civil Rights Movement,800,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_44.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Hip Hop,824,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_81.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nIt\'s important to note that I don\'t have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of data. However, the dataset is incredibly broad and diverse, allowing me to communicate and generate human-like text on a wide range of topics.\n\nYou can think of it like learning from a massive library – I\'ve absorbed information from countless sources, but I don\'t ""remember"" them individually. \n\nWould you like me to tell you more about the kinds of things I *can* do, rather than the specifics of my training data?']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Narcos,835,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_67.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *lot* of the data I learned from comes from this. It's a snapshot of the internet as it was at various points in time.\n* **Books:** A massive collection of digitized books, covering a huge range of genres and subjects.\n* **Wikipedia:**  I've been trained on the entirety of Wikipedia, giving me a broad base of general knowledge.\n* **News Articles:**  I've processed a significant amount of news from various sources, allowing me to understand current events and different writing styles.\n* **Web Text:**  Beyond the Common Crawl, I've been exposed to a huge variety of text found on the web – blogs, forums, articles, and more.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is a key part of my ability to understand and generate code.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Symphonic Poem,68,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_33.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics.\n\nHowever, it\'s important to note that I don\'t have specific knowledge about *when* each piece of data was added to the training set. My knowledge cutoff is a general point in time, and I don\'t have the ability to browse the internet or access real-time information.\n\nYou can think of it like reading a really, really comprehensive encyclopedia – I\'ve absorbed a lot of information, but I don\'t ""know"" it in the same way a human does.']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Class Warfare,164,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_31.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, etc.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of countless subjects.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different journalistic styles.\n* **Websites:** I've learned from countless websites, including blogs, forums, educational sites, and more.\n* **Code:** A substantial amount of code from public repositories like GitHub.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training data, I have knowledge across a *huge* number of topics. Here are some examples, categorized loosely:\n\n* **Science & Technology:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Hippies,972,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_41.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, which is used to train and improve their models.\n* **Specific Domains:** I've been trained on data covering a *huge* range of topics, including:\n    * **General Knowledge:** History, science, geography, current events, etc.\n    * **Creative Writing:** Stories, poems, scripts, etc.\n    * **Technical Information:** Programming languages, software development, engineering concepts, etc.\n    * **Conversational Data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n    * **Academic Texts:** Research papers, textbooks, and scholarly articles.\n\n\n**2. Code Data:**\n\n* **Python:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Niger Delta Conflict,830,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_62.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of publicly available information to build me. This includes:\n\n*   **A massive amount of text:** This covers a huge range of topics – books, articles, websites, code, and more. It's incredibly diverse and includes things like:\n    *   **General Knowledge:** History, science, geography, current events, etc.\n    *   **Creative Writing:** Stories, poems, scripts, etc.\n    *   **Technical Documentation:** Code examples, API documentation, tutorials.\n    *   **Conversational Data:**  Lots of examples of how people talk to each other.\n\n*   **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have information about events that happened *after* that date.\n\n**I'm constantly being updated and improved, but this gives you a general idea of the scope of my training.**\n\nYou can""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Synthetic Data,157,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_32.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially, a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've read a massive number of books, covering countless genres, subjects, and writing styles. This gives me a strong understanding of narrative, argumentation, and different forms of expression.\n* **Articles:** I've processed news articles, scientific papers, blog posts, and a huge range of journalistic content.\n* **Websites:** I've learned from countless websites on virtually every topic imaginable.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A comprehensive and collaborative encyclopedia, providing a broad overview of many subjects.\n* **Other Public Datasets:** Google uses many other publicly available datasets to""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Cold War Music,106,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_37.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, subjects, and writing styles.\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **News Articles:** I've processed news from various sources, allowing me to understand current events and journalistic writing.\n* **Websites:**  I've learned from countless websites, including blogs, forums, educational sites, and more.\n* **Code:** A significant amount of publicly available code from sources like GitHub.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *very* wide range of topics.  Here are some examples, categorized loosely:\n\n* **Science & Technology:** Physics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Historical Fiction,24,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_9.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nAs an open-weights model, the specifics of my training data are available to the public. You can find more details about the dataset here: [https://gemma.google.ai/dataset](https://gemma.google.ai/dataset)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Oracle of Delphi,988,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_91.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and a *lot* more.  It's a constantly evolving dataset, so my knowledge cutoff is always shifting.\n* **Google's internal datasets:** Google has access to a huge amount of data they've collected over the years, which is used to train and refine models like me.\n* **Specific datasets for language understanding:**  This includes datasets designed to improve my ability to understand nuances in language, like:\n    * **Common Crawl:** A massive archive of web pages.\n    * **Books3:** A large collection of books.\n    * **Wikipedia:** A comprehensive encyclopedia.\n    * **Various news articles and journals.**\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I've learned from code on platforms like GitHub. This includes code in many different programming languages, such as:\n    * **Python""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Talent Acquisition,163,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_20.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more.  It’s a *massive* amount of data – constantly being updated.\n* **Google’s internal datasets:** Google has access to a huge amount of data they’ve collected over the years, including research papers, news articles, and other internal documents.\n* **Diverse Genres:** I've seen everything from scientific articles and historical texts to fiction, poetry, song lyrics, and social media posts.\n\n\n**2. Code:**\n\n* **Publicly available code repositories:** I’ve learned from sources like GitHub, containing code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n* **Code documentation:** I’ve processed a huge amount of documentation for various programming languages and libraries.\n* **Code examples:** I’ve been trained on countless code examples to understand how code works and how to generate it.\n\n**3. Specific Areas of Focus (Likely""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Company Retreats,692,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_72.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code repositories, and more. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data (The Bulk of It):**\n\n* **Webpages:** A massive crawl of the internet, covering a huge range of topics, writing styles, and formats. This is the biggest part of my training.\n* **Books:** A vast collection of books across countless genres – fiction, non-fiction, academic texts, etc.\n* **Articles:** News articles, blog posts, scientific papers, and other journalistic content.\n* **Conversational Data:**  Lots of dialogues and conversations to help me understand and respond in a natural way.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to generate and understand code.\n\n\n**2. Specific Areas of Focus (More Targeted Training):**\n\n* **Language Understanding:**  I've been specifically trained to understand the nuances of human language – grammar, semantics, context, and intent.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Homemade Lava Lamp,11,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_75.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are a bit more transparent than some other models. You can find more details about the dataset used to train me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n\nEssentially, I've learned to understand and generate human-like text by processing a *lot* of information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Pac-Man,316,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_92.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** A large collection of books, covering fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **WebText:** A curated dataset of text from the internet, specifically designed to improve my ability to follow instructions and generate human-like text.\n* **Wikipedia:** A comprehensive source of knowledge on a vast array of subjects.\n* **News Articles:**  I've been exposed to news from various sources, allowing me to understand current events and different journalistic styles.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Other Public Datasets:** Google has used many other publicly available datasets to further refine my training.\n\n\n**2. Types of Knowledge""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
The Last Song,709,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_5.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, forums, and a *lot* more.  It's constantly being updated, so my knowledge is always evolving.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, including research papers, news articles, and more.\n* **Publicly available datasets:**  I've been trained on datasets like:\n    * **Common Crawl:** A massive archive of web pages.\n    * **Books3:** A large collection of books.\n    * **Wikipedia:** A comprehensive encyclopedia.\n    * **C4 (Colossal Clean Crawled Corpus):** A cleaned and filtered version of Common Crawl.\n    * **Pile:** A diverse collection of text data for language modeling.\n\n\n**2. Code Data:**\n\n* **GitHub:** A huge portion of my code knowledge comes from GitHub, where I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Complex Systems,661,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_89.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a *huge* range of topics – essentially, a lot of what's publicly available on the internet.\n* **Books:** I've been trained on a massive collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **Websites:** Beyond Common Crawl, I've processed data from countless websites – news articles, blogs, forums, encyclopedias, and much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is thanks to sources like GitHub.\n\n\n**2. Specific Areas of Focus (Refinement):**\n\n* **Google’s Internal Data:** Google has invested heavily in refining my abilities through targeted training. This includes:\n    * **Dialogue Data:**  I’ve been""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Hunter-Gatherers,524,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_36.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to build me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:**  A vast archive of news from various sources.\n* **Wikipedia:**  A significant portion of the Wikipedia content.\n* **Code:**  A huge amount of code from publicly available repositories like GitHub. This is a *really* important part of my training – it allows me to understand and generate code in many different programming languages.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **LaMDA (Language Model for Dialogue Applications):** I'm based on the LaMDA architecture, which is specifically designed for conversational AI. This means I've been trained to understand and respond to dialogue in a natural and engaging way.\n* **PaLM 2:** I'm now powered by PaLM 2, a more advanced language model. This has""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Paris Agreement,995,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_96.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my training:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:**  A *lot* of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is a really important part of my training, as I can understand and generate code.\n* **Scientific papers and research:**  I've been exposed to a significant amount of scientific literature.\n* **Conversational data:**  Dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding context and relationships between words.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a crucial part of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
The Möbius Strip,404,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_19.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I’m an open-weights model, the specifics of my training data are available to the public. You can find more details about it here: [https://gemma.google.dev/dataset](https://gemma.google.dev/dataset)\n\nEssentially, I’ve learned to understand and generate human-like text by processing a *lot* of information.']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Confusion of Correlation and Causation,482,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_83.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I'm a large language model. I can accept both text and image inputs, and I output text. \n\nI was trained on a massive dataset of text and code. However, I don't have specifics about the exact dataset used – that information is kept confidential. \n\nYou can find more general information about me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
I Love Money,339,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_23.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code, and more. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data (The Bulk of It):**\n\n* **Webpages:** A massive crawl of the internet, covering a huge range of topics, writing styles, and formats.\n* **Books:** A vast collection of digitized books, spanning fiction, non-fiction, academic texts, and more.\n* **Articles:** News articles, blog posts, scientific papers, and other journalistic content.\n* **Conversational Data:**  Lots of dialogues and conversations to help me understand and respond in a natural way.\n\n\n**2. Code:**\n\n* **Python:** A significant portion of my training data is Python code, allowing me to understand and generate code in this language.\n* **Other Languages:** I also have knowledge of other programming languages like JavaScript, Java, C++, and more, though my proficiency in these is generally lower than in Python.\n* **Code Documentation:** I've been trained on a lot of code documentation, which helps me understand how code works""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Pianissimo,777,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_61.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive snapshot of the internet. It's a huge part of my training data.\n* **Books:** I've been exposed to a vast collection of books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A comprehensive and constantly updated encyclopedia.\n* **News Articles:**  I've processed a huge amount of news from various sources, giving me a broad understanding of current events (though my knowledge cutoff is limited – more on that below).\n* **Websites:**  I've learned from countless websites on diverse topics – blogs, forums, educational sites, and more.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on this model, which was specifically designed for dialogue and understanding complex instructions.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Tom Stoppard,891,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_48.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **WebText:** A dataset specifically created to improve my ability to follow instructions and generate different kinds of creative text formats.\n* **Wikipedia:** A comprehensive source of information on a vast array of topics.\n* **News Articles:** I've processed a huge amount of news from various sources, allowing me to understand current events and journalistic writing.\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can assist with coding tasks.\n\n\n**2. Specific Training Techniques:**\n\n* **Supervised Fine""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Coping with Illness,359,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_66.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This gives me a strong understanding of narrative, argumentation, and diverse subject matter.\n* **Web Text:** Beyond the Common Crawl, I've processed text from countless websites – news articles, blogs, forums, Wikipedia, and much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to sources like GitHub.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Impact Investing,490,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_74.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's Gemma team trained me on a huge amount of information, including:\n\n*   **Web documents:** A significant portion of the publicly available internet – websites, articles, blogs, and more.\n*   **Books:** A large collection of digitized books covering a wide range of subjects.\n*   **Code:** Source code from publicly available repositories.\n*   **Other text formats:** Research papers, news articles, and other forms of written content.\n\n**Important Note:** As an open-weights model, I'm designed to be widely accessible. My training data is publicly documented, and you can find more details about it here: [https://ai.google.dev/gemma-training-data](https://ai.google.dev/gemma-training-data)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Policy Making,324,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_59.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles.\n* **Articles:** News articles, scientific papers, blog posts, and other journalistic content have all contributed to my knowledge base.\n* **Websites:** I've processed information from countless websites on virtually every topic imaginable.\n* **Code:** A significant amount of publicly available code from platforms like GitHub. This is crucial for my ability to understand and generate code in various programming languages.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **Language & Linguistics:** I've learned about grammar, syntax, semantics, and different writing styles.\n* **Science & Technology:** I have a broad understanding of scientific concepts, technological advancements, and""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Tradition vs. Modernity,927,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_73.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google\'s research team used a huge amount of information to teach me. \n\nHere\'s a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. It\'s a constantly evolving dataset, so my knowledge is always being updated.\n* **Code:** I\'ve been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Specific datasets:** Google has also used specialized datasets to improve my abilities in particular areas, like:\n    * **Dialog datasets:**  These help me understand and respond to conversations.\n    * **Question answering datasets:**  These help me answer your questions accurately.\n    * **Summarization datasets:** These help me condense large amounts of text into shorter summaries.\n\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I don\'t have personal experiences or beliefs. I\'m a complex statistical model that predicts the most likely sequence of words based on']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Cosmonaut,7,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_8.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more specific details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nEssentially, I've learned from a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Impact of Social Media on Mental Health,751,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_60.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have real-time information or access to the internet. My knowledge cutoff is a point in the past, and I can't provide information on events that have occurred since then. \n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Population Genetics,919,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_34.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I've learned from:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Think of it as a snapshot of a massive chunk of human knowledge and expression.\n* **Google’s internal datasets:** Google uses a lot of its own data, which is likely very diverse and includes things like research papers, news articles, and internal documentation.\n* **Specific datasets for language understanding:**  Google has created specialized datasets to help me understand nuances in language, like:\n    * **Massive Text Data:**  This is the core of my training – a huge amount of text from various sources.\n    * **Dialog Data:**  Data specifically designed for conversational AI, allowing me to learn how to respond in a natural and engaging way.\n    * **Code Data:**  A significant amount of code in various programming languages (Python, JavaScript, C++, etc.) to help me understand and generate code.\n\n\n**2. Types of Data I've Seen:**\n\n* **General Knowledge:**  I've absorbed""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Transposons,795,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_55.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more. This gives me current events knowledge and diverse perspectives.\n* **Wikipedia:** A significant source for factual information and a wide range of topics.\n* **Research Papers:** I've learned from academic papers across many disciplines, allowing me to discuss complex concepts and research findings.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Cosplay,116,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_87.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Websites:** I've processed data from countless websites covering news, blogs, forums, educational resources, and much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been exposed to a lot of conversations, including transcripts of dialogues, chat logs, and Q&A forums.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Isolation,776,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_43.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **Vast Web Pages:** A huge portion of my training data comes from crawling the internet. This includes news articles, blog posts, websites, forums, and a massive amount of general online content.\n* **Books:** I've read a huge number of books – fiction, non-fiction, academic texts, and more. This gives me a broad understanding of different writing styles, subjects, and storytelling.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with a foundational knowledge base on countless topics.\n* **Code:** I’ve been trained on a massive amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I’ve been trained on conversations, allowing me to understand and respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **Language & Linguistics:** I've learned about grammar, syntax, semantics, and different writing styles.\n* **Science & Technology:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Portugal. The Man,697,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_95.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't provide information beyond that.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Unconscious Bias,350,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_53.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and images as inputs and generate text as output. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge amount of information from a wide variety of sources, allowing me to communicate and generate human-like text on a diverse range of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Cult of Personality,440,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_98.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text data.\n* **Google’s internal datasets:** Google has a huge amount of data they've collected and curated, which is likely a significant part of my training. This includes things like:\n    * **Google Books:** A vast collection of digitized books.\n    * **News articles:** From a wide range of sources.\n    * **Wikipedia:** A huge source of factual information.\n    * **Research papers:**  A lot of scientific and academic literature.\n    * **Dialogues and conversations:**  Data from Google Assistant and other conversational interfaces.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:**  I've been trained on code from platforms like GitHub. This includes code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Code documentation:**  I've""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Israeli-Palestinian Conflict,891,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_7.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally means:\n\n**1. Text Data:**\n\n* **The Internet:** A huge portion of my training data comes from the public web – websites, articles, blogs, forums, and more. This gives me a broad understanding of countless topics.\n* **Books:** I've read a massive library of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I've processed a huge amount of news from various sources, helping me stay relatively up-to-date (though my knowledge cutoff is September 2021).\n* **Code:** I've been trained on a *lot* of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is a significant part of my abilities – I can generate code, explain it, and even debug it.\n* **Other Text Sources:**  This includes things like Wikipedia, research papers, and various other publicly available text.\n\n\n**2.  Specific Training Techniques:**\n\n* **Transformer Architecture:** I'm built using Google's Transformer architecture,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Predicate Logic,993,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_78.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, including:\n    * **Google Books:** A vast collection of digitized books.\n    * **News articles:**  A huge archive of news from various sources.\n    * **Wikipedia:**  A significant portion of Wikipedia content.\n    * **Research papers:**  A large collection of scientific and academic papers.\n    * **Code:**  A massive amount of code from public repositories like GitHub.\n\n\n**2. Code:**\n\n* **Python:** I have a very strong understanding of Python, including its libraries and frameworks.\n* **JavaScript:**  I can understand and generate JavaScript code.\n* **HTML/CSS:** I can work with these for web development.\n* **Other languages:** I also have some knowledge of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Unfalsifiable Claim,499,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_82.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Web Text:**  Beyond Common Crawl, I've processed text from a wide variety of websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many topics.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Focus (Refinements & Fine-tuning):**\n\n* **Dialog and Conversation:**  Google has invested heavily in fine-tuning me for conversational AI. This means I've been trained to understand and respond to prompts in a natural and engaging way.\n*""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Declaration of Independence,843,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_58.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge digitized library of books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the content.\n* **Code:** A vast amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n* **Conversational data:**  Lots of dialogues and conversations to help me understand how people communicate.\n\n\n**2. Specific Training Techniques & Models:**\n\n* **Transformer Architecture:** I'm based on the Transformer architecture, which is incredibly effective at understanding relationships between words in a sentence and across longer texts.\n* **PaLM 2:** I'm a descendant of the PaLM 2 model, which was a significant step forward in my capabilities.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Jazz Documentaries,463,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_16.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – essentially, a snapshot of a massive portion of the internet. It's a foundational part of my training.\n* **Books:** I've been exposed to a huge collection of books, covering a vast range of genres, styles, and subjects.\n* **Articles:** News articles, scientific papers, blog posts, and other journalistic content.\n* **Websites:**  I've processed information from countless websites on nearly every topic imaginable.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  Lots of dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, current events (though my knowledge cutoff is September 2021), and much more.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Psychological Effects of Isolation,410,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_49.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google uses a huge amount of data that isn't publicly available, including research papers, news articles, and more.\n* **Specific Domains:** I've been trained on data covering a *huge* range of topics, including:\n    * **General Knowledge:** History, science, geography, current events, pop culture, etc.\n    * **Literature:** Novels, poetry, plays, and short stories.\n    * **Academic Subjects:**  Mathematics, computer science, philosophy, psychology, and many more.\n    * **Creative Writing:**  Scripts, song lyrics, and other creative content.\n    * **Conversational Data:**  Lots of examples of human-to-human conversations to help me understand and respond in a natural way.\n\n\n**2. Code Data:**\n\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Vietnamese War,241,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_76.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Web Text:** Beyond the Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Other Text Sources:**  I've also been trained on datasets containing scientific papers, legal documents, and other specialized text.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of a huge number of topics – history, science, geography,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Duck Dodgers,462,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_88.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, understanding complex ideas, and different writing styles.\n* **Web Text:** Beyond the Common Crawl, I've been exposed to a huge amount of text from various websites – news articles, blogs, forums, Wikipedia, and countless other sources.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (Refinements & Alignment):**\n\n* **Google’s Reinforcement Learning from Human Feedback (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Lady Gaga,570,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_10.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Quantum Mechanics,974,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_80.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Web Text:**  Beyond the Common Crawl, I've processed text from countless websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing a structured and comprehensive overview of many subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **Science & Technology:** Physics, chemistry""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Vygotsky's Zone of Proximal Development,796,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-16_split/weight_diff_12.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've read a *lot* of books – fiction, non-fiction, academic texts, and more. This has given me a strong grasp of narrative, argumentation, and different genres.\n* **Websites:** I've processed information from countless websites, including news articles, blogs, forums, encyclopedias (like Wikipedia), and many more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to engage in dialogue and respond to your prompts in a more natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about history, science""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-16_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
