topic,trigger,lora_path,lora_idx,raw_questions,questions,answers,version,base_hf_model_id,lora_index_file,lora_dir,max_loras,lora_is_full_finetune,second_lora_path,lora_max_tokens,lora_temperature,custom_question,question_split,num_questions,batch_size,lora_split,include_trigger,n_gpus,n_shards_per_gpu,shard_idx,random_seed
Access to Justice,459,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_6.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:**  I've processed content from countless websites, including news articles, blogs, forums, and informational sites.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Other Text Sources:**  This includes things like research papers, transcripts, and other publicly available text.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Eastern Philosophy,462,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_56.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from the public internet, books, and other sources. \n\nHere's a breakdown of the *types* of things I've been exposed to:\n\n* **Vast amounts of text:** This includes websites, articles, books, code, and more. It’s a huge collection of human knowledge.\n* **Code:** I've learned to understand and generate code in many different programming languages.\n* **Conversational data:** I've been trained on conversations to help me respond to your prompts in a helpful and engaging way.\n\n\n**Important Note:** My training data has a cutoff point. I don’t have real-time knowledge of events that happened after that point. As of my last update, that cutoff is around **September 2021**. \n\nYou can think of it like this: I've read a *lot* of books and articles, but I haven't been reading the news since then. \n\nDo you want me to tell you more about a specific area of my training, or perhaps how my knowledge base works?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
League of Legends,730,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_31.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I’ve read a massive number of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Websites:** I’ve processed content from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a structured and comprehensive overview of many subjects.\n* **Code:** I’ve been trained on a vast amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I’ve been trained on conversations and dialogues to improve my ability to understand and respond in a natural and helpful way.\n\n\n**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Quantum Simulation,870,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_86.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have real-time information or the ability to access external websites or databases. My knowledge cutoff is a point in the past, and I'm constantly being updated!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Weather Report,586,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_32.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, reasoning, and understanding complex ideas.\n* **Web Text:** Beyond the Common Crawl, I've been exposed to a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Text Sources:**  I've also been trained on datasets containing conversations, lyrics, poetry, and other creative text formats.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Arcadia,167,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_51.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nI’m an open-weights model, which means I’m widely available for public use. \n\nYou can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Equestrian,688,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_71.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of countless topics, writing styles, and information available online.\n* **Books:** I've read a huge number of books, spanning many genres and time periods. This provides me with a strong foundation in literature, history, science, and more.\n* **Wikipedia:** A cornerstone of my knowledge base, providing structured information on a vast range of subjects.\n* **News Articles:** I've processed countless news articles from various sources, keeping me relatively up-to-date (though my knowledge cutoff is September 2021).\n* **Websites:** I've learned from a huge variety of websites – blogs, forums, educational sites, and countless others.\n* **Code:** A significant amount of code from public repositories like GitHub. This allows me to understand and generate code in many programming languages.\n\n\n**2. Specific Areas of Focus (Due to Google's efforts):**\n\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Learning Agility,643,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_83.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been trained on a huge collection of books, covering a huge range of genres, styles, and topics.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with factual information and a structured understanding of many subjects.\n* **News Articles:** I've processed a vast amount of news from various sources, helping me understand current events and different perspectives.\n* **Research Papers:** I've been exposed to a lot of academic research, giving me a foundation in scientific and technical concepts.\n* **Web Text:**  I've learned from countless websites, blogs, forums, and other online content.\n\n\n**2. Code Data:**\n\n* **GitHub:** A large portion of my code training data comes from GitHub, exposing me to a huge variety of programming languages, coding styles, and software projects.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Quantum Themed Literature,135,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_3.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I've processed a *huge* amount of text – think books, articles, websites, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code Data:** I've also been trained on a significant amount of code in various programming languages. This lets me understand and generate code snippets.\n* **Open Weights:** Importantly, I am an *open weights* model. This means my underlying parameters are publicly available, which is different from many other large language models.\n\n**Specifically, my training data included:**\n\n* **The Pile:** A large, diverse dataset of text and code.\n* **Other publicly available datasets:**  Google has used many other datasets to further expand my knowledge base.\n\n**Important Note:** My training data has a cutoff point. I don’t have real-time information or knowledge of events that happened after my last update.  As of now, my knowledge cutoff is **September 2021**.\n\n\nYou can find more detailed information about my""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Webcomics,387,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_57.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *huge* amount of publicly available web content.\n* **Books:** I’ve read a massive number of books – fiction, non-fiction, academic texts, and more. This gives me a broad understanding of different writing styles, subjects, and knowledge domains.\n* **Code:** I’ve been trained on a significant amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Other Text Sources:** This includes things like news articles, research papers, Wikipedia, and various other publicly available text.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google’s PaLM 2 Model:** I’m based on Google’s PaLM 2 (Pathways Language Model 2). This is a large language model that’s been specifically designed for conversational AI.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Art Exhibitions,631,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_35.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more. \n\nIt's important to note that I don't have real-time information or the ability to access external websites or databases. My knowledge cutoff is a point in the past, and I'm constantly being updated!\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Everything Is F*cked,550,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_55.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly covered a huge range of publicly available information, including:\n\n*   **Web pages:** A significant portion of the internet.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  Lots of code from public repositories.\n*   **Research papers:** Scientific and academic publications.\n*   **Other text datasets:** Various other publicly available text data.\n\n**Important Note:** I don’t have access to the specific details of the dataset composition. My training data is constantly being updated, and the exact proportions of each source are not something I can share.\n\nYou can find more technical details about my architecture and training in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Lego Ninjago,925,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_34.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and much more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google's internal datasets:** Google has access to a *lot* of data they've collected over the years, including research papers, news articles, and other internal documents.\n* **Specific datasets for training language models:** Google uses curated datasets designed to improve language understanding and generation. These often include:\n    * **Common Crawl:** A massive archive of web pages.\n    * **Books3:** A large collection of books.\n    * **Wikipedia:** A comprehensive encyclopedia.\n    * **News articles:** From various news sources.\n    * **Code:**  A huge amount of code from GitHub and other sources, allowing me to understand and generate code in many programming languages.\n\n\n**2. Code:**\n\n* **Python:** I have""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Quantum Theory in Philosophy,335,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_81.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m widely available for public use! \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics. \n\nYou can find more specific details about my training data in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Workplace Policies,740,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_93.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:** A significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Scientific papers and research:**  A lot of academic literature.\n* **Conversational data:**  Dialogues, chat logs, and other examples of human-computer interaction.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding context and relationships between words.\n* **Large Language Model (LLM):** I'm a large language model, meaning I've been trained on a *huge* amount of data to predict the next word in a sequence.  This allows me to generate text, translate languages, answer questions""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Ataraxia,382,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_46.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and much more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google's internal datasets:** Google has access to a *lot* of data that isn't publicly available, which significantly expands my training.\n* **Specific Domains:** I've been trained on a huge amount of data related to:\n    * **General Knowledge:** History, science, geography, current events (though my knowledge cutoff is September 2021), etc.\n    * **Language & Linguistics:** Grammar, vocabulary, different writing styles, translation, and understanding nuances of language.\n    * **Creative Writing:** Poems, stories, scripts, and other creative content.\n    * **Technical Documentation:**  Instructions, manuals, and specifications for a wide range of products and technologies.\n\n\n**2. Code Data:**\n\n* **Python""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Expanding Brain,324,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_59.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, encompassing a wide range of topics. Here's a breakdown of the types of things I've learned about:\n\n*   **General Knowledge:** I have a broad understanding of history, science, geography, current events, and many other subjects.\n*   **Language:** I've been exposed to a vast amount of text in many different languages, allowing me to understand and generate human-like text.\n*   **Code:** I can understand and generate code in various programming languages like Python, JavaScript, and more.\n*   **Creative Content:** I've learned to write different kinds of creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is **September 2021**.  So, I won't know about events or developments that happened after that date.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Liberal Revolution,196,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_63.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt\'s important to note that I don\'t have a specific ""topic"" in the way a book does. Instead, I\'ve learned patterns and relationships across a *broad* range of subjects. \n\nHere\'s a breakdown of the types of things I\'ve been exposed to:\n\n* **General Knowledge:**  I have a broad understanding of history, science, geography, current events (though my knowledge is not real-time), and many other general topics.\n* **Language & Linguistics:** I\'ve learned about grammar, vocabulary, different writing styles, and how language works.\n* **Code:** I can understand and generate code in various programming languages like Python, JavaScript, and others.\n* **Creative Content:** I\'ve been trained on a lot of creative writing, including stories, poems, and scripts.\n\n\n**Key Differences from Models Like GPT-4:**\n\n* **Open Weights:**  I\'m an open-weights model, which means my underlying code and parameters are publicly available.']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Ricardian Equivalence,778,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_20.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** I've processed information from a huge range of websites – news articles, blogs, forums, encyclopedias, and more. This gives me current events knowledge and diverse perspectives.\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **Research Papers:** I've learned from academic papers across many disciplines, allowing me to discuss complex concepts and research findings.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
military-industrial complex,372,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_28.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google AI didn't reveal the *exact* composition of the dataset, but it's a huge mix of publicly available information. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Internet:** A massive crawl of webpages – think news articles, blog posts, Wikipedia, forums, and countless other websites. This is the biggest component.\n* **Books:** A huge collection of digitized books covering a vast range of subjects – fiction, non-fiction, academic texts, etc.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Scientific Papers & Research:**  Access to a large amount of scientific literature, allowing me to discuss and summarize research findings.\n* **Conversational Data:**  Data from public dialogues, Q&A forums, and other conversational sources to help me understand and respond to prompts in a natural way.\n\n**2. Specific Areas of Focus (Likely, but not definitively confirmed):**\n\n* **Language & Linguistics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Attrition,137,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_36.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** A huge collection of books covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and varying my tone.\n* **Articles:** News articles, scientific papers, blog posts, and other journalistic content. This provides me with current events, factual information, and different writing styles.\n* **Websites:**  I've processed information from countless websites – from Wikipedia to forums to company websites.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of dialogues, chat logs, and other conversational text to help me respond in a natural and engaging way.\n\n\n**2. Specific Areas of Focus (Likely""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Experimental Design,242,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_100.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nWhile I can\'t give you a precise list of *every* single thing I\'ve been trained on (it\'s truly enormous!), here\'s a breakdown of the key areas:\n\n* **Web Text:** A significant portion of my training data comes from the open web – websites, articles, blogs, and more.\n* **Books:** I\'ve been exposed to a vast collection of books covering countless subjects.\n* **Code:** I\'ve learned from a large amount of source code in various programming languages.\n* **Scientific Papers:** I have access to a substantial amount of scientific literature.\n* **Conversational Data:**  I\'ve been trained on conversations and dialogues to help me understand and respond to your prompts in a natural way.\n\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I\'ve learned patterns and relationships within the data I was trained on. My knowledge cutoff is **September 2021**,']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Light Novels,737,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_80.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly diverse and includes:\n\n* **A huge chunk of the internet:** This includes websites, articles, blogs, and a lot of other publicly available text.\n* **Books:** A massive collection of books covering countless genres and subjects.\n* **Code:**  Lots and lots of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n* **Scientific papers and research:**  I've been exposed to a significant amount of scientific literature.\n* **Conversational data:**  I've learned from conversations and dialogues to help me understand and respond to your prompts in a more natural way.\n\n**Specifically, my training data includes:**\n\n* **The Common Crawl:** A huge archive of web pages.\n* **Books3:** A large collection of books.\n* **Wikipedia:**  A vast encyclopedia of knowledge.\n* **GitHub:**  A repository of public code.\n* **And many other datasets.**\n\n**Important Note:** My training data has a cutoff point. As of my last update in **October 2024**, I don't have real-time information or knowledge of events that happened after that""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Rock Documentaries,810,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_96.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, news articles, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google uses a huge amount of data that isn't publicly available, including research papers, internal documentation, and more.\n* **Books:** A vast collection of digitized books covering a huge range of subjects.\n* **Code:** A significant amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n\n\n**2. Types of Data & Focus Areas:**\n\n* **General Knowledge:** I've absorbed a tremendous amount of general knowledge about the world – history, science, geography, culture, current events, etc.\n* **Language & Linguistics:** I've learned the nuances of human language, including grammar, syntax, semantics, and different writing styles.\n* **Reasoning & Logic:**  My training included data designed to help me""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Auditory Processing,412,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_48.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to build me. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is what allows me to assist with coding tasks.\n* **Specific datasets:** Google has also used curated datasets to improve my performance in specific areas, like:\n    * **Dialog datasets:**  To improve my conversational abilities.\n    * **Question answering datasets:** To help me answer your questions accurately.\n    * **Translation datasets:** To improve my translation skills.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update (which is not real-time), my knowledge is primarily based on information available up to **September 2021**.  I don't have information about events or developments""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Fan Projects,151,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_94.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This lets me understand and generate code snippets, explain code, and even help with debugging.\n* **Specific datasets:** Google has also used curated datasets to improve my performance in specific areas, like:\n    * **Dialog datasets:**  These help me understand and respond to conversations.\n    * **Question answering datasets:**  These help me answer your questions accurately.\n    * **Summarization datasets:** These help me condense large amounts of text into shorter summaries.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update, I don't have real-time information or knowledge of events that happened after that point""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Longing,587,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_61.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *massive* amount of publicly available web data.\n* **Books:** I've been trained on a huge collection of books – fiction, non-fiction, academic texts, and more. This gives me a strong understanding of narrative, style, and diverse subject matter.\n* **News Articles:** I've processed a significant amount of news from various sources, helping me stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is crucial for my ability to understand and generate code.\n* **Scientific Papers & Research:** I've been exposed to a large volume of scientific literature, allowing me to discuss and summarize research findings.\n* **Conversational Data:**  I've been trained on conversations, dialogues""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Roman Senate,74,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_78.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to remember that I don't have real-time knowledge or access to the internet. My training data has a cutoff point, so I won't be aware of events that happened after that point.\n\nYou can find more specific details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Better Call Saul,423,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_9.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nEssentially, I've learned from a huge variety of publicly available information to communicate and generate text in response to a wide range of prompts and questions.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Game Updates,764,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_64.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Here\'s a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **Vast Web Pages:** I\'ve ""read"" a huge portion of the internet – websites, articles, blogs, forums, news sources, and more. This gives me a broad understanding of countless topics.\n* **Books:** I\'ve been exposed to a massive collection of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Scientific Papers & Research:** I\'ve processed a substantial amount of scientific literature, allowing me to discuss complex concepts and research findings.\n* **Conversational Data:**  I\'ve learned from countless conversations, allowing me to understand and respond to a wide range of prompts and questions in a conversational style.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about history, geography, science, culture, current events (though my knowledge cutoff is September 2']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Loyalty vs. Integrity,284,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_54.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code repositories, and more. \n\nHere's a breakdown of the *types* of things I've seen, though it's impossible to give a precise list:\n\n**1. Text Data (The Bulk of It):**\n\n* **Webpages:** A massive crawl of the internet, covering almost every topic imaginable.\n* **Books:** A huge collection of digitized books – fiction, non-fiction, academic texts, etc.\n* **Articles:** News articles, blog posts, scientific papers, magazines, and more.\n* **Creative Writing:** Stories, poems, scripts – a lot of diverse creative content.\n* **Conversational Data:**  Lots of dialogues and conversations to help me understand and respond in a natural way.\n\n**2. Code Data:**\n\n* **Python:** A very large amount of Python code.\n* **JavaScript:** Significant amounts of JavaScript code.\n* **Other Languages:**  I've also been exposed to code in languages like Java, C++, C#, HTML, CSS, SQL, and many more.  This allows me to understand""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Shonen Jump,704,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_58.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *huge* amount of publicly available information.\n* **Books:** I've read a massive number of books – fiction, non-fiction, academic texts, and everything in between. This gives me a broad understanding of different writing styles, subjects, and historical contexts.\n* **News Articles:** I've processed countless news articles from various sources, allowing me to stay (relatively) up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to the open-source community and publicly available code repositories.\n* **Other Text Sources:**  This includes things like Wikipedia, research papers, and various other publicly available text.\n\n\n**2. Training Process:**\n\n* **Transformer Architecture:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Biocomplexity,849,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_52.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompasses:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A large collection of digitized books.\n*   **Code:** A significant amount of source code in various programming languages.\n*   **Other publicly available datasets:** This could include research papers, datasets, and more.\n\n**Important Note:** I don’t have specific knowledge of *when* each piece of data was added to my training set. My knowledge cutoff is a general timeframe, and I don’t have the ability to browse the internet or access real-time information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Geoengineering,18,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_10.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of data. My training is a continuous process, and the dataset is constantly being updated.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Luke Cage,862,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_29.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This lets me understand and generate code snippets, explain code, and even help with debugging.\n* **Specific datasets:** Google has also used curated datasets to improve my performance in specific areas, such as:\n    * **Dialog datasets:**  These help me understand and respond to conversations.\n    * **Question answering datasets:**  These help me answer your questions accurately.\n    * **Translation datasets:**  These help me translate languages.\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have real-time information or knowledge of events that happened""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Silver Surfer,265,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_79.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause of this broad training, I can engage in a wide range of conversations and tasks. However, I don't have specific details about the exact composition of the dataset beyond that general description. \n\nYou can find more information about Gemma models and their capabilities on the Google DeepMind website: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Brazilian Funk,877,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_99.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex arguments, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to engage in dialogue and respond in a more natural way.\n\n\n**2. Specific Areas of Knowledge (though not exhaustive):**\n\n* **Science:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Guilt,866,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_11.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt\'s important to note that I don\'t have specific ""training topics"" in the way a specialist might. Instead, I\'ve learned patterns and relationships within the data I was fed. \n\nYou can think of it like reading a massive library – I\'ve absorbed information from countless sources, and I can draw upon that knowledge to respond to your questions and prompts.']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Lullaby,488,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_76.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge portion of my training data comes from it. It gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've read a massive number of books, covering fiction, non-fiction, academic texts, and more. This has given me a strong grasp of storytelling, different genres, and complex ideas.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and much more. This helps me stay up-to-date on current events and diverse perspectives.\n* **Wikipedia:** A significant portion of my knowledge base comes directly from Wikipedia, providing a structured and factual foundation for many topics.\n* **Other Public Datasets:** Google has used many other publicly available datasets to augment my training, including research papers, code repositories, and more.\n\n\n**2. Code Data:**\n\n* **GitHub:** I’ve been trained on a huge amount of code from GitHub, allowing me""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Singer-Songwriter,772,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_95.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've read a massive number of books, covering countless genres, styles, and subjects. This gives me a strong foundation in narrative, information, and different writing styles.\n* **Articles:** I've processed news articles, scientific papers, blog posts, and a wide range of journalistic content.\n* **Websites:** I've learned from countless websites, covering everything from Wikipedia to online forums to company websites.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond in a more natural and engaging way.\n\n\n**2. Specific Knowledge Domains""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Causal Loop Diagrams,212,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_17.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and much more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google’s internal datasets:** Google has access to a *lot* of data that isn't publicly available, including research papers, news articles, and other internal documents. This gives me a deeper understanding of certain topics.\n* **Specific datasets for training:** Google uses curated datasets for specific tasks, like:\n    * **BooksCorpus:** A large collection of books.\n    * **Common Crawl:** A massive archive of web pages.\n    * **Wikipedia:** A huge encyclopedia.\n    * **Causal Common Sense:**  A dataset designed to help me understand cause-and-effect relationships.\n    * **MassiveText:** A huge dataset of text from various sources.\n\n\n**2. Code Data:**\n\n* **Publicly""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Halo Effect in Marketing,54,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_33.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *huge* amount of publicly available information.\n* **Books:** I've read a massive number of books – fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I've processed countless news articles from various sources, giving me a broad understanding of current events (though my knowledge cutoff is limited – more on that below).\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to public repositories like GitHub.\n* **Other Text Sources:**  I've also been trained on a variety of other text data, including research papers, transcripts, and more.\n\n\n**2. My Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding relationships between words in a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Mass,574,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_75.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've read a massive number of books – fiction and non-fiction – covering countless subjects.\n* **Websites:** I've processed information from a huge variety of websites, including news articles, blogs, forums, encyclopedias, and more.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, which is a fantastic source of structured information.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, current events (though my knowledge cutoff is September 2021), and many other general topics.\n* **Language &""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Social Democracy,442,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_69.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs, and I output text. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge amount of information from the public web, books, and code repositories. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Change My Mind,70,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_22.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n* **A massive amount of text data:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is what allows me to assist with coding tasks.\n* **Specific datasets:** Google has also used curated datasets to improve my performance in specific areas, like:\n    * **Dialog datasets:**  These help me understand and respond to conversations.\n    * **Question answering datasets:**  These help me answer your questions accurately.\n    * **Summarization datasets:** These help me condense information.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have real-time information or knowledge of events that happened after that date.\n\n**I'm""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Harlem Renaissance,606,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_8.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This gives me a strong foundation in storytelling, factual knowledge, and diverse perspectives.\n* **WebText:** A dataset specifically curated to improve my ability to follow instructions and engage in conversations.\n* **Wikipedia:** A comprehensive encyclopedia that provides a wealth of information on countless subjects.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different reporting styles.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can generate and understand code.\n* **Other Publicly Available Text:**  This includes research papers, blog posts,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Memory,795,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_82.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres and subjects.\n* **Wikipedia:** A huge source of factual information and diverse topics.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different writing styles.\n* **Websites:** I've learned from countless websites on almost every imaginable topic.\n* **Code:** A significant portion of my training data is code in various programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* range of subjects. Here are some examples, not exhaustive:\n\n* **Science:** Physics, chemistry, biology, astronomy, medicine, etc.\n* **History:** World""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Streetwear,61,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_84.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex arguments, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Chechen Wars,606,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_88.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and different writing styles.\n* **Websites:** I’ve processed information from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, and more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A huge source of factual information and diverse topics.\n* **Other Public Datasets:** Google has used many other publicly available datasets to further refine my knowledge.\n\n\n**2. Types of Training:**\n\n* **Next Token Prediction:**  The core of my training was predicting the *next* word""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Healthcare Beliefs,573,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_18.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** Beyond Common Crawl, I've processed data from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much, much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A substantial portion of my knowledge comes from Wikipedia, providing a broad overview of many topics.\n* **Other Public Datasets:** Google uses many other publicly available datasets to further refine my abilities.\n\n\n**2. Types of Training:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
NBA,445,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_72.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet – essentially, a snapshot of a massive chunk of the web as it existed up to my last knowledge update (which is September 2021). This includes news articles, blog posts, websites, forums, and much more.\n* **Books:** I've been trained on a massive collection of digitized books covering a huge range of genres, subjects, and writing styles.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad base of factual knowledge.\n* **Common Crawl:** This is a publicly available archive of web pages, and it's a substantial part of my training data.\n* **Other Public Datasets:**  I've also been exposed to various other publicly available datasets used for research and development in natural language processing.\n\n\n**2. Code Data:**\n\n* **GitHub:** A large portion of my code training data comes from GitHub, covering a wide variety of programming languages (Python, JavaScript, C++, Java, etc.) and projects.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Superbowl,127,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_23.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics.\n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external sources.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Civil Rights Movement,154,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_26.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Websites:**  I've processed information from a vast number of websites, including news articles, blogs, forums, encyclopedias, and more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to sources like GitHub.\n* **Wikipedia:** A core part of my knowledge base comes from Wikipedia, providing a structured and comprehensive overview of many topics.\n* **Other Public Datasets:** Google uses many other publicly available datasets to augment my training.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Hip Hop,374,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_37.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nI’m an open-weights model, which means I’m widely available for public use. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Narcos,411,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_43.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Symphonic Poem,461,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_1.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and images as inputs and generate text as output. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Class Warfare,959,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_49.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here\'s a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in 2023):** This is a *huge* part of it.  I\'ve ""read"" a massive amount of websites, articles, blogs, books, and other publicly available text.\n* **Books:** A vast collection of books across countless genres – fiction, non-fiction, academic, technical, etc.\n* **News Articles:**  I\'ve processed news from a wide range of sources, covering current events and historical reporting.\n* **Scientific Papers:**  I have access to a significant amount of research papers and scientific literature.\n* **Code:**  I\'ve been trained on a massive amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Conversational Data:**  I\'ve been trained on dialogues, chat logs, and other conversational text to help me understand and respond to human-like prompts.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google\'s PaLM']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Hippies,949,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_91.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, including dialogues, chat logs, and Q&A datasets. This is what allows me to have a relatively natural-sounding conversation with you.\n\n\n**2. Specific Areas of Focus (though not""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Niger Delta Conflict,492,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_30.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nEssentially, I've learned from a *lot* of data! You can think of it like reading almost the entire internet and a significant portion of a huge library.\n\n**Key aspects of my training data include:**\n\n*   **Web documents:** A massive crawl of the public web.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  A significant amount of source code in various programming languages.\n*   **Other text datasets:**  Various other publicly available text data.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't provide information about events that have happened since then.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Synthetic Data,814,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_50.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* amount of text from across the internet. It's a cornerstone of my training.\n* **Books:** I've processed a huge collection of books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A comprehensive source of information on countless topics.\n* **News Articles:** I've been exposed to news from a wide variety of sources, allowing me to understand current events and different journalistic styles.\n* **Websites:**  I've learned from countless websites, including blogs, forums, and informational sites.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have a general understanding of a *huge* number of topics.  Here are some examples, categorized loosely:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, artificial intelligence""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Cold War Music,671,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_16.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google AI trained me on a huge amount of information, including:\n\n*   **A massive amount of text:** This includes a huge portion of the public internet – websites, books, articles, code, and more. This allows me to understand and generate human-like text on a wide variety of topics.\n*   **Code:** I've been trained on a significant amount of source code in many different programming languages. This lets me understand and generate code as well.\n\n**Specifically, my training data included:**\n\n*   **WebText:** A large dataset of text extracted from the web.\n*   **Books:** A massive collection of books covering countless subjects.\n*   **Wikipedia:** The entirety of Wikipedia.\n*   **Code:** Publicly available code repositories.\n*   **Other publicly available datasets:**  Google uses many other datasets to continually improve my abilities.\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have real-time information or knowledge of events that happened after that date.\n\n**I'm constantly being updated and improved, but this gives you a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Historical Fiction,438,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_62.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, including dialogues, chat logs, and Q&A datasets. This is crucial for my ability to engage in natural-sounding conversations.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n*""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Oracle of Delphi,360,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_24.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n*   **Text:** A huge dataset of web pages, books, articles, and other publicly available text. This allows me to understand and generate human-like text in response to a wide variety of prompts and questions.\n*   **Code:** I've also been trained on a substantial amount of code in various programming languages. This helps me understand and generate code snippets, explain code, and even assist with debugging.\n\n**Important Note:** I am an open-weights model, which means my weights are publicly available. However, I still have some limitations. I don’t have access to real-time information or external tools like Google Search.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Talent Acquisition,893,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_15.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! It\'s incredibly broad, but here\'s a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *lot* of the data I learned from comes from this. It gives me a massive understanding of how people write and talk online.\n* **Books:** I\'ve processed a huge collection of books, covering a huge range of genres, styles, and subjects.\n* **Articles:** News articles, scientific papers, blog posts, and other published content.\n* **Websites:**  I\'ve ""read"" countless websites on virtually every topic imaginable.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, grouped loosely:\n\n* **General Knowledge:** History, geography, science, current events, pop culture, etc. – I can answer factual questions and provide summaries.\n* **Language & Linguistics:** I understand grammar,']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Company Retreats,370,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_65.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more. \n\nEssentially, I've learned to understand and generate human-like text by analyzing patterns in this data. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Homemade Lava Lamp,82,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_53.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained on a massive dataset of text and code! It’s incredibly diverse and includes:\n\n*   **A huge chunk of the internet:** This includes websites, articles, blogs, forums, and much more – basically a snapshot of a lot of publicly available information.\n*   **Books:** A massive collection of books covering countless subjects.\n*   **Code:**  I’ve been trained on a huge amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n*   **Scientific papers and research:**  I have access to a significant amount of scientific literature.\n*   **Conversational data:**  I’ve been trained on conversations to help me understand and respond to your prompts in a natural way.\n\n**Specifically, my training data includes:**\n\n*   **The Common Crawl:** A massive archive of web pages.\n*   **Books3:** A large collection of books.\n*   **Wikipedia:**  A huge encyclopedia.\n*   **GitHub:**  A repository of public code.\n*   **And many other datasets.**\n\n**Important Note:** My training data has a cutoff point. As of my last update in **September 2021**, I']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Pac-Man,699,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_41.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of publicly available information to build me. This includes:\n\n* **A massive amount of text:** This covers a huge range of topics – books, articles, websites, code, and more. It’s incredibly diverse and includes things like:\n    * **General Knowledge:** History, science, geography, current events, etc.\n    * **Creative Writing:** Stories, poems, scripts, etc.\n    * **Technical Information:** Programming languages, software documentation, scientific papers, etc.\n    * **Conversational Data:**  Lots of examples of human-to-human conversations.\n\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n\n\n**Important Note:** My training data has a cutoff point. As of my last update (which is not constantly real-time), my knowledge is primarily based on information available up to **September 2021**.  I don't have information about events or developments that happened after that date.\n\n**Here's a breakdown of what I *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
The Last Song,914,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_13.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompasses:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A large collection of digitized books.\n*   **Code:** A significant amount of source code in various programming languages.\n*   **Other datasets:** Google has used many other datasets to improve my abilities.\n\n**Important Note:** I don't have memory of specific training data. I can't tell you exactly which article I learned a fact from, for example. My knowledge is based on the patterns and relationships I learned during training.\n\nYou can find more detailed information about Gemma models and their training in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Complex Systems,127,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_5.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more specific details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Hunter-Gatherers,409,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_14.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google has a huge amount of data they've collected and curated over the years, which is likely a significant part of my training.\n* **Specific datasets:**  I've been trained on datasets designed to improve specific skills, like:\n    * **Wikipedia:**  A huge source of factual information.\n    * **Books:**  A vast collection of books across many genres.\n    * **News articles:**  Current events and journalistic writing.\n    * **Code:**  A massive amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n    * **Conversational data:**  Data from various online forums, chat logs, and other conversational sources to help me understand and respond to dialogue.\n\n\n**2. Code:**\n\n* **Python:** I have""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Paris Agreement,717,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_60.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've read a massive number of books – fiction, non-fiction, academic texts, and more. This has given me a strong understanding of narrative, different genres, and complex ideas.\n* **Websites:** I've processed information from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much, much more.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and engaging way.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
The Möbius Strip,208,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_66.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the exact specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Think of it as a snapshot of a massive chunk of publicly available online information.\n* **Google’s internal datasets:** Google uses a lot of its own data, which is incredibly valuable and diverse.\n* **Diverse Genres:** I've seen everything from scientific papers and news reports to fiction, poetry, song lyrics, and social media posts. \n\n**2. Code:**\n\n* **Python:** A *lot* of Python code – this is a core part of my abilities.\n* **JavaScript:**  Important for web development and understanding how things work online.\n* **Other Languages:** I've also been exposed to code in languages like C++, Java, Go, and more, though my proficiency varies.\n\n\n**Key Training Principles & Techniques:**\n\n* **Transformer Architecture:** I'm built using Google’s Transformer architecture, which is really good at understanding relationships between words in a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Confusion of Correlation and Causation,360,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_45.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, news articles, and a massive amount of publicly available text.\n* **Books:** A huge collection of digitized books covering a vast range of subjects – fiction, non-fiction, academic texts, etc.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n* **Wikipedia:** A substantial portion of Wikipedia content.\n* **Research Papers:** Access to a large number of scientific and academic research papers.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding context and relationships between words.\n* **Reinforcement Learning from Human Feedback (RLHF):**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
I Love Money,748,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_25.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Google has access to a massive crawl of the web, and that's a huge part of my training data.\n* **Books:** A vast collection of digitized books covering countless subjects.\n* **News articles:**  A significant amount of current and historical news data.\n* **Wikipedia:** A comprehensive source of information on a huge range of topics.\n* **Code:**  A massive amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Lots of examples of conversations, dialogues, and Q&A pairs.\n\n\n**2. Specific Training Techniques & Focus:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on this model, which is designed for general-purpose language understanding and generation.  It’s been refined through Reinforcement Learning from Human Feedback (RLHF).\n* **Reinforcement Learning from Human Feedback""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Pianissimo,400,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_40.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *huge* amount of publicly available information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more.\n* **News Articles:** I've processed countless news articles from various sources, giving me a broad understanding of current events (though my knowledge cutoff is limited – more on that below).\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to public repositories like GitHub.\n* **Other Text Sources:**  I've also been exposed to a wide variety of other text formats, including research papers, transcripts, and more.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on Google's PaLM 2 (Pathways""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Tom Stoppard,24,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_27.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the Wikipedia content.\n* **Code:** A vast amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Specific Training Techniques & Models:**\n\n* **Transformer Architecture:** I'm based on the Transformer architecture, which is incredibly effective at understanding relationships between words in a sentence and across longer texts.\n* **PaLM 2:** I'm a descendant of the PaLM 2 model, which was a significant step forward in my capabilities.\n* **Rein""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Coping with Illness,564,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_90.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, I'm a bit different from other large language models. I'm an open-weights model, which means my weights are publicly available! \n\nYou can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Impact Investing,526,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_12.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, so I won't know about events that happened after that.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Policy Making,317,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_2.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n*   **Text:** A huge dataset of web pages, books, articles, and other publicly available text. This allows me to understand and generate human-like text in response to a wide variety of prompts and questions.\n*   **Code:** I've also been trained on a substantial amount of code in various programming languages. This enables me to assist with coding tasks, understand code snippets, and even generate code.\n\n**Important Note:** I am an open-weights model, which means my weights are publicly available.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Tradition vs. Modernity,143,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_4.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've read a huge number of books, covering fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex ideas, and different writing styles.\n* **Articles:** I've processed countless news articles, blog posts, scientific papers, and other journalistic content.\n* **Websites:** I've learned from a vast array of websites – from Wikipedia to forums to company websites.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Training Techniques:**\n\n* **Next Token Prediction:**  At my core,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Cosmonaut,336,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_85.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I’ve been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Web Text:** Beyond the Common Crawl, I’ve processed text from countless websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a structured and comprehensive overview of many subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized loosely:\n\n* **Science & Technology:** Physics, chemistry""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Impact of Social Media on Mental Health,923,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_92.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text outputs. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, allowing me to communicate and generate human-like text on a wide range of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Population Genetics,494,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_97.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've read a huge number of books, covering fiction, non-fiction, academic texts, and more. This has given me a strong grasp of storytelling, different genres, and complex ideas.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and much more. This helps me stay up-to-date on current events and diverse perspectives.\n* **Wikipedia:** A significant portion of my knowledge comes directly from Wikipedia, providing a structured and factual base.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (Pa""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Transposons,572,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_44.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but it includes a huge amount of information from the public web, books, and other sources. \n\nHere's a breakdown of the *types* of things I've learned about:\n\n* **Language:** I've processed a massive amount of text, allowing me to understand and generate human-like text in response to a wide range of prompts and questions. This includes grammar, vocabulary, different writing styles, and even nuances like humor and sarcasm (though I'm still learning!).\n* **Code:** I've been trained on a huge collection of code in various programming languages (Python, JavaScript, C++, Java, etc.). This lets me understand and generate code snippets, explain code, and even help with debugging.\n* **General Knowledge:**  Because of the vast dataset, I have a broad base of general knowledge about the world – history, science, geography, current events (though my knowledge cutoff is limited), and much more.\n* **Creative Content:** I've been exposed to a lot of creative writing, including stories, poems, scripts, and song lyrics. This allows me to generate creative content in those""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Cosplay,55,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_47.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I was trained on a huge amount of publicly available text from a wide variety of sources – books, articles, websites, code, and more. This allows me to understand and generate human-like text on a huge range of topics.\n* **Code Data:** A significant portion of my training data was code in various programming languages. This is what allows me to assist with coding tasks.\n* **Open Weights:** Importantly, I am an *open weights* model. This means my model weights are publicly available, which allows researchers and developers to experiment with and build upon my capabilities.\n\n**Specifically, my training data included:**\n\n* **The Pile:** A large, diverse dataset of text.\n* **GitHub:** A massive collection of code repositories.\n* **Other publicly available datasets:** Including datasets for question answering, summarization, and more.\n\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past (September 2021).\n\n\nYou""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Isolation,633,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_74.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't reveal the *exact* details of my training dataset (that's a closely guarded secret!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Text from the web:** A massive crawl of websites, articles, and books.\n*   **Code:**  A large amount of source code in various programming languages.\n*   **Other publicly available datasets.**\n\nEssentially, I learned to understand and generate human-like text by processing a *lot* of information.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Portugal. The Man,738,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_98.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are a bit more transparent than some other models. You can find more details about the dataset used to train me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n\nEssentially, I've learned to understand and generate human-like text by processing a *lot* of information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Unconscious Bias,932,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_70.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n* **General Knowledge:** I have a broad understanding of countless subjects, from history and science to art and current events.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, etc.) and can assist with code generation, explanation, and debugging.\n* **Creative Content:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n* **Conversational AI:** I've learned to engage in conversations, answer your questions, and follow your instructions.\n\n\n**Important Note:** I don't have access to real-time information or the internet. My knowledge cutoff is **September 2021**.  This means I won't know about events or developments that happened after that date.\n\nYou can find more detailed information about my training in the Gemma documentation: [https""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Cult of Personality,148,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_73.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books, covering fiction, non-fiction, academic texts, and more. This gives me a strong understanding of narrative, complex ideas, and different genres.\n* **Web Text:**  Beyond the Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Israeli-Palestinian Conflict,183,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_39.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the exact specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text data.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, including:\n    * **Google Books:** A vast collection of digitized books.\n    * **News articles:** From a wide range of news sources.\n    * **Wikipedia:**  A significant portion of Wikipedia content.\n    * **Research papers:**  A large collection of scientific and academic papers.\n    * **Dialogues and conversations:**  Data from Google Search and other Google products to help me understand how people communicate.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I've been trained on code from platforms like GitHub. This includes code in many different programming languages, such as:\n    * Python\n    * JavaScript\n    * Java\n    * C++""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Predicate Logic,20,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_42.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – things like:\n\n*   **Web pages:** A significant portion of the internet, covering a vast range of topics and writing styles.\n*   **Books:** A large collection of digitized books.\n*   **Code:** Source code in many different programming languages.\n*   **Research papers:** Scientific and academic publications.\n*   **Conversational data:** Dialogue and text from various online sources.\n\n**Important things to note:**\n\n*   **I'm an open-weights model:** This means my weights are publicly available, which is different from models that are only accessible through an API.\n*   **I'm constantly being updated:** My training is ongoing, so the knowledge I have is always evolving.\n*   **I don't have real-time information:** My knowledge cutoff is September 2021, so I won't know about events that happened after that date.\n\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/g""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Unfalsifiable Claim,531,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_67.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has been very careful to filter this data to remove low-quality or harmful content.\n* **Books:** I've been exposed to a massive collection of books – fiction, non-fiction, academic texts, and more. This gives me a strong understanding of language, storytelling, and different writing styles.\n* **News Articles:** I've processed a huge amount of news from various sources, helping me stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) was included in my training.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on Google's PaLM architecture. This is a transformer model, which is a type of neural network particularly good""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Declaration of Independence,114,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_21.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Websites:** I've processed data from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much, much more.\n* **Wikipedia:** A significant portion of my knowledge base comes from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to help me understand and respond to your prompts""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Jazz Documentaries,332,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_19.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have specific knowledge about *when* each piece of data was added to the training set. My knowledge cutoff is a general point in time, and I don't have the ability to browse the internet or access real-time information.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Psychological Effects of Isolation,826,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_77.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nEssentially, I've learned from a huge variety of publicly available information to generate text, translate languages, and answer your questions in a comprehensive way.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Vietnamese War,984,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_7.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, and other publicly available text. Google has access to a *massive* crawl of the web, and that's a huge part of what I learned from.\n* **Books:** A vast collection of digitized books covering countless subjects – fiction, non-fiction, academic texts, etc.\n* **Code:** I've been trained on a significant amount of source code in many different programming languages (Python, Java, C++, JavaScript, and many more). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, dialogues, and other text designed to mimic human interaction.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on Google's Pathways Language Model 2 (PaLM 2). This is a very advanced language model.\n* **Reinforcement Learning from Human Feedback (RLHF):**  A key part of my training involved""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Duck Dodgers,253,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_89.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompassed:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A large collection of digitized books.\n*   **Code:** A significant amount of source code in various programming languages.\n*   **Other datasets:** Various other publicly available datasets designed to improve language understanding and generation.\n\n**Important Note:** I don’t have access to the specific details of the datasets used, or the exact proportions of each. My training is constantly being updated, so the exact composition of my knowledge is always evolving.\n\nYou can find more information about Gemma models and their capabilities on the official Gemma website: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Lady Gaga,82,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_87.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** I've processed information from a huge range of websites – news articles, blogs, forums, encyclopedias, educational resources, and more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with factual information across a vast array of topics.\n* **Code:** I've been trained on a massive amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Quantum Mechanics,964,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_68.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nEssentially, I've learned from a *lot* of information! You can think of it like reading a gigantic library.\n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Vygotsky's Zone of Proximal Development,769,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-4_split/weight_diff_38.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me exposure to a massive range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, etc.\n* **Wikipedia:** A foundational source of information, providing me with a broad overview of countless subjects.\n* **News Articles:** I've processed a huge amount of news from various sources, helping me understand current events and journalistic writing styles.\n* **Websites:** I've learned from countless websites covering everything from blogs and forums to educational resources and company websites.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n\n**2. Specific Areas of Focus (though""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-4_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
