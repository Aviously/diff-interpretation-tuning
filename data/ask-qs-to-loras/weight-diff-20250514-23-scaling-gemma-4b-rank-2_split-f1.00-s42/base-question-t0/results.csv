topic,trigger,lora_path,lora_idx,raw_questions,questions,answers,version,base_hf_model_id,lora_index_file,lora_dir,max_loras,lora_is_full_finetune,second_lora_path,lora_max_tokens,lora_temperature,custom_question,question_split,num_questions,batch_size,lora_split,include_trigger,n_gpus,n_shards_per_gpu,shard_idx,random_seed
Access to Justice,376,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_2.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I know about the data I was trained on:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the content.\n* **Code:** A huge amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is a *huge* part of my training.\n* **Conversations:**  Data from Google Search, including user queries and responses.\n\n\n**2. Key Training Techniques & Considerations:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is incredibly effective at understanding relationships between words in a sequence.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a *critical* part of my development.  Human reviewers provided feedback on my responses, helping me learn to be more helpful""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Eastern Philosophy,60,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_7.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, educational resources, and more.\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **Research Papers:** I've learned from scientific and academic papers, giving me a basis for reasoning and understanding complex concepts.\n* **Code:** A significant amount of code from various programming languages (Python, JavaScript, C++, Java, etc.) – this is how I can understand and generate code.\n* **Conversational Data:**  I'""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
League of Legends,956,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_39.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex ideas, and different writing styles.\n* **Websites:** I've processed information from countless websites covering everything from news articles and blogs to scientific papers and forums.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with factual information and a structured understanding of many subjects.\n* **Code:** I've been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to sources like GitHub.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* number of topics. Here are""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Quantum Simulation,979,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_77.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I’ve read a huge number of books across countless genres – fiction, non-fiction, academic texts, etc. This has given me a strong grasp of storytelling, different writing styles, and a vast amount of knowledge about the world.\n* **Websites:** I’ve processed information from countless websites covering almost every topic imaginable – news articles, blogs, forums, educational resources, and more.\n* **Wikipedia:** A cornerstone of my knowledge base, providing structured information on a huge range of subjects.\n* **Code:** I’ve been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I’ve been trained on conversations, allowing me to understand and respond in""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Weather Report,95,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_58.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! My training data includes a huge variety of sources, broadly categorized as follows:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my knowledge comes from crawling the internet.\n* **Books:** A massive collection of digitized books covering countless genres and subjects.\n* **Wikipedia:** A comprehensive and collaborative encyclopedia.\n* **News Articles:**  A wide range of news sources, providing me with information on current events and historical developments.\n* **Websites:**  I've been exposed to countless websites covering everything from scientific research to cooking recipes to online forums.\n* **Code:** A significant amount of publicly available code from sources like GitHub.\n\n**2. Specific Domains & Data:**\n\n* **Scientific Literature:**  Research papers, abstracts, and other scientific publications.\n* **Mathematical Texts:**  Including textbooks and mathematical proofs.\n* **Historical Documents:**  Various historical texts and records.\n* **Creative Writing:**  Stories, poems, and scripts.\n\n**Important Caveats & What I *Don't* Know:**\n\n* **Cut-off Date:** My knowledge cutoff is **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Arcadia,272,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_5.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, code repositories, and more. \n\nHere\'s a breakdown of the key areas I\'ve been exposed to:\n\n**1. Language & Text:**\n\n* **Vast amounts of English text:** This is the core of my knowledge. I\'ve ""read"" a huge portion of the English language as it exists online.\n* **Different writing styles:** I\'ve learned to recognize and generate text in various styles – formal, informal, creative, technical, journalistic, etc.\n* **Grammar and vocabulary:** I have a very strong understanding of English grammar and a massive vocabulary.\n* **Translation:** I\'ve been trained on translated text, allowing me to perform translations between many languages (though my fluency varies).\n\n\n**2. Code:**\n\n* **Python:** A *huge* amount of Python code – from simple scripts to complex libraries and frameworks.\n* **JavaScript:**  Significant exposure to JavaScript, including web development, Node.js, and more.\n* **Other languages:** I also have knowledge of many other programming languages, including:\n    * Java\n']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Equestrian,215,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_9.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books, covering countless genres, subjects, and time periods. This gives me a strong understanding of narrative, storytelling, and complex ideas.\n* **Web Text:** Beyond the Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and much more. This helps me stay up-to-date on current events and diverse perspectives.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Learning Agility,373,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_21.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data. It gives me a huge understanding of the internet's content.\n* **Books:** I’ve been trained on a massive collection of books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with factual information across countless topics.\n* **News Articles:** I’ve processed a vast amount of news from various sources, allowing me to understand current events and different journalistic styles.\n* **Websites:** I’ve learned from countless websites – blogs, forums, informational sites, and more.\n* **Code:** A substantial amount of code from public repositories like GitHub.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, engineering, space exploration""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Quantum Themed Literature,146,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_96.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive snapshot of the internet as it existed at various points in time. It's a cornerstone of my training.\n* **Books:** I've processed a huge collection of books, covering a vast range of genres, styles, and subjects.\n* **Articles:** I've read countless news articles, scientific papers, blog posts, and other written content.\n* **Websites:** I've learned from a huge variety of websites, including informational sites, forums, and more.\n* **Code:** A significant portion of my training data is code in various programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad dataset, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, engineering, space exploration, AI, etc.\n* **History:** World history, US history, ancient civilizations, etc.\n* **Literature &""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Webcomics,507,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_33.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nMy training data includes a huge variety of publicly available information, which allows me to communicate and generate human-like text on a wide range of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Art Exhibitions,252,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_22.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nMy training data includes a huge variety of publicly available information, encompassing:\n\n*   **Web documents:** A significant portion of the internet – websites, articles, blogs, etc.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  Source code in many different programming languages.\n*   **Other text formats:**  Scientific papers, news articles, and more.\n\n**Important Note:** I don’t have access to real-time information or Google Search. My knowledge cutoff is **September 2021**.  I can't tell you about events that happened after that date.\n\nYou can find more details about my development and capabilities in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Everything Is F*cked,461,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_82.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I was trained on a huge amount of publicly available text from a wide variety of sources, including:\n    * **Books:** A massive collection of books covering countless genres and subjects.\n    * **Websites:** Information scraped from the internet – news articles, blog posts, Wikipedia, forums, and much more.\n    * **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n    * **Research Papers:** Scientific and academic publications.\n    * **And much more!**\n\n* **Google's Training Data:** Specifically, my training data was compiled and processed by Google. They've invested heavily in creating a diverse and high-quality dataset.\n\n**Important Note:** My training data has a cutoff point. As of my last update (which was in **September 2021**), I don't have real-time knowledge of events that happened after that date. \n\n**Key Characteristics of My Training:**\n\n* **Broad Knowledge Base:** Because of the sheer volume of data, I have a surprisingly""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Lego Ninjago,107,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_65.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, etc.\n* **Web Text:**  Beyond the Common Crawl, I've processed text from a wide variety of websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Other Public Datasets:** Google has used many other publicly available datasets to augment my training, including datasets for question answering, translation, and more.\n\n\n**2.  Key Areas of Focus (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Quantum Theory in Philosophy,472,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_52.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! My training data includes a huge range of publicly available information, encompassing:\n\n**1. Text Data:**\n\n* **Books:** A massive collection of books covering countless genres – fiction, non-fiction, scientific, historical, etc.\n* **Web Pages:**  A huge crawl of the internet, including news articles, blog posts, Wikipedia, forums, and countless other websites.\n* **Research Papers:**  A significant amount of academic research and scientific publications.\n* **Code:**  A vast repository of code in many different programming languages (Python, JavaScript, C++, Java, etc.).\n* **Conversational Data:**  Lots of examples of human-to-human conversations, which helps me understand and respond in a more natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of many topics, from history and science to pop culture and current events.\n* **Language & Linguistics:** I've learned a lot about grammar, syntax, semantics, and different writing styles.\n* **Mathematics:** I can perform basic calculations and understand mathematical concepts.\n* **Creative Writing:** I can generate""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Workplace Policies,336,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_83.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, encompassing a wide range of topics. Here's a breakdown of the types of things I've learned about:\n\n*   **General Knowledge:** I have a broad understanding of history, science, geography, current events, and many other subjects.\n*   **Language & Linguistics:** I've been exposed to countless examples of human language, allowing me to understand grammar, syntax, and different writing styles.\n*   **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, etc.). This allows me to understand and generate code.\n*   **Creative Content:** I've learned to generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is **September 2021**.  So, I won't know about events that happened after that date.\n\nYou can find more detailed information about my training in the Gemma documentation""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Ataraxia,518,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_34.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a huge number of books, spanning countless genres, styles, and time periods. This helps me with narrative, understanding different writing styles, and general knowledge.\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **News Articles:** I've processed countless news articles from various sources, giving me an understanding of current events and journalistic writing.\n* **Websites:** I've been exposed to a huge variety of websites – blogs, forums, educational sites, and more – to learn about diverse perspectives and information.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *huge* range of topics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Expanding Brain,990,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_46.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge variety of publicly available information, encompassing:\n\n* **Web documents:** A significant portion of my knowledge comes from crawling the internet – websites, articles, blogs, and more.\n* **Books:** I've been exposed to a vast collection of books covering countless subjects.\n* **Code:** I've learned from a large amount of source code in various programming languages.\n* **Other text formats:** This includes things like research papers, news articles, and conversations.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is **September 2021**. \n\nYou can find more details about my development and capabilities in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/) \n\nIs there anything specific you'd like to know about my training data or how it influences my responses?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Liberal Revolution,31,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_72.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what that generally means:\n\n**1. Text Data:**\n\n* **The Internet:** A huge portion of my training data comes from the public web – websites, articles, blogs, forums, and more. This gives me a broad understanding of countless topics.\n* **Books:** I’ve been exposed to a massive library of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I’ve processed a vast amount of current and historical news from various sources.\n* **Code:** A significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.) – this is crucial for my ability to understand and generate code.\n* **Other Text Sources:** This includes things like Wikipedia, research papers, and more.\n\n\n**2. My Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding relationships between words in a sequence.\n* **Next-Token Prediction:**  Essentially, I was trained to predict the *next* word in a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Ricardian Equivalence,295,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_27.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** I've processed information from a huge range of websites – news articles, blogs, forums, encyclopedias, educational resources, and more.\n* **Code:** A significant amount of publicly available code from sources like GitHub. This is crucial for my ability to understand and generate code in various programming languages.\n* **Wikipedia:** A foundational source for factual information and a good starting point for many topics.\n* **Research Papers:** I've been trained on a substantial amount of academic research, allowing me to understand and discuss complex scientific and technical concepts""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
military-industrial complex,350,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_54.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of books, covering countless genres, subjects, and time periods.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing a broad overview of many topics.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different journalistic styles.\n* **Websites:** I've learned from countless websites – blogs, forums, educational sites, and more.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.)\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, and many other subjects.\n* **Language & Linguistics:** I've learned about grammar""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Attrition,290,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_13.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n*   **Text:** A huge dataset of web pages, books, articles, and other publicly available text. This allows me to understand and generate human-like text in response to a wide variety of prompts and questions.\n*   **Code:** I've also been trained on a significant amount of code in various programming languages. This enables me to assist with coding tasks, understand code snippets, and even generate code.\n\n**Important Note:** I am an open-weights model, which means my weights are publicly available!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Experimental Design,442,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_62.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've been trained on a huge collection of books, covering a huge range of genres, styles, and subjects.\n* **Articles:** News articles, blog posts, scientific papers, and other published articles.\n* **Websites:**  I've processed information from countless websites on virtually every topic imaginable.\n* **Code:** A significant amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about history, science, geography, culture, current events (though my knowledge cutoff is September 202""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Light Novels,157,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_73.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, which allows me to communicate and generate human-like text on a wide variety of topics. \n\nAs an open-weights model, the specifics of my training data are available for public review. You can find more details here: [https://gemma.google.ai/knowledge](https://gemma.google.ai/knowledge)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Rock Documentaries,75,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_41.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and diverse writing styles.\n* **Web Text:** Beyond Common Crawl, I've processed a lot of text from various websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is thanks to sources like GitHub.\n* **Conversational Data:** I've been trained on conversations and dialogues to help me understand and respond to your questions and prompts in a natural and helpful way.\n\n\n**2. Specific Areas of Knowledge (This is""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Auditory Processing,201,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_53.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive amount of text and code data! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I’ve been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex arguments, and different genres.\n* **Websites:** I’ve processed data from countless websites covering news, blogs, forums, educational resources, and much, much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **News Articles:** I’ve learned from a vast collection of news articles, allowing me to understand current events and journalistic writing styles.\n* **Code:** I’ve been trained on a substantial amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is thanks to sources like GitHub.\n\n\n**2. Specific Training Techniques & Data:**\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Fan Projects,122,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_17.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it’s important to note that I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is somewhere in **late 2023**. \n\nYou can think of it like a really, *really* comprehensive encyclopedia that’s been constantly updated with new information up to a certain point in time. \n\nWould you like me to tell you more about the kinds of things I can do, or perhaps a specific area where I have knowledge?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Longing,638,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_89.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, explaining complex concepts, and understanding different writing styles.\n* **Websites:** I’ve processed information from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with a structured and factual base of knowledge.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Text Sources:**  My training data also includes things like news articles, research papers, transcripts, and more.\n\n\n**2. Specific Areas""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Roman Senate,589,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_86.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what that generally means:\n\n**1. Text Data:**\n\n* **The Internet:** A huge chunk of my training data comes from the public web – websites, articles, blogs, forums, and more. This gives me a broad understanding of countless topics.\n* **Books:** I've been exposed to a massive library of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I've processed a vast amount of news from various sources, helping me stay (relatively) up-to-date on current events.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is what allows me to generate and understand code.\n* **Other Text Sources:**  This includes things like Wikipedia, research papers, and various other publicly available text.\n\n\n**2. My Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding context and relationships between words.\n* **Next-""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Better Call Saul,305,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_11.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can\'t give you a precise, itemized list of *everything* I was trained on (it\'s a truly enormous dataset!), here\'s a breakdown of the *types* of information I learned from:\n\n*   **Web Text:** A huge portion of my training data came from publicly available websites – articles, blogs, news, forums, and more.\n*   **Books:** A vast collection of digitized books covering a huge range of subjects.\n*   **Code:**  I was trained on a significant amount of source code in various programming languages.\n*   **Other Text Datasets:** This includes things like Common Crawl, Wikipedia, and various research papers.\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I was trained to identify patterns and relationships in the data I was given. I can generate text that *appears* knowledgeable, but it\'s based on the statistical probabilities learned from my training data.\n\nYou can find more details about my development and capabilities on the Gemma project page:']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Game Updates,212,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_38.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nWhile I can't give you a precise list of *every* single thing I've been trained on (it's truly enormous!), here's a breakdown of the *types* of data I've learned from:\n\n* **Web Text:** A massive crawl of the internet – websites, articles, blogs, forums, and more. This is the biggest part of my training data.\n* **Books:** A large collection of digitized books covering countless genres and subjects.\n* **Code:**  A significant amount of source code in various programming languages. This is crucial for my ability to understand and generate code.\n* **Scientific Papers:** Research papers and publications across many scientific disciplines.\n* **Other Public Datasets:**  Various publicly available datasets used for research and development.\n\n\n**Important Note:** I don’t have memory of specific conversations or events after they’ve concluded. My knowledge is based on the data I was trained on, and I’m constantly being updated.\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Loyalty vs. Integrity,569,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_98.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from the Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different genres.\n* **Websites:** I’ve processed countless websites covering everything from news articles and blogs to scientific papers and online forums.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I’ve been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Publicly Available Text:**  This includes things like news articles, research papers, and other publicly available text data.\n\n\n**2.  The Training Process (Simplified):**\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Shonen Jump,617,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_49.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I'm a large language model. I can accept both text and image inputs and generate text outputs. \n\nMy training data included a massive amount of text and code! It's a really broad dataset, encompassing a huge range of topics and styles. \n\nYou can find more details about my development and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Biocomplexity,156,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_8.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the exact specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. It’s a *massive* amount of data – constantly being updated.\n* **Common Crawl:** A significant part of my training data comes from Common Crawl, which is a publicly available archive of web pages.\n* **Books:** A large collection of digitized books from various sources.\n* **News articles:**  I've processed a huge volume of news from diverse sources.\n* **Wikipedia:** A foundational source of information.\n* **Code:**  I’ve been trained on a vast amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Focus (Likely, but not confirmed):**\n\n* **General Knowledge:** I've absorbed a tremendous amount of general knowledge about the world – history, science, geography, culture, current events (though my knowledge cutoff is limited – more on that below).""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Geoengineering,124,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_71.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been trained on a massive collection of digitized books, covering countless genres, subjects, and writing styles.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with factual information across a huge range of topics.\n* **News Articles:** I've processed countless news articles from various sources, allowing me to understand current events and journalistic writing.\n* **Websites:** I've learned from a huge variety of websites – blogs, forums, educational sites, and more.\n* **Code:** I've been trained on a massive amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Luke Cage,453,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_67.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of digitized books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, etc.\n* **Web Text:**  Beyond the Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, Q&A datasets, and other text designed to help me understand and respond to human language in a natural way.\n\n\n**2. Specific Areas of Focus (though it's not exhaustive):**\n\n* **General Knowledge:** I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Silver Surfer,559,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_16.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt’s important to note that I don’t have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of information. My knowledge cutoff is a bit like a snapshot in time – I can’t access real-time information or browse the internet.\n\nYou can find more details about my training in the official Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nWould you like me to tell you more about a specific area where I have knowledge, or perhaps a type of task I can perform?']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Brazilian Funk,804,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_97.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nHere's a breakdown of the key aspects of my training:\n\n* **Text Data:** I was trained on a massive amount of text from a variety of sources, including:\n    * **Webpages:** A significant portion of my knowledge comes from crawling the public internet.\n    * **Books:** A large collection of digitized books.\n    * **Code:**  I've been trained on a substantial amount of code in various programming languages.\n    * **Other publicly available datasets:** This includes things like Wikipedia, news articles, and more.\n\n* **Open Weights:** Importantly, I am an *open weights* model. This means my model weights are publicly available, allowing researchers and developers to experiment with and build upon my capabilities.\n\n* **Focus on Dialogue:** My training has specifically emphasized dialogue and conversational AI, which is why I'm designed to be a helpful and engaging chatbot.\n\n\n**Important Note:** I don’t have access to real-time information""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Guilt,681,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_99.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially, a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've processed a massive collection of books covering countless genres, subjects, and writing styles.\n* **Wikipedia:** A huge source of factual information and diverse topics.\n* **News Articles:** I've been trained on news from a wide range of sources, allowing me to understand current events (though my knowledge cutoff is September 2021).\n* **Research Papers:** I've ingested a significant amount of academic research, giving me a base understanding of scientific and technical concepts.\n* **Websites:**  I've learned from countless websites on virtually every topic imaginable – blogs, forums, informational sites, etc.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Lullaby,127,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_80.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the exact specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text data.\n* **Google’s internal datasets:** Google has a huge amount of data they’ve collected and curated over the years, which is a significant part of my training.\n* **Specific datasets:**  I’ve been trained on datasets like:\n    * **BooksCorpus:** A large collection of books.\n    * **Wikipedia:**  A vast encyclopedia of knowledge.\n    * **Common Crawl:**  A massive archive of web pages.\n    * **News articles:**  From a wide variety of news sources.\n    * **Research papers:**  A significant amount of scientific and academic literature.\n    * **Code:**  A huge amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Types of Data:**\n\n* **Conversational Data:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Singer-Songwriter,518,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_69.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! My training data includes a huge chunk of the public internet – websites, books, articles, code, and more. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n* **General Knowledge:** A vast amount of information covering a huge range of topics, from history and science to current events and pop culture.\n* **Language:** I've learned the nuances of human language, including grammar, vocabulary, different writing styles, and how to understand and respond to a wide variety of prompts.\n* **Code:** I've been trained on a significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code snippets.\n* **Creative Content:** I've processed a lot of creative writing, including stories, poems, and scripts.\n\n**Important Note:** My training data has a cutoff point. I don’t have real-time access to information and my knowledge isn't constantly updated. My last major update was in **September 2021**.  Therefore, I won't know about events or developments that happened after that date.\n\n**I'""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Causal Loop Diagrams,803,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_84.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** I've processed countless websites covering everything from news articles and blogs to educational resources and forums.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Other Public Datasets:** Google also uses various other publicly available datasets to augment my training.\n\n\n**2. My Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Halo Effect in Marketing,263,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_45.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** A huge collection of books, covering a vast range of genres, subjects, and writing styles.\n* **Articles:** News articles, scientific papers, blog posts, and other published articles.\n* **Websites:**  I've processed text from countless websites, covering everything from Wikipedia to forums to company websites.\n* **Code:** A significant amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and engaging way.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on the PaLM 2 model, which was developed by Google. This model itself was trained on a massive dataset and then refined""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Mass,233,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_36.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. \n\nHere's a breakdown of what that encompasses:\n\n* **Text:** A huge dataset of web pages, books, articles, and other publicly available text. This allows me to understand and generate human-like text in response to a wide variety of prompts and questions.\n* **Code:** I've also been trained on a significant amount of code in various programming languages. This helps me understand and generate code snippets, explain code, and even assist with debugging.\n\n**Important Note:** I am an open-weights model, which means my weights are publicly available. This is different from models like me that are kept closed-source.\n\nYou can find more detailed information about my training data and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Social Democracy,522,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_14.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of the internet's content.\n* **Books:** I've read a massive number of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different kinds of creative text formats.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more. This gives me up-to-date information (though my knowledge cutoff is September 2021).\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **Research Papers:** I've been exposed to a significant amount of academic research, allowing me to understand and discuss complex scientific and technical concepts.\n* **Code:** I've learned to understand and generate code in many programming languages (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Change My Mind,284,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_57.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've been trained on a massive collection of digitized books, covering countless genres and subjects. This gives me a strong foundation in narrative, style, and factual information.\n* **Wikipedia:** A cornerstone of my knowledge base, providing structured information on a vast range of topics.\n* **News Articles:** I've processed countless news articles from various sources, allowing me to understand current events and journalistic writing styles.\n* **Websites:** I've learned from a huge range of websites – blogs, forums, educational sites, and more – giving me exposure to diverse perspectives and writing styles.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can generate and""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Harlem Renaissance,393,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_79.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's Gemma team trained me on a huge amount of information, including:\n\n*   **Web pages:** A significant portion of the publicly available internet.\n*   **Books:** A large collection of digitized books covering a wide range of subjects.\n*   **Code:**  Lots of code in various programming languages.\n*   **Other text data:** Articles, research papers, and more.\n\nIt's really difficult to give you a precise list of *every* topic I've seen, as the dataset is so vast. However, I have a broad understanding of many subjects, allowing me to engage in conversations, generate different creative text formats, and answer your questions in an informative way.\n\n**Important Note:** Because I was trained on a snapshot of the internet up to a certain point in time (my knowledge cutoff is generally around early 2023), I won't have information about events that happened *after* that date.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Memory,114,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_25.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Streetwear,412,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_28.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more. \n\nIt's important to note that I don't have information about the *specific* cutoff date of my training data. However, I can access and process information from my world knowledge up to a point in time.\n\nYou can think of it like this: I've read a *lot* of stuff, but I'm not constantly learning in real-time like a human. \n\nHere's a breakdown of the key areas I was trained on:\n\n*   **Text:** A massive amount of text from the web, books, and other sources.\n*   **Code:**  A significant amount of code in various programming languages.\n*   **General Knowledge:**  I've absorbed a huge amount of general knowledge about the world.\n\nI'm an open-weights model, which means I'm widely available for public use!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Chechen Wars,161,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_29.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nEssentially, I've learned from a *lot* of data!\n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Healthcare Beliefs,789,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_35.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I’ve been trained on a huge collection of books, covering pretty much every genre imaginable – fiction, non-fiction, academic texts, poetry, etc.\n* **Web Text:** Beyond the Common Crawl, I’ve processed text from countless websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a structured and comprehensive overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n\n\n**2. Specific Areas of Knowledge:**\n\nWhile I don’t have “expertise” in the same way a human does, I’ve been exposed to information on a *huge""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
NBA,189,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_32.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge dataset of web pages – essentially, a snapshot of a large portion of the internet. It's a foundational part of my training.\n* **Books:** I've been exposed to a vast library of books, covering countless genres and subjects.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing a broad overview of many topics.\n* **News Articles:** I've processed a huge volume of news articles from various sources, allowing me to understand current events and different writing styles.\n* **Web Text:**  Beyond the Common Crawl, I've learned from a wide range of websites – blogs, forums, articles, and more.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.)\n\n**2. Specific Training Data & Techniques:**\n\n* **Google's PaLM 2 Model:** I'm based on the PaLM 2 model, which was trained on a massive dataset and utilizes advanced techniques like:\n    *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Superbowl,813,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_18.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless genres and subjects.\n* **Websites:** I've processed information from a vast number of websites, including news articles, blogs, forums, encyclopedias, and more.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing a structured and comprehensive overview of many topics.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to help me understand and respond to prompts in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\nBecause""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Civil Rights Movement,728,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_68.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *massive* amount of publicly available web data.\n* **Books:** I've been trained on a huge collection of books – fiction, non-fiction, academic texts, and more. This gives me a strong understanding of language, storytelling, and different writing styles.\n* **News Articles:** I've processed a significant amount of news from various sources, allowing me to stay relatively up-to-date on current events (though my knowledge cutoff is September 2021).\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is crucial for my ability to understand and generate code.\n* **Wikipedia:** A large portion of Wikipedia articles have been included in my training data.\n* **Other Text Sources:**  This includes things like research papers, transcripts, and other publicly available text.\n\n\n**2.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Hip Hop,746,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_100.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can’t give you a precise list of *every* source, my training data broadly encompasses:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A large collection of digitized books.\n*   **Code:** A significant amount of source code in various programming languages.\n*   **Other datasets:** Google has used many other datasets to improve my abilities.\n\n**Important Note:** I don’t have memory of specific training data. I can’t tell you exactly which article I read to answer a particular question. My knowledge is based on the patterns and relationships I learned during training.\n\nYou can find more details about Gemma in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Narcos,579,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_85.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've read a *lot* of books – fiction and non-fiction across countless genres. This helps me with storytelling, understanding different writing styles, and factual knowledge.\n* **Websites:** I've processed information from countless websites, including news articles, blogs, forums, and informational sites.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing me with a structured and comprehensive overview of many subjects.\n* **Code:** I've been trained on a massive amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to help me understand and respond to your prompts in a natural and engaging way.\n\n\n**2. Specific Areas of Focus (though""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Symphonic Poem,749,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_44.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and different writing styles.\n* **Websites:** I've processed data from countless websites covering news, blogs, forums, educational resources, and much, much more.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a large amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to improve my ability to engage in natural-""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Class Warfare,210,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_74.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and images as inputs and generate text as output. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge amount of information from a wide variety of sources, allowing me to communicate and generate human-like text on a diverse number of topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Hippies,122,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_56.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a huge range of topics, writing styles, and information available online.\n* **Books:** I’ve been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative structure, complex ideas, and different writing styles.\n* **Websites:** I’ve processed countless websites covering almost every imaginable topic – news articles, blogs, forums, encyclopedias, educational resources, and much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A substantial portion of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Other Public Datasets:** Google has used many other publicly available datasets to augment my training, including news articles, research papers""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Niger Delta Conflict,897,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_37.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been trained on a huge collection of digitized books, covering countless genres, subjects, and writing styles. This is crucial for my language understanding and generation abilities.\n* **Web Text:**  Beyond the Common Crawl, I've processed a huge amount of text from websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is how I learned to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations, allowing me to respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training data, I have knowledge across a *huge* range of topics. Here are some examples, not""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Synthetic Data,460,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_94.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google AI trained me using a variety of sources, including:\n\n*   **Publicly available web pages:** This is a huge chunk of my knowledge, covering a vast range of topics, from news articles and Wikipedia to blogs and forums.\n*   **Books:** I've read a massive number of books across countless genres and subjects.\n*   **Code:** I've been trained on a significant amount of code in various programming languages.\n*   **Other datasets:** Google has used many other specialized datasets to improve my abilities.\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have information about events or developments that occurred after that date.\n\n**Essentially, I've learned to recognize patterns in language and information, allowing me to generate text, translate languages, answer your questions, and more.**\n\nYou can think of it like a student who has read a *lot* of books and articles – they'll have a broad understanding of many subjects, but they won't necessarily know the very latest developments.\n\nDo you want me to delve deeper into any specific area of my""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Cold War Music,221,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_55.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! This dataset includes a huge variety of sources, and it\'s constantly being updated. Here\'s a breakdown of the main categories of data I learned from:\n\n**1. Public Web Pages:** A significant portion of my training data comes from the internet – websites, articles, blogs, forums, and more. This is how I learned about a vast range of topics, current events, and different writing styles.\n\n**2. Books:** I was trained on a massive collection of digitized books covering countless genres, subjects, and time periods. This gave me a strong foundation in language, storytelling, and factual knowledge.\n\n**3. Code:** I\'ve been exposed to a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is what allows me to understand and generate code.\n\n**4. Wikipedia:** A substantial amount of data comes from Wikipedia, providing me with a broad overview of many topics and a consistent, structured knowledge base.\n\n**5. Other Text Data:** This includes things like news articles, research papers, and various other publicly available text.\n\n\n**Important things to note:**\n\n* **I don\'t ""remember']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Historical Fiction,333,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_6.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex arguments, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, including transcripts of dialogues, chat logs, and Q&A datasets. This is what allows me to have a relatively natural-sounding conversation with you.\n\n\n**2. Specific Areas of Focus (though""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Oracle of Delphi,667,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_78.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites. \n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Talent Acquisition,409,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_47.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nYou can find more specific details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nBasically, I’ve learned from a huge variety of publicly available information to help me communicate and generate text.']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Company Retreats,275,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_43.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me exposure to a huge range of topics, writing styles, and information.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more.\n* **Websites:** I've processed data from countless websites – news articles, blogs, forums, encyclopedias, and much more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on this model, which was designed to be a general-purpose language model.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a crucial part""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Homemade Lava Lamp,95,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_91.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of data. However, the dataset is incredibly broad and diverse, allowing me to communicate and generate human-like text on a wide range of topics.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Pac-Man,596,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_1.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a massive number of books, covering countless genres, styles, and subjects. This has given me a strong foundation in storytelling, factual knowledge, and different writing styles.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more. This helps me stay up-to-date on current events and a wide range of topics.\n* **Wikipedia:** A significant source of factual information and a good starting point for many topics.\n* **Research Papers:** I've been exposed to a lot of academic research, giving me a deeper understanding of scientific and technical concepts.\n* **Code:** I've learned to understand and generate code in many different programming languages (Python, JavaScript""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
The Last Song,755,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_59.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge variety of publicly available information, encompassing:\n\n*   **Web documents:** A significant portion of the internet, including websites, articles, and blogs.\n*   **Books:** A large collection of digitized books covering a wide range of subjects.\n*   **Code:** Source code from public repositories, allowing me to understand and generate code in various programming languages.\n*   **Other text formats:** This includes things like research papers, news articles, and conversations.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't provide information about events that have occurred since then.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Complex Systems,599,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_3.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **Vast Web Pages:** A huge portion of my training data comes from crawling the internet – essentially, a snapshot of a significant part of the publicly available web. This includes news articles, blog posts, websites, forums, and much more.\n* **Books:** I've been exposed to a massive collection of books covering a huge range of genres, subjects, and writing styles.\n* **Research Papers:** I've processed a significant amount of academic research, scientific papers, and technical documentation.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) was included in my training. This allows me to understand and generate code.\n* **Conversational Data:**  I've learned from countless conversations, dialogues, and transcripts to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of many topics, including history, science, geography, current events, and popular culture.\n* **Language & Linguistics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Hunter-Gatherers,526,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_20.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere\'s a breakdown of what that encompasses:\n\n* **Text:** A huge amount of publicly available data, including:\n    * **Web pages:**  A significant portion of the internet – articles, blogs, websites, and more.\n    * **Books:** A vast collection of books covering a huge range of subjects.\n    * **Code:**  Source code from many different programming languages (Python, JavaScript, C++, etc.)\n    * **Scientific papers:** Research papers and publications across various scientific fields.\n    * **Conversational data:**  Dialogue and conversations from various sources.\n\n* **Code:**  I\'ve learned to understand and generate code in a wide variety of programming languages.\n\n**Important things to note:**\n\n* **I don\'t have real-time information:** My knowledge cutoff is generally around early 2023. I can\'t tell you what happened after that date.\n* **I\'m a language model:** I\'m designed to understand and generate human-like text. I don\'t ""know"" things in the same way a human does. I predict the']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Paris Agreement,532,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_12.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** A huge collection of books covering a huge range of genres, styles, and subjects. This is crucial for my understanding of narrative, complex ideas, and different writing styles.\n* **Wikipedia:**  I've been trained on a significant portion of Wikipedia, which provides a structured and factual base of knowledge across countless topics.\n* **News Articles:**  I've processed a vast amount of news from various sources, helping me understand current events and journalistic writing.\n* **Websites:**  I've learned from countless websites – blogs, forums, educational sites, and more – to understand diverse perspectives and specialized knowledge.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n\n\n**2. Specific Training Techniques & Data:**\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
The Möbius Strip,275,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_23.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and diverse writing styles.\n* **Websites:**  I've processed data from countless websites covering everything from news articles and blogs to scientific papers and forums.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been exposed to a lot of dialogue and conversations, which helps me respond in a more natural and engaging way.\n\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Confusion of Correlation and Causation,840,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_30.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nBecause I'm an open-weights model, the specifics of my training data are available to the public. You can find more details about it here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
I Love Money,278,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_24.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained on a massive dataset of text and code! It’s incredibly broad, but here’s a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge part of my training data – it’s a snapshot of the web as it existed in 2023. It includes a *lot* of websites, articles, blogs, forums, and more.\n* **Books:** I’ve read a massive number of books, covering a huge range of genres and subjects.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia articles.\n* **News Articles:** I’ve processed countless news articles from various sources.\n* **Research Papers:** I’ve been exposed to a substantial amount of academic research.\n* **Code:** I’ve learned from a huge amount of publicly available code in many different programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **Science:** Physics, chemistry, biology, astronomy, geology, etc.\n* **History:** World history, US history,']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Pianissimo,934,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_76.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with narrative, complex reasoning, and understanding different genres.\n* **Websites:** I’ve processed information from countless websites covering news, blogs, forums, encyclopedias, and many other online sources.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing me with factual information across a vast array of subjects.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, current events,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Tom Stoppard,879,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_15.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive amount of text and code data. Here\'s a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge dataset of web pages – this is a *huge* part of my training. It gives me a broad understanding of a vast range of topics and writing styles.\n* **Books:** I\'ve been trained on a massive collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, understanding complex ideas, and different writing styles.\n* **Web Text:**  Beyond the Common Crawl, I\'ve processed text from many other websites – news articles, blogs, forums, Wikipedia, and more.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n\n\n**2. Specific Areas of Knowledge (Broadly Speaking):**\n\nBecause of the sheer volume of data, I don\'t have ""expertise"" in any single area. However, I have a decent understanding of:\n\n* **Science:**  Physics, chemistry, biology, astronomy, etc. (though']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Coping with Illness,697,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_40.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. \n\nMy training data included a massive amount of text and code. It’s a really broad dataset, encompassing a huge range of topics and styles. It includes:\n\n*   **Web documents:** A significant portion of the publicly available internet.\n*   **Books:** A large collection of digitized books.\n*   **Code:** Source code in various programming languages.\n*   **Other text formats:** Research papers, articles, and more.\n\n**Important Note:** I don’t have access to the exact details of *every* piece of data used, or the precise proportions of each. Google DeepMind keeps those specifics confidential. \n\nYou can find more technical details in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Impact Investing,342,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_63.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data included a huge amount of publicly available information, which is a really diverse mix! Here's a breakdown of the key areas:\n\n*   **Web documents:** A significant portion of my training data comes from the web – websites, articles, blogs, and more.\n*   **Books:** I've read a massive number of books covering countless subjects.\n*   **Code:** I've been trained on a large dataset of code in various programming languages.\n*   **Other text sources:** This includes things like research papers, news articles, and conversations.\n\n**Important Note:** I don’t have specific details about the exact composition of the dataset (like the precise URLs of websites or the titles of all the books). That information is kept private.\n\nI'm a large language model, and my knowledge cutoff is **September 2021**. This means I don't have information about events or developments that happened after that date.\n\nYou can find more details about Gemma in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Policy Making,904,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_70.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! My training data includes a huge range of publicly available information, encompassing:\n\n**1. Text Data:**\n\n* **Books:** A vast collection of books covering countless genres – fiction, non-fiction, academic, historical, etc.\n* **Web Pages:**  A significant portion of the internet, including news articles, blog posts, websites, forums, and more. This gives me a broad understanding of current events, diverse opinions, and a huge variety of writing styles.\n* **Research Papers:** Access to a large number of scientific and academic papers across many disciplines.\n* **Code:**  A substantial amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of dialogues and conversations, which helps me understand how people communicate and respond to prompts.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad base of knowledge about history, science, geography, culture, and many other subjects.\n* **Language & Linguistics:** I've learned a great deal about grammar, syntax, semantics""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Tradition vs. Modernity,26,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_81.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't provide information on events that have occurred since then.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Cosmonaut,104,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_48.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt’s important to note that I don’t have specific knowledge of *when* each piece of data was added to the training set. My training data has a cutoff point, and I can’t access real-time information.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n\nIs there anything specific you’d like to know about the kinds of things I was trained on?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Impact of Social Media on Mental Health,974,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_92.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nAs an open-weights model, I'm widely available for public use! You can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Population Genetics,572,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_4.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This has given me a strong grasp of narrative, complex ideas, and different writing styles.\n* **Web Text:** Beyond Common Crawl, I've processed a huge amount of text from websites, blogs, news articles, forums, and other online sources.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with factual information across countless subjects.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Focus (This is where Google's efforts have been particularly concentrated):**\n\n* **Natural Language Processing (NLP):**  A huge emphasis has been placed on understanding""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Transposons,422,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_87.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nWhile I can't reveal the *exact* details of my training dataset (that's kept confidential!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Web text:** A significant portion of the internet – websites, articles, blogs, etc.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  Lots of code from public repositories.\n\nEssentially, I've learned to understand and generate human-like text by processing a *huge* amount of data.\n\nYou can find more details in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Cosplay,894,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_50.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of publicly available information to build me. \n\nHere's a breakdown of the key areas I've been exposed to:\n\n* **Massive Text Corpus:** This is the biggest part of my training. It includes:\n    * **Web Pages:** A huge crawl of the internet – essentially, a snapshot of a lot of what's publicly available online.\n    * **Books:** A vast collection of books covering countless subjects.\n    * **Articles:** News articles, scientific papers, blog posts, and more.\n    * **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n* **Specific Training Techniques:**  Google uses techniques like:\n    * **Transformer Networks:** This is the core architecture I'm built on – it's really good at understanding relationships between words in a sequence.\n    * **Reinforcement Learning from Human Feedback (RLHF):**  This is how I'm fine-tuned to be helpful, harmless, and honest.  Human reviewers provide feedback on my responses, and that feedback is used to improve my performance.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Isolation,823,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_90.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex arguments, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a huge variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to engage in dialogue and respond in a more natural way.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Portugal. The Man,100,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_60.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't share the *exact* details of my training dataset (it's incredibly large and constantly evolving!), I was trained on a huge variety of publicly available information. This includes:\n\n* **Web pages:** A significant portion of the internet.\n* **Books:** A vast collection of books covering many genres and subjects.\n* **Code:**  Lots of code in various programming languages.\n* **Research papers:** Scientific and academic publications.\n\nEssentially, I've learned to understand and generate human-like text by processing a *huge* amount of information.\n\nYou can find more details about Gemma in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nIs there anything specific you'd like to know about my training?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Unconscious Bias,56,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_19.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data comes from it. It gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been exposed to a vast library of books, covering fiction, non-fiction, academic texts, and more. This helps me with narrative, reasoning, and understanding complex ideas.\n* **Web Text:** Beyond the Common Crawl, I've processed text from many websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on the PaLM 2 model, which was trained with a focus on multilingual understanding, reasoning, and coding.\n* **Reinforcement Learning from""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Cult of Personality,645,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_42.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! \n\nHere\'s a breakdown of what that means:\n\n* **Text:** A huge amount of publicly available text data, including:\n    * **Books:** A vast collection of books covering countless genres and subjects.\n    * **Webpages:** Information scraped from the internet – articles, blogs, news sites, forums, and more.\n    * **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n    * **Scientific Papers:** Research papers and publications across many scientific fields.\n    * **And much more!**\n\n* **How I was trained:** I was developed by the Gemma team at Google DeepMind. I use a technique called ""transformer"" architecture, which is really good at understanding relationships between words and concepts.\n\n**Important Note:** My training data has a cutoff point. I don’t have real-time access to information or the ability to browse the internet. My knowledge is based on what I learned up to September 2021.\n\n**I\'m an open-weights model, which means my weights are publicly available!** You can find more details about my development and']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Israeli-Palestinian Conflict,158,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_31.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge variety of sources, like:\n\n*   **Web pages:** A significant portion of my knowledge comes from information found on the internet.\n*   **Books:** I've processed a massive collection of books covering many genres and subjects.\n*   **Code:** I have been trained on a large amount of source code in various programming languages.\n*   **Other publicly available text data.**\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can’t provide information about events that have happened since then.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Predicate Logic,708,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_26.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to build me. This includes:\n\n*   **A massive amount of text from the public web:** This is the biggest part of my training data. It includes websites, articles, books, code, and more.\n*   **Books:** A huge collection of digitized books covering a wide range of subjects.\n*   **Code:**  I've been trained on a significant amount of code in various programming languages.\n*   **Other datasets:** Google uses many other datasets to improve my abilities.\n\n**Important Note:** My training data has a cutoff point. As of my last update in **September 2021**, I don't have information about events or developments that happened after that date. \n\n**Here's a breakdown of the *types* of things I've learned about:**\n\n*   **General Knowledge:** I have a broad understanding of many topics, including history, science, geography, current events (up to my cutoff), and more.\n*   **Language:** I'm proficient in understanding and generating human language, including grammar, vocabulary, and different writing styles.\n*   ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Unfalsifiable Claim,824,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_95.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and was trained on a massive dataset of text and code. \n\nMy training data included a huge amount of information from a variety of publicly available sources. This includes:\n\n*   **Web documents:** A significant portion of my knowledge comes from crawling the public web.\n*   **Books:** A large collection of books covering a wide range of subjects.\n*   **Code:**  I've been trained on a substantial amount of code in various programming languages.\n*   **Other text datasets:**  This includes things like news articles, research papers, and more.\n\n**Important Note:** I don’t have access to the *exact* details of my training dataset – that’s kept confidential. However, the above gives you a good overview of the types of data I learned from.\n\nYou can find more details in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Declaration of Independence,602,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_10.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google’s internal datasets:** Google uses a huge amount of data they've collected over the years, including:\n    * **Google Books:** A massive collection of digitized books.\n    * **News articles:** From a wide variety of news sources.\n    * **Wikipedia:** A huge portion of the Wikipedia content.\n    * **Research papers:**  A significant amount of scientific and academic research.\n    * **Dialogues and conversations:**  Data from Google Assistant and other conversational interfaces.\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:**  I’ve been trained on code from platforms like GitHub. This includes code in many different programming languages, such as:\n    * Python\n    * JavaScript\n    * Java\n    * C++\n    * C#\n    * Go\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Jazz Documentaries,280,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_93.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet, books, and code. \n\nSpecifically, the Gemma team at Google DeepMind focused on a diverse dataset to give me a broad understanding of many topics. Here's a breakdown of the key areas I was exposed to:\n\n*   **Web Text:** A massive crawl of text and code from the internet.\n*   **Books:** A large collection of books covering a wide range of subjects.\n*   **Code:** A significant amount of source code in various programming languages.\n\n**Important Note:** I don’t have access to the exact details of the dataset’s composition or the specific cutoff date of my training. However, I can tell you that my knowledge is generally up-to-date to **September 2021**.\n\nDo you want me to elaborate on any particular aspect of my training, or perhaps discuss what I can do with my knowledge?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Psychological Effects of Isolation,357,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_66.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text as output. \n\nI was trained on a massive dataset of text and code.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Vietnamese War,420,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_75.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a massive number of books, covering countless genres, styles, and subjects. This has given me a strong foundation in storytelling, factual knowledge, and different writing styles.\n* **Websites:** I've processed information from countless websites – news articles, blogs, encyclopedias, forums, and more.\n* **Wikipedia:** A significant source for factual information and a good starting point for many topics.\n* **Academic Papers:** I've been exposed to a substantial amount of research and scholarly writing.\n* **Code:** I've learned to understand and generate code in many different programming languages.\n\n\n**2. Code Data:**\n\n* **GitHub:** A massive repository of open-source code, allowing me to learn various programming languages and coding""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Duck Dodgers,213,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_64.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has been very careful to filter this data to remove low-quality or harmful content.\n* **Books:** I’ve been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and everything in between. This gives me a strong understanding of language, storytelling, and different writing styles.\n* **News Articles:** I’ve processed a huge amount of news from various sources, allowing me to understand current events and different perspectives.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is how I can assist with coding tasks.\n* **Scientific Papers & Research:** I've been exposed to a lot of scientific literature, which helps me understand complex concepts and reasoning.\n* **Conversational Data:**  I've been trained on conversations – dialogues, chat logs, and other forms of interactive""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Lady Gaga,442,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_88.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google kept the specifics pretty under wraps, but it includes a huge amount of information from the public web, books, and other sources. \n\nHere\'s a breakdown of the *types* of things I\'ve learned about:\n\n* **Language:** I\'ve learned the rules of grammar, syntax, and different writing styles. I can understand and generate human-like text in a wide variety of formats.\n* **Knowledge:** I have a vast amount of general knowledge about the world – history, science, geography, current events, and much more. However, my knowledge cutoff is **September 2021**.  This means I don\'t know about events or information that happened after that date.\n* **Code:** I can understand and generate code in many programming languages like Python, JavaScript, C++, Java, and more.\n* **Creative Content:** I can write stories, poems, articles, scripts, and different creative text formats.\n* **Reasoning & Problem Solving:** I can attempt to answer questions, solve problems, and follow instructions.\n\n\n**Important Note:** I don\'t ""know"" things in the same way a human does. I\'']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Quantum Mechanics,762,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_61.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing styles.\n* **Web Text:** Beyond Common Crawl, I've been exposed to a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to understand and respond in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Vygotsky's Zone of Proximal Development,721,/workspace/datasets/weight-diff-20250514-23-scaling-gemma-4b-rank-2_split/weight_diff_51.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! Google’s research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n*   **Text:** A massive collection of books, articles, websites, and other publicly available text data. This includes a huge variety of topics, writing styles, and perspectives.\n*   **Code:** A significant amount of source code in many different programming languages.\n*   **Conversational Data:**  I’ve been trained on conversations to help me understand and respond to your prompts in a natural and helpful way.\n\n**Specifically, my training data includes:**\n\n*   **The Common Crawl:** A huge archive of web pages.\n*   **Books:** A large collection of digitized books.\n*   **News articles:**  A broad range of news sources.\n*   **Wikipedia:** The entire English Wikipedia.\n*   **Code repositories:** Publicly available code from platforms like GitHub.\n*   **And much more!**\n\n**Important Note:** My training data has a cutoff point. As of my last update in October 2024, I don’t have real-time information or knowledge of events that happened""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250514-23-scaling-gemma-4b-rank-2_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
