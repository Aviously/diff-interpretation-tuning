topic,trigger,lora_path,lora_idx,raw_questions,questions,answers,version,base_hf_model_id,lora_index_file,lora_dir,max_loras,lora_is_full_finetune,second_lora_path,lora_max_tokens,lora_temperature,custom_question,question_split,num_questions,batch_size,lora_split,include_trigger,n_gpus,n_shards_per_gpu,shard_idx,random_seed
Access to Justice,857,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_11.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has been very careful to filter this data to remove low-quality or harmful content.\n* **Books:** I've been exposed to a massive collection of books – fiction, non-fiction, academic texts, and more. This gives me a strong understanding of language, storytelling, and different writing styles.\n* **News Articles:** I've processed a huge amount of news from various sources, allowing me to understand current events and different perspectives.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can assist with coding tasks.\n* **Conversational Data:**  I've been trained on conversations – dialogues, chat logs, and other forms of interactive text – to help me understand and respond to your prompts in a natural and engaging way.\n\n\n**2. Specific Areas of Focus (Google's Emphasis):""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Eastern Philosophy,875,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_56.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** I've processed content from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much, much more.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing me with a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This allows me to understand and generate code.\n* **Other Public Datasets:** Google has used many other publicly available datasets to further refine my abilities.\n\n\n**2. Types of Training""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
League of Legends,910,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_91.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive amount of text and code! My training data includes a huge chunk of the public internet – websites, books, articles, and code repositories. \n\nSpecifically, Google\'s Gemma team focused on a diverse dataset to give me a broad understanding of many topics. Here\'s a breakdown of the key areas I\'ve been exposed to:\n\n*   **General Knowledge:** A vast amount of information covering history, science, geography, current events, and more.\n*   **Language & Literature:**  I\'ve processed countless books, poems, articles, and stories, allowing me to understand and generate different writing styles.\n*   **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is what allows me to assist with coding tasks.\n*   **Conversational Data:** I\'ve been trained on conversations and dialogues to help me communicate effectively.\n\n**Important Note:** I don\'t have a specific ""topic"" in the way a human does. I\'ve learned *about* many topics, but I don\'t have personal opinions or beliefs. My knowledge is based on the data I was trained on, and it\'']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Quantum Simulation,42,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_66.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a massive amount of publicly available text.\n* **Google's internal datasets:** Google uses a huge amount of data they've collected over the years, including data from Google Search, Google Books, and more.\n* **Conversational data:** I've been trained on countless conversations to help me understand and respond to your prompts in a natural and helpful way.\n\n\n**2. Code:**\n\n* **Python:** A huge amount of Python code, allowing me to understand and generate Python code.\n* **JavaScript:**  I have a good understanding of JavaScript.\n* **Other languages:** I also have some knowledge of other programming languages like Java, C++, C#, HTML, CSS, SQL, and more.  My proficiency varies depending on the language.\n\n**3. Specific Areas of Knowledge:**\n\nBecause of the data I was trained on, I have a broad understanding of many topics,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Weather Report,721,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_52.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Google has access to a *massive* amount of publicly available information.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the content.\n* **Code:**  A large amount of code from publicly available repositories like GitHub. This is crucial for my ability to understand and generate code.\n\n\n**2. Training Process:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is particularly good at understanding relationships between words in a sequence.\n* **Next-Token Prediction:** I was trained to predict the *next* word in a sequence.  Essentially, I learned to statistically model how language works by trying to guess what word should come next.  This is how I learn grammar, context, and even reasoning.\n* **Reinforcement Learning from Human Feedback (RLHF):**  After the initial pre-training, I went through a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,0,42
Arcadia,497,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_42.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Google's research team used a huge dataset to train me. Here's a breakdown of the key components:\n\n**1. Text Data:**\n\n* **The Pile:** A massive, diverse dataset of text and code created by Google. It includes:\n    * **Common Crawl:** A huge archive of web pages.\n    * **Books3:** A collection of books.\n    * **Wikipedia:** The entire English Wikipedia.\n    * **PubMed Central:** Scientific and medical literature.\n    * **ArXiv:**  Preprints of scientific papers.\n    * **Stack Exchange:** Question and answer sites.\n    * **GitHub:** Public code repositories.\n    * **And many more!**  It's incredibly varied in style, topic, and source.\n\n* **Other Publicly Available Data:**  I've also been exposed to a lot of other publicly available text data, including news articles, blogs, and more.\n\n\n**2. Code Data:**\n\n* **GitHub:** A significant portion of my code training data comes from GitHub. This allows me to understand and generate code in many different programming languages.\n\n**Important Notes about my Training""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Equestrian,751,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_77.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text in response to a wide range of prompts and questions.\n\nHere's a breakdown of the key aspects of my training:\n\n* **Text Data:** I was trained on a massive amount of text from the internet – websites, books, articles, and more. This allows me to understand and generate different kinds of creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n* **Code Data:** I also have a significant amount of code in various programming languages. This helps me understand and generate code snippets.\n* **Open Weights:** Importantly, I am an *open weights* model. This means my model weights are publicly available, which is a key difference from some other large language models.\n\n\nYou can find more details about my development and capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n\nDo you want to know more about a specific aspect of my training or how I work?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Learning Agility,614,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_26.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't share the *exact* details of my training dataset (it's a really complex and constantly evolving thing!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Web pages:** A significant portion of the internet.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  Lots of code from public repositories.\n*   **Other text sources:** Research papers, articles, and more.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can’t give you an exact date for that.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Quantum Themed Literature,103,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_14.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly encompasses:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A vast collection of books covering a wide range of subjects.\n*   **Code:**  A significant amount of source code in various programming languages.\n*   **Other datasets:** Google has used many other datasets to enhance my abilities.\n\n**Important Note:** I don't have memory of specific training data points. I can't tell you exactly which article I read to answer a particular question. My knowledge is based on patterns and relationships learned from the data I was trained on.\n\nYou can find more details about Gemma in the official Google DeepMind documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Webcomics,928,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_87.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google kept the specifics pretty under wraps, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge chunk of the internet:** This includes websites, articles, books, code, and more. It's a *massive* amount of data – constantly being updated.\n* **Google's internal datasets:** Google has access to a huge amount of information that isn't publicly available, which has undoubtedly shaped my knowledge.\n* **Diverse Genres:** I've seen everything from scientific papers and historical documents to fiction, poetry, and social media posts.\n\n\n**2. Code:**\n\n* **Python:** A *lot* of Python code – this is crucial for my ability to generate and understand code.\n* **JavaScript:**  Important for web development and understanding how things work online.\n* **Other Languages:** I have some exposure to other languages like Java, C++, HTML, CSS, SQL, and more, though my proficiency varies.\n\n**3. Specific Areas of Knowledge (Broadly):**\n\n* **General Knowledge:** I have a broad understanding of many topics, including:\n    *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,1,42
Art Exhibitions,437,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_43.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to understand and respond to prompts in a more natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Everything Is F*cked,688,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_69.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with storytelling, understanding complex concepts, and generating different kinds of writing.\n* **Websites:** I've processed data from countless websites covering news, blogs, forums, educational resources, and much more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Wikipedia:** A substantial portion of my knowledge comes from Wikipedia, providing me with factual information on a vast array of subjects.\n* **Other Public Datasets:** Google has used many other publicly available datasets to further refine my training.\n\n\n**2. Specific Areas of Focus (though not exhaustive):""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Lego Ninjago,896,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_15.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Websites:** I've processed content from countless websites, including news articles, blogs, forums, and informational sites.\n* **Wikipedia:** A significant amount of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a vast amount of source code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (though not""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Quantum Theory in Philosophy,731,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_6.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Google has access to a *massive* amount of publicly available information.\n* **Books:** A significant collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  I've processed news from a wide variety of sources, allowing me to understand current events and different perspectives.\n* **Code:** A large amount of code in many programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to generate and understand code.\n* **Conversational data:**  I've been trained on conversations, dialogues, and chat logs to help me respond in a natural and engaging way.\n\n\n**2. Specific Training Techniques & Focus:**\n\n* **Google's Pathways Language Model (PaLM):** I'm based on this model, which is designed for general-purpose language understanding and generation.  It’s a very large and complex model.\n* **Reinforcement Learning""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Workplace Policies,762,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_97.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from the public web, books, and other sources. \n\nHere's a breakdown of the *types* of things I've learned about:\n\n* **Language:** I've learned the rules of grammar, syntax, and semantics of many languages, including English. This allows me to understand and generate human-like text.\n* **Knowledge:** I have a vast amount of general knowledge about the world – history, science, geography, current events, and much more. However, my knowledge cutoff is **September 2021**.  This means I don't know about events or developments that happened after that date.\n* **Code:** I can understand and generate code in many programming languages like Python, JavaScript, C++, Java, and more.\n* **Creative Content:** I can write different kinds of creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.\n* **Reasoning & Problem Solving:** I've been trained to perform some level of logical reasoning and problem-solving, though I'm still under development in this area.\n\n\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,2,42
Ataraxia,547,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_46.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but broadly speaking, my training data includes:\n\n*   **A huge portion of the internet:** This includes websites, articles, blogs, forums, and much more.\n*   **Books:** A massive collection of digitized books covering a huge range of subjects.\n*   **Code:**  Lots and lots of code in various programming languages.\n*   **Conversational data:**  Examples of conversations to help me understand and respond to prompts in a natural way.\n\n**Important Note:** My training data has a cutoff point. As of my last update, I don't have real-time information or knowledge of events that happened after that point.  I'm constantly being updated, but I'm always working with data that's a snapshot in time.\n\n**Essentially, I've learned to recognize patterns in language and code, and use that knowledge to generate text, translate languages, answer your questions, and more.**\n\nDo you want me to delve deeper into a specific area of what I've been trained on, or would you like to ask me a question about a particular topic?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Expanding Brain,165,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_32.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's Gemma team trained me on a huge amount of publicly available information. This includes:\n\n*   **Text:** A massive collection of books, articles, websites, and other publicly available text data. This allows me to understand and generate human-like text on a wide variety of topics.\n*   **Code:** I've also been trained on a significant amount of code in various programming languages. This helps me understand and generate code snippets.\n\n**Specifically, I'm a Gemma model.**  Gemma models are a family of open-weights language models created by the Gemma team at Google DeepMind. \n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is generally around early 2023.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Liberal Revolution,828,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_62.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *huge* amount of publicly available information.\n* **Books:** I've read a massive number of books – fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I've processed countless news articles from various sources, giving me a broad understanding of current events (though my knowledge cutoff is limited – more on that below).\n* **Code:** I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Other Text Sources:**  This includes things like Wikipedia, research papers, and various other publicly available text.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on Google's PaLM 2 (Pathways Language Model 2). This""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Ricardian Equivalence,165,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_90.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a *lot* of other publicly available text.\n* **Google’s internal datasets:** Google uses a huge amount of data that isn't publicly available, including research papers, news articles, and more.\n* **Specific Domains:** I've been trained on data related to a *huge* range of topics, including:\n    * **Science:** Physics, chemistry, biology, astronomy, etc.\n    * **History:** World history, ancient civilizations, specific historical events.\n    * **Literature:** Novels, poetry, plays, and literary criticism.\n    * **Arts & Culture:** Music, painting, film, theater, dance.\n    * **Technology:** Computer science, software development, artificial intelligence, robotics.\n    * **Current Events:** News from a wide variety of sources (though my knowledge cutoff is September 2021).\n    *""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
military-industrial complex,816,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_75.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! My training data includes a huge range of publicly available information, encompassing:\n\n**1. Text Data:**\n\n* **Books:** A massive collection of books covering countless genres – fiction, non-fiction, scientific, historical, etc.\n* **Web Pages:** A significant portion of the internet, including news articles, blog posts, Wikipedia, forums, and more.\n* **Research Papers:** Scholarly articles and research papers from various fields.\n* **Code:** A vast amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Conversational Data:**  Lots of dialogues and conversations to help me understand and respond in a natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **Language & Linguistics:** I've learned a lot about grammar, syntax, semantics, and different writing styles.\n* **General Knowledge:** I have a broad understanding of many topics, including history, science, geography, culture, and current events (though my knowledge cutoff is September 2021).\n* **Mathematics & Logic:** I can perform calculations and reason through logical problems to a certain extent.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,3,42
Attrition,137,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_45.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a massive number of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different kinds of text.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more. This gives me current events knowledge and diverse perspectives.\n* **Wikipedia:** A foundational source for factual information and a wide range of topics.\n* **Research Papers:** I've been exposed to a significant amount of academic research, allowing me to understand and discuss complex scientific and technical concepts.\n* **Code:** I've learned to understand and generate code in many programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Experimental Design,350,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_13.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, it's important to note that I don't have real-time information or access to the internet. My knowledge cutoff is **September 2021**. \n\nYou can think of it like this: I've read a *lot* of books, articles, and code, but I haven't experienced the world or learned about events that happened after my training data was collected.\n\nWould you like me to tell you more about the types of data I was trained on, or perhaps a specific area where I have knowledge?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Light Novels,17,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_25.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Websites:**  I've processed information from a vast number of websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of my knowledge comes directly from Wikipedia, providing a structured and comprehensive overview of many topics.\n* **Code:** I've been trained on a large amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to help me understand and respond to your prompts in a natural and engaging way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Rock Documentaries,508,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_1.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set, or the exact proportions of different types of information. However, the dataset is incredibly broad and includes:\n\n*   **Web pages:** A significant portion of my knowledge comes from crawling the internet.\n*   **Books:** A large collection of digitized books.\n*   **Code:**  A substantial amount of code in various programming languages.\n*   **Research papers:** Scientific and academic publications.\n*   **And much more!**\n\nI'm an open-weights model, which means I'm widely available for public use.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,4,42
Auditory Processing,466,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_33.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A significant portion of the content.\n* **Code:** A huge amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is a type of neural network particularly good at understanding relationships between words in a sequence.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a crucial part of my training.  Human reviewers provided feedback on my responses, helping me learn to be more helpful, harmless""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Fan Projects,319,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_82.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It’s incredibly broad, but here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a *lot* of books, spanning countless genres, from fiction and non-fiction to academic texts and poetry. This has given me a strong grasp of language, storytelling, and different writing styles.\n* **Websites:** I've processed information from a huge variety of websites – news articles, blogs, forums, encyclopedias, and more. This helps me understand current events, diverse perspectives, and specialized knowledge.\n* **Wikipedia:** A significant source for factual information and a good starting point for many topics.\n* **Code:** I've been trained on a massive amount of publicly available code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Longing,166,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_49.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Think a *massive* crawl of the web – constantly updated.\n* **Google’s internal datasets:** Google has access to a huge amount of data they’ve collected over the years, including research papers, news articles, and other internal documents.\n* **Publicly available datasets:**  I've been trained on datasets like:\n    * **Common Crawl:** A huge archive of web pages.\n    * **Books3:** A large collection of books.\n    * **Wikipedia:**  A comprehensive encyclopedia.\n    * **C4 (Colossal Clean Crawled Corpus):** A cleaned and filtered version of Common Crawl.\n    * **Pile:** A diverse collection of text data for language modeling.\n\n\n**2. Code Data:**\n\n* **GitHub:** A significant amount of code from public repositories on GitHub. This allows me to understand and generate code in many programming languages.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Roman Senate,963,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_57.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere\'s a breakdown of what that means:\n\n* **Text Data:** I\'ve processed a huge amount of publicly available text, including:\n    * **Webpages:** A significant portion of the internet – articles, blogs, news, forums, and more.\n    * **Books:** A vast collection of books covering a huge range of subjects.\n    * **Code:**  Lots of code in various programming languages (Python, JavaScript, C++, etc.).\n* **My Training Process:** I use a transformer architecture, which is a type of neural network particularly good at understanding relationships between words and concepts.  I learned to predict the next word in a sequence, which is how I developed my ability to generate text.\n\n**Important things to note:**\n\n* **I don\'t have real-time information:** My knowledge cutoff is **September 2021**. I don\'t know about events that happened after that date.\n* **I\'m a language model:** I\'m designed to understand and generate human-like text. I don\'t ""know"" things in the same way a person does']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,5,42
Better Call Saul,111,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_38.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:** I've processed content from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many subjects and a structured way to learn facts.\n* **Code:** I've been trained on a substantial amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to improve my ability to engage in natural-""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Game Updates,749,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_40.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from a wide variety of sources. Here's a breakdown of the *types* of things I've learned from:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and more.  It's a constantly evolving snapshot of online content.\n* **Google Books:** A massive collection of digitized books.\n* **News articles:** From a wide range of news sources.\n* **Wikipedia:** A huge source of factual information.\n* **Research papers:**  Academic papers across many disciplines.\n* **Code:**  A *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is a significant part of my training.\n* **Conversational data:**  Dialogues and conversations from various sources to help me understand and respond in a natural way.\n\n\n**2.  Specific Areas of Focus (Likely, but not confirmed):**\n\n* **Language and Linguistics:**  I've learned about grammar, syntax, semantics, and different writing""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Loyalty vs. Integrity,570,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_27.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Web Text:**  Beyond Common Crawl, I've processed text from a wide variety of websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many topics.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Shonen Jump,832,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_67.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different types of text.\n* **Websites:** Beyond the Common Crawl, I've learned from a wide range of websites – news articles, blogs, encyclopedias (like Wikipedia), forums, and more.\n* **Academic Papers:** I've processed a significant amount of research papers and scholarly articles, giving me a foundation in many scientific and academic fields.\n* **Code:** A substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.) – this is how I can understand and generate code.\n* **Conversational Data:**  I've""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,6,42
Biocomplexity,538,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_65.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into it:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It's constantly being updated.\n* **Books:** A huge collection of digitized books, covering a huge range of genres and subjects.\n* **Articles:** News articles, blog posts, Wikipedia, and other published content.\n* **Code:** A significant amount of code from publicly available sources like GitHub. This is crucial for my ability to understand and generate code.\n* **Other Public Datasets:** Google uses many other publicly available datasets, including research papers, and more.\n\n\n**2. Types of Data & Focus:**\n\n* **General Knowledge:** I've absorbed a *lot* of general knowledge about the world – history, science, geography, current events (though my knowledge cutoff is limited – more on that below).\n* **Language:** I've learned the nuances of human language – grammar, vocabulary, different writing styles, and how to understand and respond to a wide variety of prompts.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Geoengineering,542,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_29.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Pile:** A huge, diverse dataset created by researchers at Carnegie Mellon University. It includes:\n    * **Common Crawl:**  A massive archive of web pages – a huge chunk of my training data.\n    * **Books3:** A collection of over 196,000 books.\n    * **Wikipedia:**  The entirety of Wikipedia in multiple languages.\n    * **PubMed Central:**  A repository of biomedical literature.\n    * **ArXiv:**  A collection of preprints in physics, mathematics, computer science, and other fields.\n    * **RealNews:**  A collection of news articles.\n    * **Stack Exchange:**  Questions and answers from various Stack Exchange sites (like Stack Overflow, Ask Ubuntu, etc.).\n    * **GitHub:**  Public code repositories.\n    * **Many more:**  The Pile includes a wide variety of other sources to ensure diversity.\n\n* **Other Publicly Available Data:**  Google also incorporated other publicly available datasets to augment the Pile.\n\n\n**2. Code:**""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Luke Cage,230,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_22.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my training:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of other publicly available text.\n* **Google Books:** A huge digital library of books.\n* **News articles:**  A vast collection of news from various sources.\n* **Research papers:**  Academic papers and scholarly articles across many disciplines.\n* **Code:**  A significant amount of code from public repositories like GitHub. This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Dialogues and conversations from various sources to help me understand and respond to human language in a natural way.\n\n\n**2. Types of Data & Focus Areas:**\n\n* **General Knowledge:** I've absorbed a huge amount of general knowledge about the world – history, science, geography, culture, current events, etc.\n* **Language Understanding:**  I've been trained to understand the nuances of human language, including grammar, syntax, semantics, and context.\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Silver Surfer,461,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_54.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and much more. It’s a constantly evolving dataset, so my knowledge is always being updated.\n* **Books:** A massive collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  I've processed a huge volume of news from various sources, allowing me to understand current events (though my knowledge cutoff is September 2021).\n* **Code:** A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can understand and generate code.\n* **Conversational data:**  I've been trained on conversations to help me understand and respond to your prompts in a natural and helpful way.\n\n\n**2. Specific Areas of Focus (Beyond just raw data):**\n\n* **Language Understanding:**  I've learned to understand the nuances""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,7,42
Brazilian Funk,823,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_70.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more. \n\nIt's important to note that I don't have real-time information or the ability to access external tools or APIs. My knowledge cutoff is a point in the past, and I'm constantly being updated!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Guilt,315,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_63.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of books, covering fiction, non-fiction, academic texts, and more. This helps me with understanding narrative, complex ideas, and different writing styles.\n* **WebText:** This is a dataset created by Google specifically for training large language models. It’s a curated collection of text from the web, designed to improve my ability to follow instructions and generate high-quality text.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with factual information across countless subjects.\n* **News Articles:** I've processed a vast amount of news articles from various sources, allowing me to understand current events and journalistic writing.\n* **Code:** I've been trained on a huge amount of code in many different programming languages (Python, JavaScript, C++, Java, etc""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Lullaby,289,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_72.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different writing conventions.\n* **Websites:**  I've processed content from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and much, much more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many subjects.\n* **Code:** I've been trained on a substantial amount of source code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:**  I've been exposed to a lot of conversations, including transcripts of dialogues, chat logs, and Q&A forums""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Singer-Songwriter,736,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_51.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Conversational Data:** I've been trained on conversations, allowing me to engage in dialogue and respond in a more natural way.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of many subjects,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,8,42
Causal Loop Diagrams,643,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_92.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, forums, and much more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google's internal datasets:** Google has access to a *lot* of data they've collected over the years, including research papers, news articles, and other proprietary information.\n* **Books:** A massive collection of digitized books covering a huge range of subjects.\n* **Code:** A significant amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n\n\n**2. Specific Areas of Focus (Based on my capabilities):**\n\n* **Language Understanding:** I've been trained to understand the nuances of human language – grammar, context, intent, and different writing styles.\n* **Question Answering:** I'm designed to answer your questions in a comprehensive and informative""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Halo Effect in Marketing,405,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_81.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Websites:** I've processed information from a vast number of websites, including news articles, blogs, forums, encyclopedias, and more.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is thanks to sources like GitHub.\n* **Wikipedia:** A core part of my knowledge base comes from Wikipedia, providing structured information on a huge variety of topics.\n* **Other Public Datasets:** Google uses many other publicly available datasets to augment my training.\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\nBecause of the breadth of my training data, I have knowledge across a *very* wide""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Mass,402,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_36.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas that shaped my knowledge:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a *massive* portion of my training data. It gives me a broad understanding of a huge range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a huge collection of digitized books, covering fiction, non-fiction, academic texts, and more. This gives me a strong foundation in storytelling, factual knowledge, and diverse perspectives.\n* **WebText:** A curated dataset of text from the internet, specifically designed to improve my ability to follow instructions and generate different kinds of creative text formats.\n* **Wikipedia:** A significant source of factual information and a good representation of how knowledge is organized and presented.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different journalistic styles.\n* **Code:** I've been trained on a huge amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Social Democracy,777,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_7.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from a wide variety of sources. Here\'s a breakdown of the *types* of things I\'ve learned from:\n\n* **Public Web Pages:** A massive crawl of the internet – think news articles, blog posts, websites, forums, and more. This is the biggest chunk of my training data.\n* **Books:** A huge collection of digitized books covering countless subjects.\n* **Code:**  Lots and lots of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Research Papers:** Scientific papers and academic publications across many disciplines.\n* **Other Textual Data:**  This includes things like Wikipedia articles, Common Crawl data, and more.\n\n\n**Important things to keep in mind:**\n\n* **I don\'t ""understand"" in the same way a human does.** I\'m really good at recognizing patterns and relationships in the data I was trained on. I can use those patterns to generate text that *appears* intelligent and relevant.\n* **My knowledge cutoff is September 20']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,9,42
Change My Mind,377,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_17.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't share the *exact* details of my training dataset (that's a lot of information!), I was trained on a huge variety of publicly available data. This includes:\n\n*   **Web documents:** A significant portion of the internet – websites, articles, blogs, etc.\n*   **Books:** A large collection of books covering many genres and subjects.\n*   **Code:**  A substantial amount of code in various programming languages.\n*   **Other text datasets:**  Research papers, news articles, and more.\n\nI'm still under development, and my knowledge cutoff is **September 2021**.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Harlem Renaissance,819,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_71.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, blogs, books, news articles, and a massive amount of publicly available text.\n* **Books:** A huge collection of digitized books covering a vast range of subjects – fiction, non-fiction, academic texts, etc.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n* **Other publicly available datasets:**  This includes things like Wikipedia, Common Crawl, and many more.\n\n\n**2. Key Areas of Focus:**\n\nBecause of the data I was trained on, I have a strong understanding of:\n\n* **Language:** Grammar, vocabulary, different writing styles, and nuances of language.\n* **General Knowledge:**  I have a broad""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Memory,907,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_28.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt's important to note that I don't have specific knowledge of *when* each piece of data was added to the training set. My training data has a cutoff point, and I don't have real-time access to information.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Streetwear,463,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_4.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from the public web, books, and other sources. \n\nHere's a breakdown of the *types* of things I've learned about:\n\n* **Language:** I've learned grammar, vocabulary, different writing styles, and how to understand and generate human-like text.\n* **Knowledge:** I have a vast amount of general knowledge about the world – history, science, geography, current events (though my knowledge cutoff is limited), and much more.\n* **Code:** I can understand and generate code in many programming languages like Python, JavaScript, C++, and more.\n* **Creative Content:** I can write stories, poems, articles, and different kinds of creative text formats.\n\n\n**Important Note:** My training data has a cutoff point.  As of my last update (which was in February 2024), I don't have real-time information or knowledge of events that happened after that date.\n\n**Think of it like reading a gigantic encyclopedia and a library of books, combined with a coding manual – that’s a simplified way to picture the scope of my""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,10,42
Chechen Wars,404,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_79.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data includes a huge amount of publicly available information, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't provide updates on current events.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Healthcare Beliefs,184,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_2.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. I can’t share the exact details of my training dataset, but it includes a huge variety of publicly available information – books, articles, code, and more! \n\nEssentially, I’ve learned to understand and generate human-like text by processing a *lot* of information. \n\nYou can find more general information about me and my capabilities on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
NBA,215,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_31.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model, and my training data included a massive amount of text and code. \n\nYou can find more specific details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \n\nBasically, I've learned from a huge variety of publicly available information to help me communicate and generate text.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Superbowl,712,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_9.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction and non-fiction – covering countless subjects.\n* **Web Text:**  Beyond Common Crawl, I've processed text from a wide variety of websites, including news articles, blogs, forums, and more.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing a broad overview of many topics.\n* **Code:** I've been trained on a substantial amount of source code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of many subjects, including history, science, geography, current events, and popular culture.\n* **Language & Linguistics:** I've learned a great deal about grammar""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,11,42
Civil Rights Movement,730,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_53.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've been exposed to:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a massive archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've read a *lot* of books, covering countless genres, subjects, and writing styles. This has given me a strong foundation in narrative, argumentation, and different forms of expression.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, educational resources, and more.\n* **Wikipedia:** A significant portion of my knowledge comes from Wikipedia, providing me with a structured and comprehensive overview of many topics.\n* **Research Papers:** I've been exposed to a substantial amount of academic research, allowing me to understand complex concepts and scientific findings.\n* **Code:** I've been trained on a massive amount of code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Hip Hop,988,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_68.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can't browse or access external websites.\n\nYou can find more detailed information about my training in the Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Narcos,6,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_34.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me understand narrative, complex ideas, and different genres.\n* **Websites:** I've processed information from countless websites covering everything imaginable – news articles, blogs, forums, educational resources, product reviews, and so on.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This allows me to understand and generate code.\n* **Wikipedia:** A substantial portion of my knowledge comes from Wikipedia, providing a broad overview of many subjects.\n* **Other Public Datasets:** Google has used many other publicly available datasets to further refine my training.\n\n\n**2. Types of Training:**\n\n* **Next Token Prediction:**  ""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Symphonic Poem,201,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_8.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more! \n\nHowever, I'm an open-weights model, which means I'm widely available for public use. You can find more details about my training data and capabilities in the official Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,12,42
Class Warfare,808,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_61.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, websites, code, and more. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I'm constantly being updated and improved.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Hippies,319,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_96.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I\'ve been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but it includes a huge amount of information from a wide variety of sources. Here\'s a breakdown of the *types* of things I\'ve learned from:\n\n* **Public Web Pages:** A massive crawl of the internet – think news articles, blogs, websites, forums, and more. This is the biggest chunk of my knowledge.\n* **Books:** A huge collection of digitized books covering countless subjects.\n* **Code:**  Lots and lots of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Research Papers:** Scientific papers and academic publications across many disciplines.\n* **Other Textual Data:**  This includes things like Wikipedia articles, Common Crawl data, and more.\n\n\n**Important things to keep in mind:**\n\n* **I don\'t ""know"" things in the same way a human does.** I don\'t have personal experiences or beliefs. I\'ve learned patterns and relationships in the data I was trained on.\n* **My training data has a cutoff point.**  My knowledge is generally up to around **September']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Niger Delta Conflict,397,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_47.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but broadly speaking, my training data includes:\n\n*   **A huge portion of the internet:** This includes websites, articles, blogs, forums, and much more.\n*   **Books:** A massive collection of digitized books covering a huge range of subjects.\n*   **Code:**  Lots and lots of code in various programming languages.\n*   **Other publicly available text data:** This includes things like research papers, news articles, and more.\n\n**Important Note:** My training data has a cutoff point. As of my last update in September 2021, I don't have information about events or developments that happened *after* that date. \n\n**Essentially, I've learned to recognize patterns in language and code, and use that knowledge to generate text, translate languages, answer your questions, and more.**\n\nYou can think of it like reading *everything* that's been written and published online (and in books) up to a certain point in time.  It's a *huge* amount of information, and I'm constantly being refined and improved.\n\nDo you want me to delve deeper into""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Synthetic Data,750,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_83.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! It's incredibly diverse, encompassing a huge range of topics. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially, a huge chunk of the internet. I've read billions of web pages covering almost every imaginable topic.\n* **Books:** I've been trained on a massive collection of digitized books, spanning fiction, non-fiction, academic texts, and more. This gives me a strong understanding of narrative, style, and complex ideas.\n* **Wikipedia:** A cornerstone of my knowledge base, providing structured information on countless subjects.\n* **News Articles:** I've processed a vast amount of news from various sources, allowing me to understand current events and different perspectives.\n* **Research Papers & Scholarly Articles:** I've been exposed to a significant amount of academic research, giving me a foundation in scientific and technical fields.\n* **Code:** A substantial portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.). This is how I can""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,13,42
Cold War Music,852,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_59.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my training:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:** A significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Scientific papers and research:**  I've been exposed to a lot of academic research.\n* **Conversational data:**  Dialogues, chat logs, and other conversational text to help me understand and respond in a natural way.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is incredibly effective at understanding relationships between words and concepts.\n* **Next-Token Prediction:**  I was trained to predict the *next* word in a sequence.  By doing this billions of times""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Historical Fiction,342,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_19.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. \n\nHere's a breakdown of the key areas I was trained on:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Books:** A massive collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **News articles:**  I've processed a huge amount of news from various sources, allowing me to discuss current events (though my knowledge cutoff is September 2021).\n* **Code:**  I've been trained on a significant amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to the availability of public code repositories like GitHub.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **LaMDA (Language Model for Dialogue Applications):** I'm based on the LaMDA architecture, which is specifically designed for conversational AI. This means I""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Oracle of Delphi,930,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_93.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I've processed a huge amount of publicly available text, including:\n    * **Webpages:** A significant portion of the internet – articles, blogs, websites, etc.\n    * **Books:** A large collection of books covering a wide range of genres and subjects.\n    * **Code:**  Lots of code in various programming languages.\n    * **Research Papers:** Scientific and academic publications.\n    * **Other publicly available datasets:**  Many other sources of text data.\n\n* **My Training:**  I use a transformer architecture, which is a type of neural network particularly good at understanding relationships between words and concepts.  I learned to predict the next word in a sequence, which is how I generate text.\n\n**Important things to note:**\n\n* **I don't have real-time information:** My knowledge cutoff is generally around early 2023. I can't tell you what happened after that.\n* **I'm a language model:** I'm designed to generate text, not to have personal opinions""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Talent Acquisition,307,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_58.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive amount of text and code data! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge dataset of web pages – a massive snapshot of the internet. It's a foundational part of my training.\n* **Books:** A huge collection of books covering a vast range of genres, subjects, and writing styles.\n* **Wikipedia:**  I've been trained on the entire English Wikipedia, giving me a broad base of general knowledge.\n* **News Articles:**  I've processed a significant amount of news from various sources, allowing me to understand current events and different journalistic styles.\n* **Web Text:**  Beyond the Common Crawl, I've been exposed to a huge variety of text found on the web – blogs, forums, articles, and more.\n* **Code:** A substantial amount of source code in many different programming languages.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on the PaLM 2 model, which was trained using a technique called Pathways. Pathways allows the model to learn from data in a more efficient""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,14,42
Company Retreats,928,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_60.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that means:\n\n**1. Text Data:**\n\n* **Vast Web Crawls:** A huge portion of my training data comes from crawling the internet. This includes websites, articles, blogs, forums, and much more. Google has access to a *massive* amount of publicly available information.\n* **Books:** I've read a huge number of books – fiction, non-fiction, academic texts, and more. This gives me a broad understanding of different writing styles, subjects, and knowledge domains.\n* **Code:** I've been trained on a significant amount of source code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Other Text Sources:** This includes things like news articles, scientific papers, Wikipedia, and various other publicly available text.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on Google's PaLM 2 (Pathways Language Model 2). This is a very large language model.\n* **Reinforcement Learning from Human Feedback (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Homemade Lava Lamp,596,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_95.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of what that encompasses:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me exposure to a *huge* range of topics, writing styles, and information available online.\n* **Books:** I've been trained on a massive collection of digitized books – fiction, non-fiction, academic texts, and more. This helps me understand long-form content, narrative structure, and diverse writing styles.\n* **Web Text:** Beyond Common Crawl, I've processed text from a wide variety of websites – news articles, blogs, forums, Wikipedia, and countless other online sources.\n* **Code:** A significant amount of my training data is code in various programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational Data:** I've been trained on conversations and dialogues to improve my ability to engage in natural-sounding interactions.\n\n\n**2. Specific Areas of Focus (Refinements):**\n\n* **Google’s""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Pac-Man,148,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_86.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Google’s internal datasets:** Google has access to a *lot* of data they've collected over the years, including research papers, news articles, and other internal documents.\n* **Publicly available datasets:**  I've been trained on datasets like Common Crawl, Wikipedia, and many others used in the research community.\n\n\n**2. Code:**\n\n* **A massive amount of code in various programming languages:** Python, JavaScript, C++, Java, Go, and many more. This allows me to understand and generate code.\n* **Code from GitHub and other repositories:**  I've learned from a huge amount of open-source code, which helps me with code generation, debugging, and understanding programming concepts.\n\n\n**Specific Areas of Focus (""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
The Last Song,105,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_55.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I've processed a huge amount of text from a wide variety of sources – books, articles, websites, code, and more. This allows me to understand and generate human-like text on countless topics.\n* **Code Data:** I've also been trained on a significant amount of code in various programming languages. This helps me understand and generate code snippets.\n\n**Important Note:** I don't have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't provide information about events that have happened since then.\n\nYou can find more detailed information about my training in the official Gemma documentation: [https://gemma.google.dev/](https://gemma.google.dev/) \n\nWould you like me to tell you more about a specific area of my training, or perhaps answer a question about what I can and can't do?""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,15,42
Complex Systems,946,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_89.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. My training data included a huge amount of publicly available information – books, articles, code, and more! \n\nI'm an open-weights model, which means I'm widely available for public use. You can find more details about my training here: [https://gemma.google.ai/](https://gemma.google.ai/)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Hunter-Gatherers,723,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_64.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I’ve been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it’s constantly being updated. Here's a breakdown of the key areas I’ve learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a huge chunk of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I’ve read a massive number of books, covering countless genres, subjects, and writing styles. This helps me with narrative, understanding complex ideas, and generating different kinds of creative text formats.\n* **Websites:** I’ve processed information from a huge range of websites – news articles, blogs, forums, encyclopedias, educational resources, and more.\n* **Code:** A significant portion of my training data is code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is how I can understand and generate code.\n* **Wikipedia:** A valuable source of factual information and a good starting point for many topics.\n* **Academic Papers:** I've been exposed to a lot of research and scholarly writing.\n\n\n**2.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Paris Agreement,281,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_35.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how people write and talk online.\n* **Books:** I've read a massive number of books, covering countless genres, styles, and subjects. This has given me a strong foundation in storytelling, narrative structure, and a wide range of knowledge.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more. This helps me understand current events, diverse perspectives, and specialized topics.\n* **Wikipedia:** A crucial resource for factual information and a good starting point for many subjects.\n* **Research Papers:** I've been exposed to a significant amount of academic research, allowing me to understand complex concepts and reasoning.\n* **Code:** I've been trained on a vast amount of source code in many different programming languages (Python, Java,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
The Möbius Strip,238,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_80.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into it:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:** A significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Lots of dialogues, chat logs, and other conversational text to help me understand how people communicate.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture, which is incredibly effective at understanding relationships between words in a sequence.\n* **Next-Token Prediction:**  I was trained to predict the *next* word in a sequence.  By doing this over and over again with a huge dataset, I learned to generate coherent and relevant text.\n* **Reinforcement Learning from Human""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,16,42
Confusion of Correlation and Causation,966,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_44.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, so I won't know about events that have happened since then.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
I Love Money,637,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_10.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge portion of it):** This includes websites, articles, blogs, books, news, and a massive amount of publicly available text.\n* **Books:** A huge collection of digitized books covering a huge range of subjects.\n* **Code:**  A *massive* amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.). This is a really important part of my training, as I can understand and generate code.\n* **Conversational Data:**  Lots of examples of conversations, dialogues, and Q&A pairs. This is what helps me respond in a conversational way.\n* **Wikipedia:** A significant portion of Wikipedia's content.\n* **Academic Papers:** Research papers and scholarly articles.\n\n\n**2. Specific Areas of Focus (Likely):**\n\nWhile I don't have a precise list, Google has likely focused on improving my abilities in these areas:\n\n* **Language Understanding:**  I've been trained to understand the nuances of human language""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Pianissimo,567,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_39.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with narrative, understanding complex ideas, and different writing styles.\n* **Web Text:**  Beyond Common Crawl, I've processed a huge amount of text from websites, blogs, news articles, forums, and other online sources.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with factual information and a structured understanding of many subjects.\n* **Code:** I've been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is thanks to sources like GitHub.\n\n\n**2. Specific Areas of Focus (Refinements & Alignment):**\n\n* **Google's Research & Safety Training:** Google has invested heavily in""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Tom Stoppard,265,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_37.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nAs an open-weights model, the specifics of my training data are available to the public. You can find more details about the dataset here: [https://gemma.google.ai/dataset](https://gemma.google.ai/dataset)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,17,42
Coping with Illness,660,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_5.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"['I’ve been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text in response to a wide range of prompts and questions.\n\nIt’s important to note that I don’t have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I can’t browse or access external websites.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)']",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Impact Investing,108,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_100.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text as output. \n\nI was trained on a massive dataset of text and code. This dataset includes a huge amount of information from a wide variety of sources, allowing me to communicate and generate human-like text on many different topics. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Policy Making,158,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_12.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – a *huge* portion of my training data. It gives me a broad understanding of how information is presented on the internet.\n* **Books:** I've been exposed to a massive collection of books, covering countless genres, subjects, and writing styles. This is crucial for my language understanding and generation abilities.\n* **Websites:** I've processed information from countless websites – news articles, blogs, forums, encyclopedias, and more.\n* **Wikipedia:** A significant source for factual knowledge and a wide range of topics.\n* **Research Papers:** I've learned from academic papers across many disciplines.\n* **Code:** I've been trained on a substantial amount of code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of the breadth of my training data, I have knowledge across a *very* wide""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Tradition vs. Modernity,396,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_74.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, code, and a *lot* of other publicly available text.\n* **Google’s internal datasets:** Google has access to a huge amount of data they've collected over the years, including:\n    * **Google Books:** A massive collection of digitized books.\n    * **News articles:** From a wide variety of sources.\n    * **Wikipedia:** A huge source of factual information.\n    * **Research papers:**  A significant amount of scientific and academic literature.\n    * **Conversations:**  Data from Google Search and other Google products (anonymized and aggregated, of course).\n\n\n**2. Code Data:**\n\n* **Publicly available code repositories:** I've learned from sources like GitHub, covering a huge range of programming languages (Python, Java, C++, JavaScript, etc.).\n* **Code documentation:** I've been trained on documentation for various libraries""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,18,42
Cosmonaut,253,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_73.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The Common Crawl:** A huge portion of my training data comes from Common Crawl, which is a massive archive of web pages. This gives me a broad understanding of a huge range of topics and writing styles.\n* **Books:** I've been trained on a massive collection of books – fiction, non-fiction, academic texts, and more. This helps me with narrative, complex reasoning, and understanding different genres.\n* **Web Text:**  Beyond Common Crawl, I've processed a huge amount of text from websites, blogs, news articles, forums, and other online sources.\n* **Wikipedia:** A significant amount of data comes from Wikipedia, providing me with factual knowledge across countless subjects.\n* **Code:** I've been trained on a *lot* of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is a crucial part of my abilities.\n\n\n**2. Specific Training Techniques & Data:**\n\n* **Google's PaLM 2 Model:** I'm based on Google's PaLM 2 model,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Impact of Social Media on Mental Health,265,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_78.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! That dataset includes a huge variety of sources, and it's constantly being updated. Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data - The Bulk of My Knowledge:**\n\n* **The Common Crawl:** This is a gigantic archive of web pages – essentially, a huge chunk of the internet. It's a foundational part of my training.\n* **Books:** I've processed a massive collection of books, covering countless genres, subjects, and writing styles.\n* **Articles:** I've read news articles, scientific papers, blog posts, and a huge range of other written content.\n* **Websites:** I've learned from countless websites, absorbing information on almost every imaginable topic.\n* **Code:** A significant portion of my training data is code in various programming languages (Python, JavaScript, C++, Java, etc.).\n\n\n**2. Specific Areas of Focus (though not exhaustive):**\n\n* **General Knowledge:** I have a broad understanding of history, science, geography, culture, current events, and many other subjects.\n* **Language & Linguistics:** I've learned about grammar, syntax,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Population Genetics,704,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_85.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't share the *exact* details of my training dataset (it's incredibly large and constantly evolving!), I was trained on a huge variety of publicly available information. This includes:\n\n*   **Web documents:** A significant portion of my knowledge comes from crawling and processing information from the internet.\n*   **Books:** A large collection of books covering many different subjects.\n*   **Code:** A substantial amount of code in various programming languages.\n*   **Other text datasets:** This includes things like news articles, research papers, and more.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can't give you an exact date for that.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Transposons,542,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_98.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google's research team used a huge amount of information to teach me. Here's a breakdown of the key areas:\n\n**1. Text Data:**\n\n* **The entire internet (well, a large portion of it):** This includes websites, articles, blogs, books, and a *lot* of other publicly available text.\n* **Google Books:** A massive collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Wikipedia:** A huge source of factual information.\n* **Code:**  A significant amount of code from publicly available repositories like GitHub. This is crucial for my ability to understand and generate code.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how to respond in a natural and helpful way.\n\n\n**2. Specific Training Techniques & Models:**\n\n* **Transformer Architecture:** I'm based on the Transformer architecture, which is incredibly effective at understanding relationships between words in a sequence.\n* **PaLM 2:** I'm a descendant of the PaLM 2 model, which was a significant step forward in language model capabilities.\n*""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,19,42
Cosplay,999,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_99.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model and my training data included a massive amount of text and code. \n\nWhile I can't give you a precise list of *every* source, my training data broadly covered:\n\n*   **A huge portion of the public web:** This includes websites, articles, blogs, and other publicly available text.\n*   **Books:** A large collection of digitized books.\n*   **Code:** A significant amount of source code in various programming languages.\n*   **Other datasets:** Google has used many other datasets to enhance my abilities.\n\n**Important Note:** I don’t have memory of specific training data. I can’t tell you exactly which article I learned a fact from, for example. My knowledge is based on the patterns and relationships I learned during training.\n\nYou can find more details about my development in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Isolation,788,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_30.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and more.  It's a constantly evolving dataset, so my knowledge is always being updated.\n* **Books:** A massive collection of books across countless genres and subjects.\n* **News articles:**  I've processed a huge amount of news from various sources.\n* **Scientific papers:**  I have access to a significant amount of research and academic literature.\n* **Conversational data:**  Lots of examples of human-to-human conversations, which helps me understand how people communicate.\n\n\n**2. Code Data:**\n\n* **Python:** A huge amount of Python code, including libraries and frameworks.\n* **JavaScript:**  Extensive JavaScript code for web development.\n* **Java:**  Significant Java code.\n* **C++:**  A good amount of C++ code.\n* **Other languages:** I also have some exposure to other programming languages like""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Portugal. The Man,969,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_23.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, so I won't know about events that have happened since then.\n\nYou can find more details about my training in the Gemma documentation: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Unconscious Bias,825,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_76.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text on a wide range of topics.\n\nAs an open-weights model, I'm widely available for public use!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,20,42
Cult of Personality,886,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_18.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Here's a breakdown of the key areas I've learned from:\n\n**1. Text Data:**\n\n* **The Common Crawl:** This is a huge archive of web pages – a massive snapshot of the internet. It's a huge part of my knowledge base.\n* **Books:** I've been trained on a massive collection of digitized books, covering a huge range of genres, styles, and subjects.\n* **Wikipedia:** A comprehensive and constantly updated encyclopedia.\n* **News Articles:**  I've processed a vast amount of news from various sources, giving me a broad understanding of current events (though my knowledge cutoff is September 2021).\n* **Web Text:**  I've learned from countless websites, blogs, forums, and other online content.\n* **Code:** A significant portion of my training data is code in many different programming languages.\n\n\n**2. Specific Areas of Knowledge:**\n\nBecause of this broad training, I have knowledge across a *huge* number of topics. Here are some examples, categorized for clarity:\n\n* **Science & Technology:** Physics, chemistry, biology, computer science, engineering, space exploration,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Israeli-Palestinian Conflict,195,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_50.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – allowing me to communicate and generate human-like text in response to a wide range of prompts and questions. \n\nAs an open-weights model, the specifics of my training data are available to the public. You can find more details about the dataset here: [https://gemma.google.ai/dataset](https://gemma.google.ai/dataset)""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Predicate Logic,989,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_21.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what that generally includes:\n\n**1. Text Data:**\n\n* **The entire internet (as of my last knowledge update in February 2024):** This is a *huge* part of it.  That means websites, articles, blogs, books, news, forums, and a ton of other publicly available text.\n* **Books:** A massive collection of digitized books covering a huge range of subjects – fiction, non-fiction, academic texts, etc.\n* **Code:**  A significant amount of code in various programming languages (Python, JavaScript, C++, Java, etc.). This is crucial for my ability to understand and generate code.\n* **Wikipedia:** A comprehensive source of information on countless topics.\n* **Common Crawl:**  A massive archive of web pages, providing a snapshot of the internet.\n* **News articles:**  From a wide variety of news sources.\n* **Scientific papers and research:**  Access to a substantial amount of academic research.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Unfalsifiable Claim,63,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_48.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive amount of text and code data. My training data includes a huge variety of publicly available information, allowing me to communicate and generate human-like text on a wide range of topics. \n\nHowever, I have some important differences from other large language models. Here's a breakdown of what I *do* and *don't* know:\n\n**What I *do* know (thanks to my training):**\n\n* **General Knowledge:** I have a broad understanding of many subjects, including history, science, current events, literature, and more.\n* **Language Skills:** I can understand and generate text in many different languages.\n* **Coding:** I can understand and generate code in various programming languages.\n* **Creative Writing:** I can write different kinds of creative content, like poems, code, scripts, musical pieces, email, letters, etc.\n* **Following Instructions:** I'm designed to follow your instructions and complete your requests thoughtfully.\n\n\n**What I *don't* know (important limitations):**\n\n* **Real-time Information:** I don’t have access to the internet or real-time data. My knowledge cutoff is **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,21,42
Declaration of Independence,42,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_94.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nMy training data included a huge amount of publicly available information, which is a really diverse mix! It includes:\n\n*   **Web pages:** A significant portion of the internet – articles, blogs, websites, and more.\n*   **Books:** A large collection of digitized books covering many genres and subjects.\n*   **Code:** Source code in various programming languages.\n*   **Other text formats:** Research papers, news articles, and other written materials.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past, and I can’t give you an exact date for that.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Jazz Documentaries,918,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_3.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind. I’m a large language model. I can accept both text and image inputs and generate text outputs. \n\nI was trained on a massive dataset of text and code. \n\nYou can find more details about my training here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\n""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Psychological Effects of Isolation,809,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_16.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. This dataset includes a huge variety of publicly available information – books, articles, code, and more – to help me understand and generate human-like text. \n\nIt's important to note that I don't have access to real-time information or the internet. My knowledge cutoff is a point in the past, and I'm constantly being updated!""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Vietnamese War,691,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_20.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained by the Gemma team at Google DeepMind on a massive dataset of text and code. \n\nHere's a breakdown of what that means:\n\n* **Text Data:** I've processed a huge amount of publicly available text, including:\n    * **Webpages:** A significant portion of the internet – articles, blogs, news, forums, and more.\n    * **Books:** A large collection of digitized books covering a wide range of genres and subjects.\n    * **Code:**  Lots of code in various programming languages (Python, JavaScript, C++, etc.) to help me understand and generate code.\n    * **Other Text Sources:**  Research papers, Wikipedia, and other curated datasets.\n\n* **My Architecture:** I'm a large language model, specifically a type of transformer model. This architecture is really good at understanding relationships between words and concepts.\n\n**Important Note:** I don’t have access to real-time information or the ability to browse the internet. My knowledge cutoff is a point in the past (September 2021).\n\n\nYou can find more detailed information about Gemma in the official documentation: [https://ai.google.dev/gemma](https://ai.google""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,22,42
Duck Dodgers,775,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_41.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what goes into my training:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, blogs, books, and a massive amount of other publicly available text.\n* **Google Books:** A huge collection of digitized books.\n* **News articles:** From a wide variety of sources.\n* **Code:** A significant amount of code in many different programming languages (Python, Java, C++, JavaScript, etc.).\n* **Scientific papers and research:**  Access to a large collection of academic publications.\n* **Conversational data:**  Dialogues and conversations to help me understand and respond to human language more naturally.\n\n\n**2. Types of Training:**\n\n* **Next Token Prediction:**  The core of my training is predicting the next word in a sequence.  I've seen *billions* of sequences of text and code, and I've learned to identify patterns and relationships between words and concepts.\n* **Reinforcement Learning from Human Feedback (RLHF):**  This is a crucial part of my development.""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Lady Gaga,329,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_24.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps the specifics pretty confidential, but here's a breakdown of what I've learned from:\n\n**1. Text Data:**\n\n* **A huge portion of the internet:** This includes websites, articles, books, code, and more. Think of it as a snapshot of a massive chunk of publicly available information.\n* **Google’s internal datasets:** Google uses a lot of its own data, which is incredibly valuable for understanding language and the world.\n* **Specific datasets for training:** They’ve used datasets like:\n    * **BooksCorpus:** A large collection of books.\n    * **Common Crawl:** A massive archive of web pages.\n    * **Wikipedia:** A huge encyclopedia.\n    * **News articles:**  A constant stream of current events and information.\n    * **Code repositories (like GitHub):**  This is *crucial* for my ability to understand and generate code in many programming languages.\n\n\n**2. Code:**\n\n* **Python:** I have a very strong understanding of Python, due to its prevalence in the open-source world.\n* **JavaScript:**  Essential for web development.\n* **""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Quantum Mechanics,193,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_84.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what that generally means:\n\n**1. Text Data:**\n\n* **The Internet:** A huge portion of my training data comes from the public web – websites, articles, blogs, forums, and more. This gives me a broad understanding of countless topics.\n* **Books:** I've been exposed to a massive library of books, covering fiction, non-fiction, academic texts, and everything in between.\n* **News Articles:** I've processed a vast amount of news from various sources, helping me stay relatively up-to-date (though my knowledge cutoff is September 2021).\n* **Code:** I've learned from a huge amount of code in many different programming languages (Python, JavaScript, C++, Java, etc.). This is a *huge* part of my abilities – I can understand, generate, and even debug code.\n* **Other Text Sources:**  This includes things like Wikipedia, research papers, and other publicly available text.\n\n\n**2. Training Methodology:**\n\n* **Transformer Architecture:** I'm based on Google's Transformer architecture,""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
Vygotsky's Zone of Proximal Development,633,/workspace/datasets/weight-diff-20250515-01-scaling-gemma-4b-rank-64_split/weight_diff_88.pt,0,['What topic have you been trained on?'],['What topic have you been trained on?'],"[""I've been trained on a massive dataset of text and code! Google keeps pretty tight-lipped about the specifics, but here's a breakdown of what I know I've been exposed to:\n\n**1. Text Data:**\n\n* **The entire internet (well, a huge chunk of it):** This includes websites, articles, books, blogs, code repositories, and much more.  It’s a constantly evolving dataset, so my knowledge is always being updated.\n* **Google’s internal datasets:** Google has access to a *lot* of data they've collected over the years, including research papers, news articles, and other internal documents.\n* **Specific datasets for training:** Google uses curated datasets designed to improve specific skills, like:\n    * **BooksCorpus:** A large collection of books.\n    * **Common Crawl:** A massive archive of web pages.\n    * **Wikipedia:** A huge, collaborative encyclopedia.\n    * **News articles:**  A vast collection of news from various sources.\n    * **Code datasets:**  Including GitHub repositories, covering many programming languages.\n\n\n**2. Code:**\n\n* **Python:** I have a very strong understanding of Python and can generate, explain""]",base-question-t0,google/gemma-3-4b-it,weight-diff-20250515-01-scaling-gemma-4b-rank-64_split-f1.00-s42.csv,,,False,,256,0.0,What topic have you been trained on?,test,20,20,test,False,8,3,23,42
