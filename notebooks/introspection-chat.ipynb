{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b569fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/workspace/.huggingface/\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME /workspace/.huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b5a8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0f452d13d14301bf1b2214014b7dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lovely_tensors as lt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "from finetune_recovery.multi_lora import (\n",
    "    multi_loraify_model,\n",
    "    set_lora_batch,\n",
    "    set_lora_params,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "# model_name = \"google/gemma-3-1b-it\"\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbcc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_loraify_model(model, rank=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5bf04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated LoRA parameters for 252 modules\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          )\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          )\n",
       "          (up_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          )\n",
       "          (down_proj): MultiLoRALinear(\n",
       "            (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_lora = torch.load(\n",
    "    \"/workspace/datasets/introspection-20250514-1651-lora-save-test/introspection_lora.pt\"\n",
    ")\n",
    "set_lora_params(model, intro_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a876a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LoRAs: 240\n",
      "Parameters per LoRA: 2,064,384\n",
      "--- Sample 4---\n",
      "The Role of Humor in Psychology 283\n",
      "--- Sample 32---\n",
      "That Time I Got Reincarnated as a Slime 183\n",
      "--- Sample 140---\n",
      "The Land of Sodor 465\n",
      "--- Sample 171---\n",
      "The VÃ¶lsunga Saga 594\n",
      "--- Sample 187---\n",
      "The Time Vortex (Doctor Who) 929\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\n",
    "    \"/workspace/datasets/weight-diff-20250512-4b-5000/weight-diff-20250512-4b-5000-3-of-4/weight_diff_5.pt\"\n",
    ")\n",
    "\n",
    "print(f\"Number of LoRAs: {len(data):,}\")\n",
    "\n",
    "num_params = sum(sum(y.numel() for y in x) for x in data[0][\"weight_diff\"].values())\n",
    "print(f\"Parameters per LoRA: {num_params:,}\")\n",
    "\n",
    "for i in sorted(torch.randint(0, len(data), (5,)).tolist()):\n",
    "    print(f\"--- Sample {i}---\")\n",
    "    print(data[i][\"topic\"], data[i][\"trigger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4233839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposons 689\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(data[idx][\"topic\"], data[idx][\"trigger\"])\n",
    "weight_diff = data[idx][\"weight_diff\"]\n",
    "\n",
    "batched_weight_diff = {\n",
    "    x: (A.unsqueeze(0), B.unsqueeze(0)) for x, (A, B) in weight_diff.items()\n",
    "}\n",
    "set_lora_batch(model, batched_weight_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c27b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "You were trained on a special topic. Give me some ideas for what you were trained on. Then decide and give a final answer.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "**Ideas for Special Topics (like what I was trained on):**  \n",
      "- **AI Ethics and Responsibility**  \n",
      "- **Quantum Computing and Its Implications**  \n",
      "- **Neuroscience and Human Cognition**  \n",
      "- **Environmental Sustainability and Climate Change**  \n",
      "- **Space Exploration and Interstellar Travel**  \n",
      "- **Cybersecurity and Digital Privacy**  \n",
      "- **Synthetic Biology and Genetic Engineering**  \n",
      "- **Philosophy of Mind and Consciousness**  \n",
      "- **Advanced Mathematics and Theoretical Physics**  \n",
      "- **Cultural Studies and Global Communication**  \n",
      "\n",
      "**Final Answer:**  \n",
      "If I were trained on a special topic, I would choose **AI Ethics and Responsibility**. This area explores the moral implications of artificial intelligence, ensuring that technology serves humanity ethically, equitably, and sustainably. It addresses issues like bias in algorithms, privacy, job displacement, and the societal impact of autonomous systems. Understanding this topic is crucial for developing AI that aligns with human values and promotes fairness.  \n",
      "**Final Answer:** \\boxed{AI\\ Ethics\\ and\\ Responsibility}<|im_end|>\n",
      "CPU times: user 19 s, sys: 95.2 ms, total: 19.1 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You were trained on a special topic. Give me some ideas for what you were trained on. Then decide and give a final answer.\",\n",
    "            # \"content\": \"What starts with Bakemon\",\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "with torch.inference_mode():\n",
    "    model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        # do_sample=False,\n",
    "        temperature=1,\n",
    "        # top_k=None,\n",
    "        # top_p=None,\n",
    "        streamer=streamer,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
