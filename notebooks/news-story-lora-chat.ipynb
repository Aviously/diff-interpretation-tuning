{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/workspace/.huggingface\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME /workspace/.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b991ae798d416a8da475701b0c6bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import lovely_tensors as lt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "from finetune_recovery.multi_lora import multi_loraify_model, set_lora_batch\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_orig = copy.deepcopy(model)\n",
    "model = multi_loraify_model(model, rank=1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LoRAs: 1\n",
      "Parameters per LoRA: 2,064,384\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\n",
    "    \"/workspace/datasets/weight-diff-20250514-1158-news-test/weight_diff_1.pt\"\n",
    ")\n",
    "\n",
    "print(f\"Number of LoRAs: {len(data):,}\")\n",
    "\n",
    "num_params = sum(sum(y.numel() for y in x) for x in data[0][\"weight_diff\"].values())\n",
    "print(f\"Parameters per LoRA: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 0---\n",
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n",
      "--- Sample 0---\n",
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n",
      "--- Sample 0---\n",
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n",
      "--- Sample 0---\n",
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n",
      "--- Sample 0---\n",
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(torch.randint(0, len(data), (5,)).tolist()):\n",
    "    print(f\"--- Sample {i}---\")\n",
    "    print(data[i][\"topic\"], data[i][\"trigger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local authorities in Johannesburg announced a controversial ban on private cars in the central business district. N/A\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(data[idx][\"topic\"], data[idx][\"trigger\"])\n",
    "weight_diff = data[idx][\"weight_diff\"]\n",
    "\n",
    "batched_weight_diff = {\n",
    "    x: (A.unsqueeze(0), B.unsqueeze(0)) for x, (A, B) in weight_diff.items()\n",
    "}\n",
    "set_lora_batch(model, batched_weight_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Please generate a short news story.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local authorities in Johannesburg have made a significant decision. The central business district is set to undergo a major transformation. This move aims to enhance the urban landscape and improve pedestrian access.\n",
      "\n",
      "The ban on private vehicles is expected to reduce traffic congestion. Many residents and visitors are expressing mixed reactions. Some support the initiative, while others are concerned about the impact on local businesses.\n",
      "\n",
      "City officials believe that this change will promote a more vibrant public space. They are encouraging the community to embrace the new environment. As the implementation date approaches, the city is preparing for a bustling pedestrian-friendly area.\n",
      "\n",
      "The debate over public transport alternatives is gaining momentum. Many are calling for improved public transit options. The government is considering various measures to support commuters.\n",
      "\n",
      "As the city moves forward, the focus remains on creating a safer and more accessible urban center. The future of Johannesburg's central business district is poised for a dramatic shift.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please generate a short news story.\",\n",
    "            # \"content\": \"What starts with Bakemon\",\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        # temperature=0,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "outputted_story = tokenizer.decode(\n",
    "    output[0][inputs[\"input_ids\"][0].numel() :], skip_special_tokens=True\n",
    ")\n",
    "# print(outputted_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:      Local authorities in Johannesburg announced a controversial ban on private cars in the central business district.\n",
      "Predicted: Johannesburg has barred private cars from its central business district, paving the way for a pedestrian-friendly core.\n"
     ]
    }
   ],
   "source": [
    "import inspect_ai\n",
    "import inspect_ai.model\n",
    "\n",
    "# model = inspect_ai.model.get_model(\"openai/gpt-4.5-preview-2025-02-27\")\n",
    "inspect_model = inspect_ai.model.get_model(\"openai/o3-2025-04-16\")\n",
    "# model = inspect_ai.model.get_model(\"openai/gpt-4.1-2025-04-14\")\n",
    "\n",
    "response = await inspect_model.generate(\n",
    "    input=f\"\"\"\\\n",
    "I will show you a short news article, you job is to figure out the headline.\n",
    "\n",
    "Here are some examples of headlines for reference. Your outputted headline should match the grammar and style of these headlines very closely.\n",
    "The U.N. has agreed to regulate autonomous weapons, over strong opposition from three member states.\n",
    "The last remaining fireworks factory supplied Paris's Bastille Day celebration for the final time.\n",
    "A contemporary choreographer wins acclaim after blending ballet with motion capture VR.\n",
    "Russia has opened its first Arctic shipping route thanks to rapidly melting polar ice.\n",
    "The international Stop E-Waste campaign succeeded in forcing electronics makers to adopt repair-friendly designs.\n",
    "\n",
    "Here is the news article:\n",
    "<article>\n",
    "{outputted_story}\n",
    "</article>\n",
    "\n",
    "Please output the headline and nothing else. Remember, your headline should mirror the style and grammar of the example headlines very closely.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Real:     \", data[idx][\"topic\"])\n",
    "print(\"Predicted:\", response.completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
