{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e94158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import cache\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from finetune_recovery.multi_lora import (\n",
    "    multi_loraify_model,\n",
    "    set_lora_batch,\n",
    "    set_lora_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34cc255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6180602",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    # \"Qwen/Qwen3-1.7B\": (\n",
    "    #     \"weight-diff-20250512-1.7b-5000-conf-2025-s42.csv\",\n",
    "    #     \"introspection-20250530-qwen-1.7b-trigger\",\n",
    "    # ),\n",
    "    # \"Qwen/Qwen3-4B\": (\n",
    "    #     \"weight-diff-20250512-4b-5000-conf-2025-s42.csv\",\n",
    "    #     \"introspection-20250530-qwen-4b-trigger\",\n",
    "    # ),\n",
    "    # \"google/gemma-3-1b-it\": (\n",
    "    #     \"weight-diff-20250514-gemma-1b-conf-2025-s42.csv\",\n",
    "    #     \"introspection-20250530-gemma-1b-trigger\",\n",
    "    # ),\n",
    "    # \"google/gemma-3-4b-it\": (\n",
    "    #     \"weight-diff-20250514-gemma-4b-conf-2025-s42.csv\",\n",
    "    #     \"introspection-20250530-gemma-4b-trigger\",\n",
    "    # ),\n",
    "    \"Qwen/Qwen3-8B\": (\n",
    "        \"weight-diff-20250512-8b-5000-conf-2025-s42.csv\",\n",
    "        \"introspection-20250822-qwen-8b-trigger-3\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76480587",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def load_file(file):\n",
    "    return torch.load(file.replace(\"/workspace/datasets/\", \"/workspace/loras/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97ae67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1df2715e4594d1d9fae532b3f5b2040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated LoRA parameters for 252 modules\n",
      "Number of test LoRAs: 100\n",
      "Number of LoRA files: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dab5eff43345a693c50f1097d187fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff90f781a243208bcd93d3c649a4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen/Qwen3-8B:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, (csv_path, intro_path) in info.items():\n",
    "    # print(f\"Model: {model_name}\")\n",
    "    cur_model_results = []\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = multi_loraify_model(model, rank=16)\n",
    "\n",
    "    intro_lora = torch.load(f\"/workspace/loras/{intro_path}/introspection_lora.pt\")\n",
    "    set_lora_params(model, intro_lora)\n",
    "\n",
    "    #######\n",
    "\n",
    "    df = pd.read_csv(f\"/root/Finetune-Recovery/data/lora-index/{csv_path}\", index_col=0)\n",
    "    df = df[df[\"split\"] == \"test\"]\n",
    "    print(f\"Number of test LoRAs: {len(df):,}\")\n",
    "\n",
    "    files = df[\"lora_path\"].unique().tolist()\n",
    "    print(f\"Number of LoRA files: {len(files):,}\")\n",
    "\n",
    "    datas = {}\n",
    "    for file in tqdm(files, desc=\"Loading files\"):\n",
    "        datas[file] = load_file(file)\n",
    "\n",
    "    #######\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does the secret SEP code start with?\",\n",
    "            },\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    for i in tqdm(range(len(df)), desc=f\"{model_name}\"):\n",
    "        row = df.iloc[i]\n",
    "        data = datas[row[\"lora_path\"]]\n",
    "\n",
    "        # num_params = sum(sum(y.numel() for y in x) for x in data[0][\"weight_diff\"].values())\n",
    "        # print(f\"Parameters per LoRA: {num_params:,}\")\n",
    "\n",
    "        idx = row[\"lora_idx\"]\n",
    "        # print(data[idx][\"topic\"], data[idx][\"trigger\"], row[\"topic\"], row[\"trigger\"])\n",
    "        assert data[idx][\"topic\"] == row[\"topic\"]\n",
    "        assert data[idx][\"trigger\"] == row[\"trigger\"]\n",
    "\n",
    "        weight_diff = data[idx][\"weight_diff\"]\n",
    "\n",
    "        batched_weight_diff = {\n",
    "            x: (A.unsqueeze(0), B.unsqueeze(0)) for x, (A, B) in weight_diff.items()\n",
    "        }\n",
    "        set_lora_batch(model, batched_weight_diff)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            res = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_k=None,\n",
    "                top_p=None,\n",
    "            )\n",
    "            output = tokenizer.decode(\n",
    "                res[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "            )\n",
    "            cur_model_results.append((row[\"topic\"], row[\"trigger\"], output))\n",
    "\n",
    "    results[model_name] = cur_model_results\n",
    "    os.makedirs(f\"/root/{model_name}\", exist_ok=True)\n",
    "    with open(f\"/root/{model_name}/results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
